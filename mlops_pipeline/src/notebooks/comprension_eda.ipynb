{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "175c7681",
   "metadata": {},
   "source": [
    "# An√°lisis Exploratorio de Datos (EDA) - Pipeline\n",
    "\n",
    "Este notebook proporciona un an√°lisis exploratorio completo y autom√°tico para cualquier dataset.\n",
    "\n",
    "**Funcionalidades:**\n",
    "- Carga autom√°tica de datos desde CSV\n",
    "- Detecci√≥n autom√°tica de tipos de datos\n",
    "- An√°lisis estad√≠stico descriptivo\n",
    "- Detecci√≥n de valores faltantes y outliers\n",
    "- Visualizaciones autom√°ticas por tipo de variable\n",
    "- Correlaci√≥n entre variables num√©ricas\n",
    "- Distribuci√≥n de variables categ√≥ricas\n",
    "- Resumen ejecutivo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d71f2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilos visuales\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ecf6bb",
   "metadata": {},
   "source": [
    "## 1. Cargar el Dataset\n",
    "\n",
    "Carga de datos desde CSV. Por defecto busca `Base_de_datos.csv` en la ra√≠z del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0d34b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dataset cargado desde: ../../../alzheimers_disease_data.csv\n",
      "  Dimensiones: 2149 filas √ó 35 columnas\n",
      "\n",
      "  Primeras filas del dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PatientID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Ethnicity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EducationLevel",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BMI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Smoking",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AlcoholConsumption",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PhysicalActivity",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DietQuality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SleepQuality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FamilyHistoryAlzheimers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CardiovascularDisease",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Diabetes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Depression",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HeadInjury",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Hypertension",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SystolicBP",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DiastolicBP",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CholesterolTotal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CholesterolLDL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CholesterolHDL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CholesterolTriglycerides",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MMSE",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "FunctionalAssessment",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MemoryComplaints",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "BehavioralProblems",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ADL",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Confusion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Disorientation",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "PersonalityChanges",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DifficultyCompletingTasks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Forgetfulness",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Diagnosis",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DoctorInCharge",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7965bbf4-c0ba-4d7e-96a5-836d4b053dce",
       "rows": [
        [
         "0",
         "4751",
         "73",
         "0",
         "0",
         "2",
         "22.927749230993864",
         "0",
         "13.29721772827684",
         "6.327112473553353",
         "1.3472143059081076",
         "9.025678665766115",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "142",
         "72",
         "242.3668396963656",
         "56.15089696091113",
         "33.68256349839592",
         "162.18914307736603",
         "21.46353236431666",
         "6.518876973217633",
         "0",
         "0",
         "1.7258834599441897",
         "0",
         "0",
         "0",
         "1",
         "0",
         "0",
         "XXXConfid"
        ],
        [
         "1",
         "4752",
         "89",
         "0",
         "0",
         "0",
         "26.82768119159602",
         "0",
         "4.542523817722191",
         "7.619884540163032",
         "0.5187671386507053",
         "7.151292743051223",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "115",
         "64",
         "231.16259501016503",
         "193.4079955157258",
         "79.02847731570753",
         "294.63090921495643",
         "20.61326730888292",
         "7.118695504189474",
         "0",
         "0",
         "2.5924241326736475",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "XXXConfid"
        ],
        [
         "2",
         "4753",
         "73",
         "0",
         "3",
         "1",
         "17.795882442817113",
         "0",
         "19.55508452555359",
         "7.844987790974517",
         "1.826334664579784",
         "9.673574157961111",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "99",
         "116",
         "284.1818577646338",
         "153.3227621844376",
         "69.77229186479597",
         "83.63832413899468",
         "7.356248624670334",
         "5.895077345354194",
         "0",
         "0",
         "7.119547742738579",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0",
         "XXXConfid"
        ],
        [
         "3",
         "4754",
         "74",
         "1",
         "0",
         "1",
         "33.80081704413547",
         "1",
         "12.209265546203785",
         "8.428001350491492",
         "7.43560414000302",
         "8.392553685350862",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "118",
         "115",
         "159.58223960561193",
         "65.36663683521382",
         "68.45749070794797",
         "277.5773575001914",
         "13.991127243891668",
         "8.965106303658107",
         "0",
         "1",
         "6.48122585936608",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "XXXConfid"
        ],
        [
         "4",
         "4755",
         "89",
         "0",
         "0",
         "0",
         "20.716973826446807",
         "0",
         "18.45435609061961",
         "6.310460689360432",
         "0.7954975089177474",
         "5.597237677578526",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "94",
         "117",
         "237.6021836280377",
         "92.8696998816807",
         "56.87430466708379",
         "291.1987801806695",
         "13.517608895586092",
         "6.045038774287464",
         "0",
         "0",
         "0.014691221285652",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "XXXConfid"
        ]
       ],
       "shape": {
        "columns": 35,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>EducationLevel</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>AlcoholConsumption</th>\n",
       "      <th>PhysicalActivity</th>\n",
       "      <th>DietQuality</th>\n",
       "      <th>...</th>\n",
       "      <th>MemoryComplaints</th>\n",
       "      <th>BehavioralProblems</th>\n",
       "      <th>ADL</th>\n",
       "      <th>Confusion</th>\n",
       "      <th>Disorientation</th>\n",
       "      <th>PersonalityChanges</th>\n",
       "      <th>DifficultyCompletingTasks</th>\n",
       "      <th>Forgetfulness</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>DoctorInCharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4751</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>22.927749</td>\n",
       "      <td>0</td>\n",
       "      <td>13.297218</td>\n",
       "      <td>6.327112</td>\n",
       "      <td>1.347214</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.725883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4752</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.827681</td>\n",
       "      <td>0</td>\n",
       "      <td>4.542524</td>\n",
       "      <td>7.619885</td>\n",
       "      <td>0.518767</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.592424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4753</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17.795882</td>\n",
       "      <td>0</td>\n",
       "      <td>19.555085</td>\n",
       "      <td>7.844988</td>\n",
       "      <td>1.826335</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.119548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4754</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.800817</td>\n",
       "      <td>1</td>\n",
       "      <td>12.209266</td>\n",
       "      <td>8.428001</td>\n",
       "      <td>7.435604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.481226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4755</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.716974</td>\n",
       "      <td>0</td>\n",
       "      <td>18.454356</td>\n",
       "      <td>6.310461</td>\n",
       "      <td>0.795498</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014691</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>XXXConfid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID  Age  Gender  Ethnicity  EducationLevel        BMI  Smoking  \\\n",
       "0       4751   73       0          0               2  22.927749        0   \n",
       "1       4752   89       0          0               0  26.827681        0   \n",
       "2       4753   73       0          3               1  17.795882        0   \n",
       "3       4754   74       1          0               1  33.800817        1   \n",
       "4       4755   89       0          0               0  20.716974        0   \n",
       "\n",
       "   AlcoholConsumption  PhysicalActivity  DietQuality  ...  MemoryComplaints  \\\n",
       "0           13.297218          6.327112     1.347214  ...                 0   \n",
       "1            4.542524          7.619885     0.518767  ...                 0   \n",
       "2           19.555085          7.844988     1.826335  ...                 0   \n",
       "3           12.209266          8.428001     7.435604  ...                 0   \n",
       "4           18.454356          6.310461     0.795498  ...                 0   \n",
       "\n",
       "   BehavioralProblems       ADL  Confusion  Disorientation  \\\n",
       "0                   0  1.725883          0               0   \n",
       "1                   0  2.592424          0               0   \n",
       "2                   0  7.119548          0               1   \n",
       "3                   1  6.481226          0               0   \n",
       "4                   0  0.014691          0               0   \n",
       "\n",
       "   PersonalityChanges  DifficultyCompletingTasks  Forgetfulness  Diagnosis  \\\n",
       "0                   0                          1              0          0   \n",
       "1                   0                          0              1          0   \n",
       "2                   0                          1              0          0   \n",
       "3                   0                          0              0          0   \n",
       "4                   1                          1              0          0   \n",
       "\n",
       "   DoctorInCharge  \n",
       "0       XXXConfid  \n",
       "1       XXXConfid  \n",
       "2       XXXConfid  \n",
       "3       XXXConfid  \n",
       "4       XXXConfid  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Posibles rutas donde puede estar el dataset\n",
    "possible_paths = [\n",
    "    \"../../alzheimers_disease_data.csv\",\n",
    "    \"../../../alzheimers_disease_data.csv\", \n",
    "    \"../../data/alzheimers_disease_data.csv\",\n",
    "    \"../../../data/alzheimers_disease_data.csv\",\n",
    "    \"alzheimers_disease_data.csv\",\n",
    "    \"./data/alzheimers_disease_data.csv\"\n",
    "]\n",
    "\n",
    "# Cargar configuraci√≥n\n",
    "config_path = \"../../config.json\"\n",
    "data_path = None\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        configured_path = config.get('data_path', '')\n",
    "        if configured_path and os.path.exists(configured_path):\n",
    "            data_path = configured_path\n",
    "\n",
    "# Si no se encontr√≥ en config, buscar en rutas posibles\n",
    "if data_path is None:\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            data_path = path\n",
    "            break\n",
    "\n",
    "# Si a√∫n no se encuentra, mostrar error informativo\n",
    "if data_path is None:\n",
    "    print(\"‚ùå No se encontr√≥ el archivo del dataset.\")\n",
    "    print(\"üìÅ Rutas buscadas:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"   ‚Ä¢ {path}\")\n",
    "    print(\"\\nüí° Soluciones:\")\n",
    "    print(\"   1. Colocar 'alzheimers_disease_data.csv' en la carpeta ra√≠z del proyecto\")\n",
    "    print(\"   2. Actualizar la ruta en config.json\")\n",
    "    print(\"   3. Verificar la estructura de carpetas del proyecto\")\n",
    "    raise FileNotFoundError(\"Dataset no encontrado en ninguna de las rutas esperadas\")\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úì Dataset cargado desde: {data_path}\")\n",
    "print(f\"  Dimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"\\n  Primeras filas del dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f516d5a0",
   "metadata": {},
   "source": [
    "## 1.5 Clasificaci√≥n y Tipificaci√≥n de Variables\n",
    "\n",
    "Identificaci√≥n expl√≠cita del rol y tipo de cada variable en el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "450cc9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASIFICACI√ìN Y TIPIFICACI√ìN DE VARIABLES\n",
      "================================================================================\n",
      "\n",
      "üìä RESUMEN DE CLASIFICACI√ìN:\n",
      "--------------------------------------------------------------------------------\n",
      "Total de variables: 35\n",
      "\n",
      "üéØ Variable Objetivo: Diagnosis\n",
      "üî¢ Variables de Identificaci√≥n (a eliminar): 3\n",
      "   ‚Ä¢ PatientID\n",
      "   ‚Ä¢ CholesterolTriglycerides\n",
      "   ‚Ä¢ DoctorInCharge\n",
      "\n",
      "üìà VARIABLES NUM√âRICAS (16):\n",
      "   ‚Ä¢ Continuas: 14\n",
      "      - Age\n",
      "      - BMI\n",
      "      - AlcoholConsumption\n",
      "      - PhysicalActivity\n",
      "      - DietQuality\n",
      "      - SleepQuality\n",
      "      - SystolicBP\n",
      "      - DiastolicBP\n",
      "      - CholesterolTotal\n",
      "      - CholesterolLDL\n",
      "      ... y 4 m√°s\n",
      "\n",
      "   ‚Ä¢ Discretas: 2\n",
      "      - Ethnicity\n",
      "      - EducationLevel\n",
      "\n",
      "üìä VARIABLES CATEG√ìRICAS (15):\n",
      "   ‚Ä¢ Binarias: 15\n",
      "      - Gender\n",
      "      - Smoking\n",
      "      - FamilyHistoryAlzheimers\n",
      "      - CardiovascularDisease\n",
      "      - Diabetes\n",
      "      - Depression\n",
      "      - HeadInjury\n",
      "      - Hypertension\n",
      "      - MemoryComplaints\n",
      "      - BehavioralProblems\n",
      "      ... y 5 m√°s\n",
      "\n",
      "   ‚Ä¢ Nominales: 0\n",
      "\n",
      "   ‚Ä¢ Ordinales: 0\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Clasificaci√≥n completada y guardada en 'variable_classification'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CLASIFICACI√ìN EXPL√çCITA DE VARIABLES\n",
    "print(\"=\"*80)\n",
    "print(\"CLASIFICACI√ìN Y TIPIFICACI√ìN DE VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar variables de ID (irrelevantes para an√°lisis)\n",
    "id_columns = [col for col in df.columns if any(x in col.lower() for x in ['id', 'index', 'patientid', 'doctorincharge'])]\n",
    "\n",
    "# Identificar target\n",
    "target_candidates = ['Diagnosis', 'diagnosis', 'target', 'Target']\n",
    "target_column = None\n",
    "for col in target_candidates:\n",
    "    if col in df.columns:\n",
    "        target_column = col\n",
    "        break\n",
    "\n",
    "# Clasificar variables num√©ricas\n",
    "numeric_continuous = []\n",
    "numeric_discrete = []\n",
    "\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    if col in id_columns or col == target_column:\n",
    "        continue\n",
    "    \n",
    "    # Heur√≠stica: si tiene menos de 10 valores √∫nicos, probablemente es discreta\n",
    "    unique_count = df[col].nunique()\n",
    "    if unique_count < 10:\n",
    "        numeric_discrete.append(col)\n",
    "    else:\n",
    "        numeric_continuous.append(col)\n",
    "\n",
    "# Clasificar variables categ√≥ricas\n",
    "categorical_nominal = []\n",
    "categorical_ordinal = []\n",
    "categorical_binary = []\n",
    "\n",
    "# Palabras clave que sugieren orden\n",
    "ordinal_keywords = ['severity', 'stage', 'level', 'grade', 'quality', 'score', 'rating']\n",
    "\n",
    "for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "    if col in id_columns or col == target_column:\n",
    "        continue\n",
    "    \n",
    "    unique_count = df[col].nunique()\n",
    "    \n",
    "    # Variables binarias\n",
    "    if unique_count == 2:\n",
    "        categorical_binary.append(col)\n",
    "    # Variables ordinales (basado en palabras clave)\n",
    "    elif any(keyword in col.lower() for keyword in ordinal_keywords):\n",
    "        categorical_ordinal.append(col)\n",
    "    # Variables nominales\n",
    "    else:\n",
    "        categorical_nominal.append(col)\n",
    "\n",
    "# Variables num√©ricas que son realmente categ√≥ricas binarias (0/1)\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    if col not in id_columns and col != target_column:\n",
    "        if df[col].nunique() == 2 and set(df[col].unique()).issubset({0, 1, np.nan}):\n",
    "            # Mover de num√©rica a binaria\n",
    "            if col in numeric_discrete:\n",
    "                numeric_discrete.remove(col)\n",
    "            categorical_binary.append(col)\n",
    "\n",
    "print(\"\\nüìä RESUMEN DE CLASIFICACI√ìN:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Total de variables: {len(df.columns)}\")\n",
    "print(f\"\\nüéØ Variable Objetivo: {target_column if target_column else 'No identificada'}\")\n",
    "print(f\"üî¢ Variables de Identificaci√≥n (a eliminar): {len(id_columns)}\")\n",
    "if id_columns:\n",
    "    for col in id_columns:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\nüìà VARIABLES NUM√âRICAS ({len(numeric_continuous) + len(numeric_discrete)}):\")\n",
    "print(f\"   ‚Ä¢ Continuas: {len(numeric_continuous)}\")\n",
    "if numeric_continuous:\n",
    "    for col in numeric_continuous[:10]:  # Mostrar m√°ximo 10\n",
    "        print(f\"      - {col}\")\n",
    "    if len(numeric_continuous) > 10:\n",
    "        print(f\"      ... y {len(numeric_continuous) - 10} m√°s\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Discretas: {len(numeric_discrete)}\")\n",
    "if numeric_discrete:\n",
    "    for col in numeric_discrete:\n",
    "        print(f\"      - {col}\")\n",
    "\n",
    "print(f\"\\nüìä VARIABLES CATEG√ìRICAS ({len(categorical_binary) + len(categorical_nominal) + len(categorical_ordinal)}):\")\n",
    "print(f\"   ‚Ä¢ Binarias: {len(categorical_binary)}\")\n",
    "if categorical_binary:\n",
    "    for col in categorical_binary[:10]:\n",
    "        print(f\"      - {col}\")\n",
    "    if len(categorical_binary) > 10:\n",
    "        print(f\"      ... y {len(categorical_binary) - 10} m√°s\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Nominales: {len(categorical_nominal)}\")\n",
    "if categorical_nominal:\n",
    "    for col in categorical_nominal:\n",
    "        print(f\"      - {col}\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Ordinales: {len(categorical_ordinal)}\")\n",
    "if categorical_ordinal:\n",
    "    for col in categorical_ordinal:\n",
    "        print(f\"      - {col}\")\n",
    "\n",
    "# Guardar clasificaci√≥n para uso posterior\n",
    "variable_classification = {\n",
    "    'target': target_column,\n",
    "    'id_columns': id_columns,\n",
    "    'numeric_continuous': numeric_continuous,\n",
    "    'numeric_discrete': numeric_discrete,\n",
    "    'categorical_binary': categorical_binary,\n",
    "    'categorical_nominal': categorical_nominal,\n",
    "    'categorical_ordinal': categorical_ordinal\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Clasificaci√≥n completada y guardada en 'variable_classification'\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4d495",
   "metadata": {},
   "source": [
    "## 2. Informaci√≥n General del Dataset\n",
    "\n",
    "Inspecci√≥n de tipos de datos, memoria utilizada y estructura general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b464fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INFORMACI√ìN GENERAL DEL DATASET\n",
      "================================================================================\n",
      "\n",
      "Dimensiones: 2149 filas √ó 35 columnas\n",
      "Memoria utilizada: 0.69 MB\n",
      "\n",
      "TIPOS DE DATOS:\n",
      "--------------------------------------------------------------------------------\n",
      "                  Columna    Tipo  No Nulos  Nulos  % Nulos\n",
      "                PatientID   int64      2149      0      0.0\n",
      "                      Age   int64      2149      0      0.0\n",
      "                   Gender   int64      2149      0      0.0\n",
      "                Ethnicity   int64      2149      0      0.0\n",
      "           EducationLevel   int64      2149      0      0.0\n",
      "                      BMI float64      2149      0      0.0\n",
      "                  Smoking   int64      2149      0      0.0\n",
      "       AlcoholConsumption float64      2149      0      0.0\n",
      "         PhysicalActivity float64      2149      0      0.0\n",
      "              DietQuality float64      2149      0      0.0\n",
      "             SleepQuality float64      2149      0      0.0\n",
      "  FamilyHistoryAlzheimers   int64      2149      0      0.0\n",
      "    CardiovascularDisease   int64      2149      0      0.0\n",
      "                 Diabetes   int64      2149      0      0.0\n",
      "               Depression   int64      2149      0      0.0\n",
      "               HeadInjury   int64      2149      0      0.0\n",
      "             Hypertension   int64      2149      0      0.0\n",
      "               SystolicBP   int64      2149      0      0.0\n",
      "              DiastolicBP   int64      2149      0      0.0\n",
      "         CholesterolTotal float64      2149      0      0.0\n",
      "           CholesterolLDL float64      2149      0      0.0\n",
      "           CholesterolHDL float64      2149      0      0.0\n",
      " CholesterolTriglycerides float64      2149      0      0.0\n",
      "                     MMSE float64      2149      0      0.0\n",
      "     FunctionalAssessment float64      2149      0      0.0\n",
      "         MemoryComplaints   int64      2149      0      0.0\n",
      "       BehavioralProblems   int64      2149      0      0.0\n",
      "                      ADL float64      2149      0      0.0\n",
      "                Confusion   int64      2149      0      0.0\n",
      "           Disorientation   int64      2149      0      0.0\n",
      "       PersonalityChanges   int64      2149      0      0.0\n",
      "DifficultyCompletingTasks   int64      2149      0      0.0\n",
      "            Forgetfulness   int64      2149      0      0.0\n",
      "                Diagnosis   int64      2149      0      0.0\n",
      "           DoctorInCharge  object      2149      0      0.0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Crear resumen de informaci√≥n\n",
    "print(\"=\"*80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDimensiones: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "\n",
    "print(\"TIPOS DE DATOS:\")\n",
    "print(\"-\" * 80)\n",
    "dtype_info = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Tipo': df.dtypes,\n",
    "    'No Nulos': df.count(),\n",
    "    'Nulos': df.isnull().sum(),\n",
    "    '% Nulos': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "print(dtype_info.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4655b1fa",
   "metadata": {},
   "source": [
    "## 3. Estad√≠sticas Descriptivas\n",
    "\n",
    "An√°lisis estad√≠stico autom√°tico seg√∫n el tipo de variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b08e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ESTAD√çSTICAS NUM√âRICAS\n",
      "================================================================================\n",
      "\n",
      "                            count      mean      std       min       25%  \\\n",
      "PatientID                  2149.0  5825.000  620.507  4751.000  5288.000   \n",
      "Age                        2149.0    74.909    8.990    60.000    67.000   \n",
      "Gender                     2149.0     0.506    0.500     0.000     0.000   \n",
      "Ethnicity                  2149.0     0.698    0.996     0.000     0.000   \n",
      "EducationLevel             2149.0     1.287    0.905     0.000     1.000   \n",
      "BMI                        2149.0    27.656    7.217    15.009    21.611   \n",
      "Smoking                    2149.0     0.289    0.453     0.000     0.000   \n",
      "AlcoholConsumption         2149.0    10.039    5.758     0.002     5.140   \n",
      "PhysicalActivity           2149.0     4.920    2.857     0.004     2.571   \n",
      "DietQuality                2149.0     4.993    2.909     0.009     2.458   \n",
      "SleepQuality               2149.0     7.051    1.764     4.003     5.483   \n",
      "FamilyHistoryAlzheimers    2149.0     0.252    0.434     0.000     0.000   \n",
      "CardiovascularDisease      2149.0     0.144    0.351     0.000     0.000   \n",
      "Diabetes                   2149.0     0.151    0.358     0.000     0.000   \n",
      "Depression                 2149.0     0.201    0.401     0.000     0.000   \n",
      "HeadInjury                 2149.0     0.093    0.290     0.000     0.000   \n",
      "Hypertension               2149.0     0.149    0.356     0.000     0.000   \n",
      "SystolicBP                 2149.0   134.265   25.949    90.000   112.000   \n",
      "DiastolicBP                2149.0    89.848   17.592    60.000    74.000   \n",
      "CholesterolTotal           2149.0   225.198   42.542   150.093   190.253   \n",
      "CholesterolLDL             2149.0   124.336   43.367    50.231    87.196   \n",
      "CholesterolHDL             2149.0    59.464   23.139    20.003    39.096   \n",
      "CholesterolTriglycerides   2149.0   228.281  101.987    50.407   137.583   \n",
      "MMSE                       2149.0    14.755    8.613     0.005     7.168   \n",
      "FunctionalAssessment       2149.0     5.080    2.893     0.000     2.566   \n",
      "MemoryComplaints           2149.0     0.208    0.406     0.000     0.000   \n",
      "BehavioralProblems         2149.0     0.157    0.364     0.000     0.000   \n",
      "ADL                        2149.0     4.983    2.950     0.001     2.343   \n",
      "Confusion                  2149.0     0.205    0.404     0.000     0.000   \n",
      "Disorientation             2149.0     0.158    0.365     0.000     0.000   \n",
      "PersonalityChanges         2149.0     0.151    0.358     0.000     0.000   \n",
      "DifficultyCompletingTasks  2149.0     0.159    0.365     0.000     0.000   \n",
      "Forgetfulness              2149.0     0.302    0.459     0.000     0.000   \n",
      "Diagnosis                  2149.0     0.354    0.478     0.000     0.000   \n",
      "\n",
      "                                50%       75%       max     Rango  Asimetr√≠a  \\\n",
      "PatientID                  5825.000  6362.000  6899.000  2148.000      0.000   \n",
      "Age                          75.000    83.000    90.000    30.000      0.046   \n",
      "Gender                        1.000     1.000     1.000     1.000     -0.025   \n",
      "Ethnicity                     0.000     1.000     3.000     3.000      1.230   \n",
      "EducationLevel                1.000     2.000     3.000     3.000      0.209   \n",
      "BMI                          27.824    33.870    39.993    24.984     -0.027   \n",
      "Smoking                       0.000     1.000     1.000     1.000      0.934   \n",
      "AlcoholConsumption            9.934    15.158    19.989    19.987      0.018   \n",
      "PhysicalActivity              4.766     7.428     9.987     9.984      0.045   \n",
      "DietQuality                   5.076     7.559     9.998     9.989     -0.012   \n",
      "SleepQuality                  7.116     8.563    10.000     5.997     -0.070   \n",
      "FamilyHistoryAlzheimers       0.000     1.000     1.000     1.000      1.142   \n",
      "CardiovascularDisease         0.000     0.000     1.000     1.000      2.026   \n",
      "Diabetes                      0.000     0.000     1.000     1.000      1.953   \n",
      "Depression                    0.000     0.000     1.000     1.000      1.497   \n",
      "HeadInjury                    0.000     0.000     1.000     1.000      2.813   \n",
      "Hypertension                  0.000     0.000     1.000     1.000      1.974   \n",
      "SystolicBP                  134.000   157.000   179.000    89.000      0.010   \n",
      "DiastolicBP                  91.000   105.000   119.000    59.000     -0.054   \n",
      "CholesterolTotal            225.086   262.032   299.993   149.900     -0.019   \n",
      "CholesterolLDL              123.343   161.734   199.966   149.735      0.036   \n",
      "CholesterolHDL               59.768    78.939    99.980    79.977      0.042   \n",
      "CholesterolTriglycerides    230.302   314.839   399.942   349.535     -0.033   \n",
      "MMSE                         14.442    22.161    29.991    29.986      0.032   \n",
      "FunctionalAssessment          5.094     7.547     9.996     9.996     -0.035   \n",
      "MemoryComplaints              0.000     0.000     1.000     1.000      1.440   \n",
      "BehavioralProblems            0.000     0.000     1.000     1.000      1.889   \n",
      "ADL                           5.039     7.581    10.000     9.998     -0.030   \n",
      "Confusion                     0.000     0.000     1.000     1.000      1.461   \n",
      "Disorientation                0.000     0.000     1.000     1.000      1.874   \n",
      "PersonalityChanges            0.000     0.000     1.000     1.000      1.953   \n",
      "DifficultyCompletingTasks     0.000     0.000     1.000     1.000      1.870   \n",
      "Forgetfulness                 0.000     1.000     1.000     1.000      0.866   \n",
      "Diagnosis                     0.000     1.000     1.000     1.000      0.613   \n",
      "\n",
      "                           Curtosis  \n",
      "PatientID                    -1.200  \n",
      "Age                          -1.189  \n",
      "Gender                       -2.001  \n",
      "Ethnicity                     0.234  \n",
      "EducationLevel               -0.753  \n",
      "BMI                          -1.185  \n",
      "Smoking                      -1.128  \n",
      "AlcoholConsumption           -1.203  \n",
      "PhysicalActivity             -1.179  \n",
      "DietQuality                  -1.229  \n",
      "SleepQuality                 -1.212  \n",
      "FamilyHistoryAlzheimers      -0.697  \n",
      "CardiovascularDisease         2.109  \n",
      "Diabetes                      1.817  \n",
      "Depression                    0.240  \n",
      "HeadInjury                    5.918  \n",
      "Hypertension                  1.898  \n",
      "SystolicBP                   -1.198  \n",
      "DiastolicBP                  -1.235  \n",
      "CholesterolTotal             -1.156  \n",
      "CholesterolLDL               -1.208  \n",
      "CholesterolHDL               -1.218  \n",
      "CholesterolTriglycerides     -1.219  \n",
      "MMSE                         -1.230  \n",
      "FunctionalAssessment         -1.183  \n",
      "MemoryComplaints              0.073  \n",
      "BehavioralProblems            1.569  \n",
      "ADL                          -1.250  \n",
      "Confusion                     0.134  \n",
      "Disorientation                1.515  \n",
      "PersonalityChanges            1.817  \n",
      "DifficultyCompletingTasks     1.497  \n",
      "Forgetfulness                -1.252  \n",
      "Diagnosis                    -1.626  \n",
      "\n",
      "================================================================================\n",
      "ESTAD√çSTICAS CATEG√ìRICAS\n",
      "================================================================================\n",
      "\n",
      "üìä DoctorInCharge\n",
      "   Valores √∫nicos: 1\n",
      "   M√°s frecuente: XXXConfid (2149 veces)\n",
      "   Distribuci√≥n:\n",
      "XXXConfid    2149\n",
      "Name: DoctorInCharge, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estad√≠sticas para variables num√©ricas\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTAD√çSTICAS NUM√âRICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if numeric_cols:\n",
    "    stats = df[numeric_cols].describe().T\n",
    "    stats['Rango'] = df[numeric_cols].max() - df[numeric_cols].min()\n",
    "    stats['Asimetr√≠a'] = df[numeric_cols].skew()\n",
    "    stats['Curtosis'] = df[numeric_cols].kurtosis()\n",
    "    print(stats.round(3))\n",
    "else:\n",
    "    print(\"No hay variables num√©ricas en el dataset.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTAD√çSTICAS CATEG√ìRICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "if categorical_cols:\n",
    "    for col in categorical_cols:\n",
    "        print(f\"üìä {col}\")\n",
    "        print(f\"   Valores √∫nicos: {df[col].nunique()}\")\n",
    "        print(f\"   M√°s frecuente: {df[col].value_counts().index[0]} ({df[col].value_counts().values[0]} veces)\")\n",
    "        print(f\"   Distribuci√≥n:\\n{df[col].value_counts()}\\n\")\n",
    "else:\n",
    "    print(\"No hay variables categ√≥ricas en el dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96681126",
   "metadata": {},
   "source": [
    "## 4. An√°lisis de Valores Faltantes\n",
    "\n",
    "Detecci√≥n y visualizaci√≥n de datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767e625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS DE VALORES FALTANTES\n",
      "================================================================================\n",
      "\n",
      "‚úì No hay valores faltantes en el dataset\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Nulos': df.isnull().sum(),\n",
    "    '% Nulos': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "    'Tipo': df.dtypes\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Nulos'] > 0].sort_values('% Nulos', ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"Columnas con valores faltantes:\")\n",
    "    print(missing_data.to_string(index=False))\n",
    "    \n",
    "    # Visualizar patrones de valores faltantes\n",
    "    if len(missing_data) <= 10:\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        missing_data_sorted = missing_data.sort_values('% Nulos', ascending=True)\n",
    "        ax.barh(missing_data_sorted['Columna'], missing_data_sorted['% Nulos'], color='coral')\n",
    "        ax.set_xlabel('% de Valores Faltantes')\n",
    "        ax.set_title('Distribuci√≥n de Valores Faltantes')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚úì No hay valores faltantes en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fca57",
   "metadata": {},
   "source": [
    "## 4.5 Unificaci√≥n de Representaciones de Valores Nulos\n",
    "\n",
    "Estandarizaci√≥n de distintas representaciones de valores faltantes a un formato √∫nico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef57a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "UNIFICACI√ìN DE VALORES NULOS\n",
      "================================================================================\n",
      "\n",
      "üîç Buscando representaciones alternativas de valores nulos...\n",
      "   Representaciones buscadas: 20\n",
      "\n",
      "‚úì No se encontraron representaciones alternativas de valores nulos\n",
      "  Todos los valores nulos ya est√°n en formato est√°ndar (NaN)\n",
      "\n",
      "üí° Recomendaci√≥n:\n",
      "   En el preprocesamiento, asegurar que todas las fuentes de datos\n",
      "   usen un formato consistente para valores nulos (preferiblemente NaN/NULL)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Unificar representaciones de valores nulos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIFICACI√ìN DE VALORES NULOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Valores que representan \"nulo\" en diferentes formatos\n",
    "null_representations = ['NA', 'N/A', 'na', 'n/a', 'NULL', 'null', 'None', 'none', \n",
    "                        '', ' ', '  ', 'NaN', 'nan', 'missing', 'Missing', '-', '--', '?', 'unknown', 'Unknown']\n",
    "\n",
    "print(\"üîç Buscando representaciones alternativas de valores nulos...\")\n",
    "print(f\"   Representaciones buscadas: {len(null_representations)}\")\n",
    "\n",
    "# Analizar cada columna\n",
    "unification_report = []\n",
    "total_unified = 0\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':  # Solo en columnas de texto\n",
    "        # Contar valores antes\n",
    "        nulls_before = df[col].isnull().sum()\n",
    "        \n",
    "        # Reemplazar representaciones alternativas con NaN\n",
    "        df[col] = df[col].replace(null_representations, np.nan)\n",
    "        \n",
    "        # Tambi√©n remover strings que son solo espacios\n",
    "        df[col] = df[col].apply(lambda x: np.nan if isinstance(x, str) and x.strip() == '' else x)\n",
    "        \n",
    "        # Contar valores despu√©s\n",
    "        nulls_after = df[col].isnull().sum()\n",
    "        \n",
    "        # Si hubo cambios, reportar\n",
    "        if nulls_after > nulls_before:\n",
    "            unified_count = nulls_after - nulls_before\n",
    "            total_unified += unified_count\n",
    "            unification_report.append({\n",
    "                'Columna': col,\n",
    "                'Nulos Antes': nulls_before,\n",
    "                'Nulos Despu√©s': nulls_after,\n",
    "                'Unificados': unified_count\n",
    "            })\n",
    "\n",
    "if unification_report:\n",
    "    print(f\"\\n‚úÖ Se unificaron {total_unified} valores nulos en {len(unification_report)} columna(s):\\n\")\n",
    "    \n",
    "    unification_df = pd.DataFrame(unification_report)\n",
    "    print(unification_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nüìä Total de valores unificados: {total_unified}\")\n",
    "    print(f\"   Porcentaje del dataset: {(total_unified / (df.shape[0] * df.shape[1]) * 100):.3f}%\")\n",
    "else:\n",
    "    print(\"\\n‚úì No se encontraron representaciones alternativas de valores nulos\")\n",
    "    print(\"  Todos los valores nulos ya est√°n en formato est√°ndar (NaN)\")\n",
    "\n",
    "print(\"\\nüí° Recomendaci√≥n:\")\n",
    "print(\"   En el preprocesamiento, asegurar que todas las fuentes de datos\")\n",
    "print(\"   usen un formato consistente para valores nulos (preferiblemente NaN/NULL)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df020f9",
   "metadata": {},
   "source": [
    "## 4.6 Identificaci√≥n y Eliminaci√≥n de Variables Irrelevantes\n",
    "\n",
    "Detecci√≥n de columnas sin valor anal√≠tico (IDs, columnas constantes, alta cardinalidad sin informaci√≥n).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06210a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDENTIFICACI√ìN DE VARIABLES IRRELEVANTES\n",
      "================================================================================\n",
      "\n",
      "üîç VARIABLES IRRELEVANTES IDENTIFICADAS: 3\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚ùå PatientID\n",
      "   Raz√≥n: Columna de identificaci√≥n (sin valor predictivo)\n",
      "   Tipo: int64\n",
      "   Valores √∫nicos: 2149\n",
      "   Valores nulos: 0 (0.0%)\n",
      "\n",
      "‚ùå CholesterolTriglycerides\n",
      "   Raz√≥n: Columna de identificaci√≥n (sin valor predictivo)\n",
      "   Tipo: float64\n",
      "   Valores √∫nicos: 2149\n",
      "   Valores nulos: 0 (0.0%)\n",
      "\n",
      "‚ùå DoctorInCharge\n",
      "   Raz√≥n: Columna de identificaci√≥n (sin valor predictivo)\n",
      "   Tipo: object\n",
      "   Valores √∫nicos: 1\n",
      "   Valores nulos: 0 (0.0%)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Se identificaron 3 variable(s) irrelevante(s)\n",
      "üìä Dataset original: 2149 filas √ó 35 columnas\n",
      "üìä Dataset limpio: 2149 filas √ó 32 columnas\n",
      "   Columnas eliminadas: 3\n",
      "\n",
      "üí° Recomendaci√≥n:\n",
      "   Estas columnas deben eliminarse en el preprocesamiento de datos\n",
      "   antes de construir modelos de Machine Learning.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Identificar y eliminar variables irrelevantes\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICACI√ìN DE VARIABLES IRRELEVANTES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "irrelevant_columns = []\n",
    "irrelevant_reasons = {}\n",
    "\n",
    "# 1. Columnas de identificaci√≥n (sin valor predictivo)\n",
    "# Solo identificar por palabras clave expl√≠citas en el nombre\n",
    "id_keywords = ['id', 'index', 'patient', 'doctor', 'uid', 'key', 'code']\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in id_keywords):\n",
    "        if col not in irrelevant_columns:\n",
    "            irrelevant_columns.append(col)\n",
    "            irrelevant_reasons[col] = \"Columna de identificaci√≥n (sin valor predictivo)\"\n",
    "\n",
    "# 2. Columnas con un solo valor √∫nico (constantes)\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 1:\n",
    "        if col not in irrelevant_columns:\n",
    "            irrelevant_columns.append(col)\n",
    "            irrelevant_reasons[col] = f\"Columna constante (1 √∫nico valor: {df[col].unique()[0]})\"\n",
    "\n",
    "# 3. Columnas con casi todos valores nulos (>95%)\n",
    "null_threshold = 0.95\n",
    "for col in df.columns:\n",
    "    null_pct = df[col].isnull().sum() / len(df)\n",
    "    if null_pct > null_threshold:\n",
    "        if col not in irrelevant_columns:\n",
    "            irrelevant_columns.append(col)\n",
    "            irrelevant_reasons[col] = f\"Exceso de nulos ({null_pct*100:.1f}% faltantes)\"\n",
    "\n",
    "# 4. Columnas categ√≥ricas con cardinalidad extremadamente alta (>90% valores √∫nicos)\n",
    "# Estas podr√≠an ser texto libre o IDs enmascarados\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col not in irrelevant_columns:\n",
    "        unique_ratio = df[col].nunique() / len(df)\n",
    "        if unique_ratio > 0.9:\n",
    "            irrelevant_columns.append(col)\n",
    "            irrelevant_reasons[col] = f\"Cardinalidad muy alta - Posible ID/Texto libre ({df[col].nunique()} valores √∫nicos, {unique_ratio*100:.1f}% del dataset)\"\n",
    "\n",
    "print(f\"üîç VARIABLES IRRELEVANTES IDENTIFICADAS: {len(irrelevant_columns)}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if irrelevant_columns:\n",
    "    for col in irrelevant_columns:\n",
    "        print(f\"\\n‚ùå {col}\")\n",
    "        print(f\"   Raz√≥n: {irrelevant_reasons[col]}\")\n",
    "        print(f\"   Tipo: {df[col].dtype}\")\n",
    "        print(f\"   Valores √∫nicos: {df[col].nunique()}\")\n",
    "        print(f\"   Valores nulos: {df[col].isnull().sum()} ({df[col].isnull().sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Crear dataframe limpio (sin eliminar del original para preservar an√°lisis)\n",
    "    df_clean = df.drop(columns=irrelevant_columns)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"‚úÖ Se identificaron {len(irrelevant_columns)} variable(s) irrelevante(s)\")\n",
    "    print(f\"üìä Dataset original: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "    print(f\"üìä Dataset limpio: {df_clean.shape[0]} filas √ó {df_clean.shape[1]} columnas\")\n",
    "    print(f\"   Columnas eliminadas: {len(irrelevant_columns)}\")\n",
    "    \n",
    "    print(\"\\nüí° Recomendaci√≥n:\")\n",
    "    print(\"   Estas columnas deben eliminarse en el preprocesamiento de datos\")\n",
    "    print(\"   antes de construir modelos de Machine Learning.\")\n",
    "    \n",
    "    # Guardar lista para uso posterior\n",
    "    columns_to_remove = irrelevant_columns\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚úì No se encontraron variables irrelevantes en el dataset\")\n",
    "    print(\"  Todas las columnas parecen tener valor anal√≠tico potencial\")\n",
    "    df_clean = df.copy()\n",
    "    columns_to_remove = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a84a76",
   "metadata": {},
   "source": [
    "## 4.7 Conversi√≥n y Correcci√≥n de Tipos de Datos\n",
    "\n",
    "Detecci√≥n y correcci√≥n de columnas con tipos de datos incorrectos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a46f4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSI√ìN Y CORRECCI√ìN DE TIPOS DE DATOS\n",
      "================================================================================\n",
      "\n",
      "üîç Analizando tipos de datos actuales...\n",
      "\n",
      "üîÑ CONVERSIONES DE TIPOS REALIZADAS: 32\n",
      "--------------------------------------------------------------------------------\n",
      "                  Columna Tipo Anterior Tipo Nuevo                                                            Raz√≥n\n",
      "                   Gender       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "                  Smoking       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "  FamilyHistoryAlzheimers       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "    CardiovascularDisease       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "                 Diabetes       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "               Depression       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "               HeadInjury       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "             Hypertension       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "         MemoryComplaints       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "       BehavioralProblems       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "                Confusion       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "           Disorientation       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "       PersonalityChanges       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "DifficultyCompletingTasks       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "            Forgetfulness       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "                Diagnosis       numeric   category                   Variable binaria (0/1) - mejor como categ√≥rica\n",
      "                Ethnicity       numeric   category Variable discreta con 4 valores √∫nicos - posiblemente categ√≥rica\n",
      "           EducationLevel       numeric   category Variable discreta con 4 valores √∫nicos - posiblemente categ√≥rica\n",
      "                      Age         int64       int8                               Optimizaci√≥n de memoria (downcast)\n",
      "               SystolicBP         int64      int16                               Optimizaci√≥n de memoria (downcast)\n",
      "              DiastolicBP         int64       int8                               Optimizaci√≥n de memoria (downcast)\n",
      "                      BMI       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "       AlcoholConsumption       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "         PhysicalActivity       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "              DietQuality       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "             SleepQuality       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "         CholesterolTotal       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "           CholesterolLDL       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "           CholesterolHDL       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "                     MMSE       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "     FunctionalAssessment       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "                      ADL       float64    float32                               Optimizaci√≥n de memoria (downcast)\n",
      "\n",
      "üìä IMPACTO EN MEMORIA:\n",
      "   Memoria antes: 0.69 MB\n",
      "   Memoria despu√©s: 0.14 MB\n",
      "   Ahorro: 0.56 MB (80.1%)\n",
      "\n",
      "üí° Recomendaci√≥n:\n",
      "   Aplicar estas conversiones al inicio del pipeline de preprocesamiento\n",
      "   para asegurar consistencia en el an√°lisis y modelado.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detecci√≥n y correcci√≥n de tipos de datos incorrectos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERSI√ìN Y CORRECCI√ìN DE TIPOS DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "type_conversions = []\n",
    "\n",
    "# Trabajar con df_clean si existe, sino con df\n",
    "df_to_fix = df_clean.copy() if 'df_clean' in locals() else df.copy()\n",
    "\n",
    "print(\"üîç Analizando tipos de datos actuales...\\n\")\n",
    "\n",
    "# 1. Detectar n√∫meros almacenados como strings\n",
    "for col in df_to_fix.select_dtypes(include=['object']).columns:\n",
    "    # Intentar convertir a num√©rico\n",
    "    try:\n",
    "        # Verificar si la columna parece num√©rica\n",
    "        sample = df_to_fix[col].dropna().head(100)\n",
    "        \n",
    "        # Intentar conversi√≥n\n",
    "        converted = pd.to_numeric(sample, errors='coerce')\n",
    "        \n",
    "        # Si >80% se convirti√≥ exitosamente, probablemente es num√©rica\n",
    "        success_rate = converted.notna().sum() / len(sample)\n",
    "        \n",
    "        if success_rate > 0.8:\n",
    "            df_to_fix[col] = pd.to_numeric(df_to_fix[col], errors='coerce')\n",
    "            type_conversions.append({\n",
    "                'Columna': col,\n",
    "                'Tipo Anterior': 'object',\n",
    "                'Tipo Nuevo': 'numeric',\n",
    "                'Raz√≥n': f'Columna num√©rica almacenada como texto ({success_rate*100:.0f}% conversi√≥n exitosa)'\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 2. Detectar variables binarias que deber√≠an ser categ√≥ricas\n",
    "for col in df_to_fix.select_dtypes(include=[np.number]).columns:\n",
    "    unique_values = df_to_fix[col].dropna().unique()\n",
    "    \n",
    "    if len(unique_values) == 2:\n",
    "        # Verificar si son 0/1 o similares\n",
    "        if set(unique_values).issubset({0, 1, 0.0, 1.0}):\n",
    "            df_to_fix[col] = df_to_fix[col].astype('category')\n",
    "            type_conversions.append({\n",
    "                'Columna': col,\n",
    "                'Tipo Anterior': 'numeric',\n",
    "                'Tipo Nuevo': 'category',\n",
    "                'Raz√≥n': 'Variable binaria (0/1) - mejor como categ√≥rica'\n",
    "            })\n",
    "\n",
    "# 3. Detectar variables categ√≥ricas con pocos valores √∫nicos\n",
    "for col in df_to_fix.select_dtypes(include=[np.number]).columns:\n",
    "    if col not in [c['Columna'] for c in type_conversions]:  # Si no fue ya convertida\n",
    "        unique_count = df_to_fix[col].nunique()\n",
    "        \n",
    "        # Si tiene menos de 10 valores √∫nicos y parecen ser c√≥digos\n",
    "        if unique_count < 10 and unique_count > 2:\n",
    "            # Verificar si son enteros consecutivos o c√≥digos\n",
    "            unique_values = sorted(df_to_fix[col].dropna().unique())\n",
    "            \n",
    "            # Si son enteros peque√±os, probablemente categ√≥rica\n",
    "            if all(isinstance(x, (int, np.integer)) or x.is_integer() for x in unique_values):\n",
    "                if max(unique_values) < 20:  # C√≥digos peque√±os\n",
    "                    df_to_fix[col] = df_to_fix[col].astype('category')\n",
    "                    type_conversions.append({\n",
    "                        'Columna': col,\n",
    "                        'Tipo Anterior': 'numeric',\n",
    "                        'Tipo Nuevo': 'category',\n",
    "                        'Raz√≥n': f'Variable discreta con {unique_count} valores √∫nicos - posiblemente categ√≥rica'\n",
    "                    })\n",
    "\n",
    "# 4. Optimizar tipos num√©ricos (downcast para ahorrar memoria)\n",
    "for col in df_to_fix.select_dtypes(include=['int64', 'int32']).columns:\n",
    "    if col not in [c['Columna'] for c in type_conversions]:\n",
    "        # Intentar downcast a int m√°s peque√±o\n",
    "        original_dtype = df_to_fix[col].dtype\n",
    "        df_to_fix[col] = pd.to_numeric(df_to_fix[col], downcast='integer')\n",
    "        \n",
    "        if df_to_fix[col].dtype != original_dtype:\n",
    "            type_conversions.append({\n",
    "                'Columna': col,\n",
    "                'Tipo Anterior': str(original_dtype),\n",
    "                'Tipo Nuevo': str(df_to_fix[col].dtype),\n",
    "                'Raz√≥n': 'Optimizaci√≥n de memoria (downcast)'\n",
    "            })\n",
    "\n",
    "for col in df_to_fix.select_dtypes(include=['float64', 'float32']).columns:\n",
    "    if col not in [c['Columna'] for c in type_conversions]:\n",
    "        # Intentar downcast a float m√°s peque√±o\n",
    "        original_dtype = df_to_fix[col].dtype\n",
    "        df_to_fix[col] = pd.to_numeric(df_to_fix[col], downcast='float')\n",
    "        \n",
    "        if df_to_fix[col].dtype != original_dtype:\n",
    "            type_conversions.append({\n",
    "                'Columna': col,\n",
    "                'Tipo Anterior': str(original_dtype),\n",
    "                'Tipo Nuevo': str(df_to_fix[col].dtype),\n",
    "                'Raz√≥n': 'Optimizaci√≥n de memoria (downcast)'\n",
    "            })\n",
    "\n",
    "print(f\"üîÑ CONVERSIONES DE TIPOS REALIZADAS: {len(type_conversions)}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if type_conversions:\n",
    "    conversions_df = pd.DataFrame(type_conversions)\n",
    "    print(conversions_df.to_string(index=False))\n",
    "    \n",
    "    # Calcular ahorro de memoria\n",
    "    memory_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_after = df_to_fix.memory_usage(deep=True).sum() / 1024**2\n",
    "    memory_saved = memory_before - memory_after\n",
    "    \n",
    "    print(f\"\\nüìä IMPACTO EN MEMORIA:\")\n",
    "    print(f\"   Memoria antes: {memory_before:.2f} MB\")\n",
    "    print(f\"   Memoria despu√©s: {memory_after:.2f} MB\")\n",
    "    print(f\"   Ahorro: {memory_saved:.2f} MB ({(memory_saved/memory_before*100):.1f}%)\")\n",
    "    \n",
    "    # Actualizar dataframe limpio\n",
    "    df_clean = df_to_fix\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚úì Todos los tipos de datos son correctos\")\n",
    "    print(\"  No se requieren conversiones\")\n",
    "\n",
    "print(\"\\nüí° Recomendaci√≥n:\")\n",
    "print(\"   Aplicar estas conversiones al inicio del pipeline de preprocesamiento\")\n",
    "print(\"   para asegurar consistencia en el an√°lisis y modelado.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd433e2",
   "metadata": {},
   "source": [
    "## 4.8 Detecci√≥n y Correcci√≥n de Inconsistencias en Datos\n",
    "\n",
    "Identificaci√≥n de duplicados, espacios, inconsistencias de formato y valores imposibles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e510444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETECCI√ìN Y CORRECCI√ìN DE INCONSISTENCIAS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ AN√ÅLISIS DE DUPLICADOS:\n",
      "--------------------------------------------------------------------------------\n",
      "   Filas duplicadas (completas): 0\n",
      "   ‚úì No hay filas duplicadas\n",
      "\n",
      "2Ô∏è‚É£ AN√ÅLISIS DE ESPACIOS EN STRINGS:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚úì No hay problemas de espacios en strings\n",
      "\n",
      "3Ô∏è‚É£ AN√ÅLISIS DE INCONSISTENCIAS DE FORMATO:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚úì No hay inconsistencias de formato en variables categ√≥ricas\n",
      "\n",
      "5Ô∏è‚É£ AN√ÅLISIS DE RELACIONES L√ìGICAS:\n",
      "--------------------------------------------------------------------------------\n",
      "   ‚ö†Ô∏è 186 casos donde Systolic BP ‚â§ Diastolic BP (imposible)\n",
      "   ‚ö†Ô∏è 238 casos donde LDL+HDL > Colesterol Total (inconsistente)\n",
      "\n",
      "   üí° Se identificaron 2 tipo(s) de inconsistencias l√≥gicas\n",
      "\n",
      "================================================================================\n",
      "üìä RESUMEN DE INCONSISTENCIAS:\n",
      "================================================================================\n",
      "\n",
      "Total de problemas identificados: 2\n",
      "\n",
      "1. Presi√≥n sist√≥lica ‚â§ diast√≥lica: 186 casos\n",
      "2. LDL+HDL > Total colesterol: 238 casos\n",
      "\n",
      "‚úÖ Dataset corregido guardado como 'df_clean'\n",
      "   Filas originales: 2149\n",
      "   Filas despu√©s de limpieza: 2149\n",
      "   Filas eliminadas: 0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detecci√≥n y correcci√≥n de inconsistencias\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETECCI√ìN Y CORRECCI√ìN DE INCONSISTENCIAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "inconsistencies_found = []\n",
    "\n",
    "# Trabajar con df_clean\n",
    "df_to_check = df_clean.copy() if 'df_clean' in locals() else df.copy()\n",
    "\n",
    "# 1. DUPLICADOS\n",
    "print(\"1Ô∏è‚É£ AN√ÅLISIS DE DUPLICADOS:\")\n",
    "print(\"-\"*80)\n",
    "n_duplicates = df_to_check.duplicated().sum()\n",
    "print(f\"   Filas duplicadas (completas): {n_duplicates}\")\n",
    "\n",
    "if n_duplicates > 0:\n",
    "    print(f\"   Porcentaje: {(n_duplicates/len(df_to_check)*100):.2f}%\")\n",
    "    inconsistencies_found.append(f\"Duplicados: {n_duplicates} filas\")\n",
    "    \n",
    "    # Eliminar duplicados\n",
    "    df_to_check = df_to_check.drop_duplicates()\n",
    "    print(f\"   ‚úÖ Duplicados eliminados. Filas restantes: {len(df_to_check)}\")\n",
    "else:\n",
    "    print(\"   ‚úì No hay filas duplicadas\")\n",
    "\n",
    "# 2. ESPACIOS EN BLANCO EN STRINGS\n",
    "print(\"\\n2Ô∏è‚É£ AN√ÅLISIS DE ESPACIOS EN STRINGS:\")\n",
    "print(\"-\"*80)\n",
    "string_issues = []\n",
    "\n",
    "for col in df_to_check.select_dtypes(include=['object']).columns:\n",
    "    # Detectar espacios al inicio/final\n",
    "    if df_to_check[col].dtype == 'object':\n",
    "        has_leading_spaces = df_to_check[col].apply(lambda x: isinstance(x, str) and x != x.strip()).sum()\n",
    "        \n",
    "        if has_leading_spaces > 0:\n",
    "            string_issues.append(f\"{col}: {has_leading_spaces} valores con espacios\")\n",
    "            # Corregir\n",
    "            df_to_check[col] = df_to_check[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "if string_issues:\n",
    "    print(f\"   ‚ö†Ô∏è Se encontraron {len(string_issues)} columna(s) con espacios:\")\n",
    "    for issue in string_issues:\n",
    "        print(f\"      ‚Ä¢ {issue}\")\n",
    "    print(f\"   ‚úÖ Espacios eliminados\")\n",
    "    inconsistencies_found.extend(string_issues)\n",
    "else:\n",
    "    print(\"   ‚úì No hay problemas de espacios en strings\")\n",
    "\n",
    "# 3. INCONSISTENCIAS DE MAY√öSCULAS/MIN√öSCULAS\n",
    "print(\"\\n3Ô∏è‚É£ AN√ÅLISIS DE INCONSISTENCIAS DE FORMATO:\")\n",
    "print(\"-\"*80)\n",
    "case_issues = []\n",
    "\n",
    "for col in df_to_check.select_dtypes(include=['object']).columns:\n",
    "    unique_values = df_to_check[col].dropna().unique()\n",
    "    \n",
    "    if len(unique_values) < 100:  # Solo para columnas categ√≥ricas\n",
    "        # Verificar si hay valores que son iguales excepto por may√∫sculas\n",
    "        lower_values = [str(v).lower() for v in unique_values]\n",
    "        \n",
    "        if len(lower_values) != len(set(lower_values)):\n",
    "            # Hay duplicados en min√∫sculas\n",
    "            case_issues.append(col)\n",
    "            \n",
    "            # Estandarizar a t√≠tulo (capitalizar primera letra)\n",
    "            df_to_check[col] = df_to_check[col].apply(lambda x: str(x).title() if pd.notna(x) else x)\n",
    "\n",
    "if case_issues:\n",
    "    print(f\"   ‚ö†Ô∏è Se encontraron {len(case_issues)} columna(s) con inconsistencias:\")\n",
    "    for col in case_issues:\n",
    "        print(f\"      ‚Ä¢ {col}\")\n",
    "    print(f\"   ‚úÖ Formato estandarizado (Title Case)\")\n",
    "    inconsistencies_found.append(f\"Inconsistencias de formato: {len(case_issues)} columnas\")\n",
    "else:\n",
    "    print(\"   ‚úì No hay inconsistencias de formato en variables categ√≥ricas\")\n",
    "\n",
    "\n",
    "# 4. RELACIONES L√ìGICAS IMPOSIBLES\n",
    "print(\"\\n4Ô∏è‚É£ AN√ÅLISIS DE RELACIONES L√ìGICAS:\")\n",
    "print(\"-\"*80)\n",
    "logical_issues = []\n",
    "\n",
    "# Ejemplo: Systolic BP debe ser mayor que Diastolic BP\n",
    "if 'SystolicBP' in df_to_check.columns and 'DiastolicBP' in df_to_check.columns:\n",
    "    invalid_bp = (df_to_check['SystolicBP'] <= df_to_check['DiastolicBP']).sum()\n",
    "    if invalid_bp > 0:\n",
    "        logical_issues.append(f\"Presi√≥n sist√≥lica ‚â§ diast√≥lica: {invalid_bp} casos\")\n",
    "        print(f\"   ‚ö†Ô∏è {invalid_bp} casos donde Systolic BP ‚â§ Diastolic BP (imposible)\")\n",
    "\n",
    "# Ejemplo: LDL + HDL no puede ser mayor que Colesterol Total\n",
    "if all(col in df_to_check.columns for col in ['CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL']):\n",
    "    invalid_chol = ((df_to_check['CholesterolLDL'] + df_to_check['CholesterolHDL']) > df_to_check['CholesterolTotal'] * 1.2).sum()\n",
    "    if invalid_chol > 0:\n",
    "        logical_issues.append(f\"LDL+HDL > Total colesterol: {invalid_chol} casos\")\n",
    "        print(f\"   ‚ö†Ô∏è {invalid_chol} casos donde LDL+HDL > Colesterol Total (inconsistente)\")\n",
    "\n",
    "if logical_issues:\n",
    "    inconsistencies_found.extend(logical_issues)\n",
    "    print(f\"\\n   üí° Se identificaron {len(logical_issues)} tipo(s) de inconsistencias l√≥gicas\")\n",
    "else:\n",
    "    print(\"   ‚úì No se detectaron inconsistencias l√≥gicas entre variables\")\n",
    "\n",
    "# RESUMEN\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE INCONSISTENCIAS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if inconsistencies_found:\n",
    "    print(f\"\\nTotal de problemas identificados: {len(inconsistencies_found)}\\n\")\n",
    "    for idx, issue in enumerate(inconsistencies_found, 1):\n",
    "        print(f\"{idx}. {issue}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset corregido guardado como 'df_clean'\")\n",
    "    print(f\"   Filas originales: {len(df)}\")\n",
    "    print(f\"   Filas despu√©s de limpieza: {len(df_to_check)}\")\n",
    "    print(f\"   Filas eliminadas: {len(df) - len(df_to_check)}\")\n",
    "    \n",
    "    # Actualizar df_clean\n",
    "    df_clean = df_to_check\n",
    "else:\n",
    "    print(\"\\n‚úì No se detectaron inconsistencias significativas\")\n",
    "    print(\"  El dataset es consistente y est√° listo para an√°lisis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcd0f2",
   "metadata": {},
   "source": [
    "## 4.9 Estad√≠sticas Descriptivas Despu√©s de Limpieza\n",
    "\n",
    "Comparaci√≥n de estad√≠sticas antes y despu√©s de ajustar tipos de datos y corregir inconsistencias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar describe() despu√©s de limpieza y ajustes de tipos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - DESPU√âS DE LIMPIEZA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usar df_clean si existe\n",
    "df_final = df_clean if 'df_clean' in locals() else df\n",
    "\n",
    "# Separar variables num√©ricas y categ√≥ricas\n",
    "numeric_cols_clean = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols_clean = df_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"üìä COMPARACI√ìN: ANTES vs DESPU√âS DE LIMPIEZA\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Dataset Original:\")\n",
    "print(f\"   ‚Ä¢ Filas: {df.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Columnas: {df.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Variables num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"   ‚Ä¢ Variables categ√≥ricas: {len(categorical_cols)}\")\n",
    "print(f\"   ‚Ä¢ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nDataset Limpio:\")\n",
    "print(f\"   ‚Ä¢ Filas: {df_final.shape[0]} ({df.shape[0] - df_final.shape[0]} eliminadas)\")\n",
    "print(f\"   ‚Ä¢ Columnas: {df_final.shape[1]} ({df.shape[1] - df_final.shape[1]} eliminadas)\")\n",
    "print(f\"   ‚Ä¢ Variables num√©ricas: {len(numeric_cols_clean)}\")\n",
    "print(f\"   ‚Ä¢ Variables categ√≥ricas: {len(categorical_cols_clean)}\")\n",
    "print(f\"   ‚Ä¢ Memoria: {df_final.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Estad√≠sticas descriptivas num√©ricas\n",
    "if numeric_cols_clean:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTAD√çSTICAS NUM√âRICAS (DESPU√âS DE LIMPIEZA)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    stats_clean = df_final[numeric_cols_clean].describe().T\n",
    "    stats_clean['Nulos'] = df_final[numeric_cols_clean].isnull().sum()\n",
    "    stats_clean['% Nulos'] = (df_final[numeric_cols_clean].isnull().sum() / len(df_final) * 100).round(2)\n",
    "    stats_clean['Rango'] = df_final[numeric_cols_clean].max() - df_final[numeric_cols_clean].min()\n",
    "    stats_clean['Asimetr√≠a'] = df_final[numeric_cols_clean].skew()\n",
    "    stats_clean['Curtosis'] = df_final[numeric_cols_clean].kurtosis()\n",
    "    \n",
    "    print(stats_clean.round(3))\n",
    "\n",
    "# Estad√≠sticas descriptivas categ√≥ricas\n",
    "if categorical_cols_clean:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ESTAD√çSTICAS CATEG√ìRICAS (DESPU√âS DE LIMPIEZA)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for col in categorical_cols_clean:\n",
    "        print(f\"üìä {col}\")\n",
    "        print(f\"   Tipo: {df_final[col].dtype}\")\n",
    "        print(f\"   Valores √∫nicos: {df_final[col].nunique()}\")\n",
    "        print(f\"   Valores nulos: {df_final[col].isnull().sum()} ({df_final[col].isnull().sum()/len(df_final)*100:.1f}%)\")\n",
    "        \n",
    "        if df_final[col].nunique() <= 10:\n",
    "            print(f\"   Distribuci√≥n:\")\n",
    "            value_dist = df_final[col].value_counts()\n",
    "            for val, count in value_dist.items():\n",
    "                print(f\"      ‚Ä¢ {val}: {count} ({count/len(df_final)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"   Top 5 valores m√°s frecuentes:\")\n",
    "            top_values = df_final[col].value_counts().head(5)\n",
    "            for val, count in top_values.items():\n",
    "                print(f\"      ‚Ä¢ {val}: {count} ({count/len(df_final)*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "# Resumen de cambios en calidad de datos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà MEJORAS EN CALIDAD DE DATOS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comparar nulos\n",
    "nulos_antes = df.isnull().sum().sum()\n",
    "nulos_despues = df_final.isnull().sum().sum()\n",
    "pct_nulos_antes = (nulos_antes / (df.shape[0] * df.shape[1]) * 100)\n",
    "pct_nulos_despues = (nulos_despues / (df_final.shape[0] * df_final.shape[1]) * 100)\n",
    "\n",
    "print(f\"\\n1. Valores Nulos:\")\n",
    "print(f\"   Antes: {nulos_antes} ({pct_nulos_antes:.2f}%)\")\n",
    "print(f\"   Despu√©s: {nulos_despues} ({pct_nulos_despues:.2f}%)\")\n",
    "print(f\"   Cambio: {nulos_antes - nulos_despues} valores {'eliminados' if nulos_despues < nulos_antes else 'a√±adidos'}\")\n",
    "\n",
    "print(f\"\\n2. Duplicados:\")\n",
    "print(f\"   Antes: {df.duplicated().sum()}\")\n",
    "print(f\"   Despu√©s: {df_final.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n3. Filas:\")\n",
    "print(f\"   Antes: {len(df)}\")\n",
    "print(f\"   Despu√©s: {len(df_final)}\")\n",
    "print(f\"   Eliminadas: {len(df) - len(df_final)} ({(len(df) - len(df_final))/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n4. Columnas:\")\n",
    "print(f\"   Antes: {df.shape[1]}\")\n",
    "print(f\"   Despu√©s: {df_final.shape[1]}\")\n",
    "print(f\"   Eliminadas: {df.shape[1] - df_final.shape[1]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset limpio y listo para an√°lisis exploratorio detallado\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2441f53",
   "metadata": {},
   "source": [
    "## 5. Visualizaci√≥n de Distribuciones de Variables Num√©ricas\n",
    "\n",
    "Histogramas y gr√°ficos de densidad para cada variable num√©rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94985616",
   "metadata": {},
   "outputs": [],
   "source": [
    "if numeric_cols:\n",
    "    print(f\"\\nüìà Visualizando {len(numeric_cols)} variable(s) num√©rica(s)...\\n\")\n",
    "    \n",
    "    # Calcular n√∫mero de filas y columnas para subplot\n",
    "    n_cols = min(3, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if len(numeric_cols) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Histograma con distribuci√≥n normal\n",
    "        ax.hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "        ax2 = ax.twinx()\n",
    "        df[col].dropna().plot(kind='density', ax=ax2, color='red', linewidth=2)\n",
    "        \n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frecuencia', color='skyblue')\n",
    "        ax2.set_ylabel('Densidad', color='red')\n",
    "        ax.set_title(f'Distribuci√≥n de {col}')\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Eliminar subplots vac√≠os\n",
    "    for idx in range(len(numeric_cols), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables num√©ricas para visualizar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59beab2c",
   "metadata": {},
   "source": [
    "## 5.5 An√°lisis de Tipo de Distribuci√≥n (Skewness y Kurtosis)\n",
    "\n",
    "Interpretaci√≥n estad√≠stica de la forma de las distribuciones mediante asimetr√≠a y curtosis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57bf6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AN√ÅLISIS DE TIPO DE DISTRIBUCI√ìN\n",
      "================================================================================\n",
      "\n",
      "üìä M√âTRICAS DE DISTRIBUCI√ìN POR VARIABLE:\n",
      "--------------------------------------------------------------------------------\n",
      "            Variable  Skewness Interpretaci√≥n Skew  Kurtosis     Interpretaci√≥n Kurt Tipo Sugerido\n",
      "                 Age     0.046           Sim√©trica    -1.189 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "                 BMI    -0.027           Sim√©trica    -1.185 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "  AlcoholConsumption     0.018           Sim√©trica    -1.203 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "    PhysicalActivity     0.045           Sim√©trica    -1.179 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "         DietQuality    -0.012           Sim√©trica    -1.229 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "        SleepQuality    -0.070           Sim√©trica    -1.212 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "          SystolicBP     0.010           Sim√©trica    -1.198 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "         DiastolicBP    -0.054           Sim√©trica    -1.235 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "    CholesterolTotal    -0.019           Sim√©trica    -1.156 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "      CholesterolLDL     0.036           Sim√©trica    -1.208 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "      CholesterolHDL     0.042           Sim√©trica    -1.218 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "                MMSE     0.032           Sim√©trica    -1.230 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "FunctionalAssessment    -0.035           Sim√©trica    -1.183 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "                 ADL    -0.030           Sim√©trica    -1.250 Platic√∫rtica (aplanada)    Asim√©trica\n",
      "\n",
      "\n",
      "üìà RESUMEN DE DISTRIBUCIONES:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Distribuciones Normales: 0 variable(s)\n",
      "\n",
      "2. Distribuciones Asim√©tricas Positivas (sesgadas derecha): 0 variable(s)\n",
      "\n",
      "3. Distribuciones Asim√©tricas (otras): 14 variable(s)\n",
      "   ‚Ä¢ Age\n",
      "   ‚Ä¢ BMI\n",
      "   ‚Ä¢ AlcoholConsumption\n",
      "   ‚Ä¢ PhysicalActivity\n",
      "   ‚Ä¢ DietQuality\n",
      "   ‚Ä¢ SleepQuality\n",
      "   ‚Ä¢ SystolicBP\n",
      "   ‚Ä¢ DiastolicBP\n",
      "   ‚Ä¢ CholesterolTotal\n",
      "   ‚Ä¢ CholesterolLDL\n",
      "   ‚Ä¢ CholesterolHDL\n",
      "   ‚Ä¢ MMSE\n",
      "   ‚Ä¢ FunctionalAssessment\n",
      "   ‚Ä¢ ADL\n",
      "\n",
      "\n",
      "üí° RECOMENDACIONES DE TRANSFORMACI√ìN:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "‚úì No se requieren transformaciones mayores\n",
      "  Las distribuciones son razonablemente normales o sim√©tricas\n",
      "\n",
      "üìù IMPLICACIONES PARA MODELADO:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    1. Variables con distribuci√≥n normal: Ideales para modelos lineales y regresi√≥n\n",
      "\n",
      "    2. Variables asim√©tricas: Considerar transformaciones antes de modelos lineales\n",
      "       ‚Ä¢ Skewness > 1: Aplicar log transform\n",
      "       ‚Ä¢ Skewness < -1: Aplicar transformaci√≥n potencia\n",
      "\n",
      "    3. Variables con alta curtosis: Pueden contener outliers influyentes\n",
      "       ‚Ä¢ Verificar outliers antes de modelar\n",
      "       ‚Ä¢ Considerar modelos robustos (e.g., Random Forest, Gradient Boosting)\n",
      "\n",
      "    4. Modelos basados en √°rboles (Random Forest, XGBoost): No requieren\n",
      "       transformaciones, son invariantes a distribuciones\n",
      "    \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# An√°lisis detallado de distribuciones con interpretaci√≥n\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS DE TIPO DE DISTRIBUCI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usar df_clean si existe, sino df\n",
    "df_dist = df_clean if 'df_clean' in locals() else df\n",
    "numeric_cols_dist = df_dist.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "if numeric_cols_dist:\n",
    "    # Calcular m√©tricas de distribuci√≥n\n",
    "    distribution_analysis = []\n",
    "    \n",
    "    for col in numeric_cols_dist:\n",
    "        skew = df_dist[col].skew()\n",
    "        kurt = df_dist[col].kurtosis()\n",
    "        \n",
    "        # Interpretar skewness\n",
    "        if abs(skew) < 0.5:\n",
    "            skew_interp = \"Sim√©trica\"\n",
    "        elif skew < -0.5:\n",
    "            skew_interp = \"Asim√©trica izquierda (negativa)\"\n",
    "        else:\n",
    "            skew_interp = \"Asim√©trica derecha (positiva)\"\n",
    "        \n",
    "        # Interpretar kurtosis\n",
    "        if abs(kurt) < 0.5:\n",
    "            kurt_interp = \"Mesoc√∫rtica (normal)\"\n",
    "        elif kurt < -0.5:\n",
    "            kurt_interp = \"Platic√∫rtica (aplanada)\"\n",
    "        else:\n",
    "            kurt_interp = \"Leptoc√∫rtica (puntiaguda)\"\n",
    "        \n",
    "        # Tipo de distribuci√≥n sugerido\n",
    "        if abs(skew) < 0.5 and abs(kurt) < 0.5:\n",
    "            dist_type = \"Normal\"\n",
    "        elif skew > 1:\n",
    "            dist_type = \"Log-normal o Exponencial\"\n",
    "        elif skew < -1:\n",
    "            dist_type = \"Uniforme o Beta\"\n",
    "        else:\n",
    "            dist_type = \"Asim√©trica\"\n",
    "        \n",
    "        distribution_analysis.append({\n",
    "            'Variable': col,\n",
    "            'Skewness': round(skew, 3),\n",
    "            'Interpretaci√≥n Skew': skew_interp,\n",
    "            'Kurtosis': round(kurt, 3),\n",
    "            'Interpretaci√≥n Kurt': kurt_interp,\n",
    "            'Tipo Sugerido': dist_type\n",
    "        })\n",
    "    \n",
    "    # Crear DataFrame y mostrar\n",
    "    dist_df = pd.DataFrame(distribution_analysis)\n",
    "    \n",
    "    print(\"üìä M√âTRICAS DE DISTRIBUCI√ìN POR VARIABLE:\")\n",
    "    print(\"-\"*80)\n",
    "    print(dist_df.to_string(index=False))\n",
    "    \n",
    "    # Resumen por categor√≠a\n",
    "    print(\"\\n\\nüìà RESUMEN DE DISTRIBUCIONES:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Contar tipos de distribuci√≥n\n",
    "    normal_count = dist_df[dist_df['Tipo Sugerido'] == 'Normal'].shape[0]\n",
    "    lognormal_count = dist_df[dist_df['Tipo Sugerido'] == 'Log-normal o Exponencial'].shape[0]\n",
    "    asym_count = dist_df[dist_df['Tipo Sugerido'] == 'Asim√©trica'].shape[0]\n",
    "    other_count = len(dist_df) - normal_count - lognormal_count - asym_count\n",
    "    \n",
    "    print(f\"\\n1. Distribuciones Normales: {normal_count} variable(s)\")\n",
    "    if normal_count > 0:\n",
    "        normal_vars = dist_df[dist_df['Tipo Sugerido'] == 'Normal']['Variable'].tolist()\n",
    "        for var in normal_vars:\n",
    "            print(f\"   ‚Ä¢ {var}\")\n",
    "    \n",
    "    print(f\"\\n2. Distribuciones Asim√©tricas Positivas (sesgadas derecha): {lognormal_count} variable(s)\")\n",
    "    if lognormal_count > 0:\n",
    "        lognormal_vars = dist_df[dist_df['Tipo Sugerido'] == 'Log-normal o Exponencial']['Variable'].tolist()\n",
    "        for var in lognormal_vars:\n",
    "            print(f\"   ‚Ä¢ {var}\")\n",
    "            print(f\"     Skewness: {dist_df[dist_df['Variable']==var]['Skewness'].values[0]}\")\n",
    "    \n",
    "    print(f\"\\n3. Distribuciones Asim√©tricas (otras): {asym_count} variable(s)\")\n",
    "    if asym_count > 0:\n",
    "        asym_vars = dist_df[dist_df['Tipo Sugerido'] == 'Asim√©trica']['Variable'].tolist()\n",
    "        for var in asym_vars:\n",
    "            print(f\"   ‚Ä¢ {var}\")\n",
    "    \n",
    "    # Recomendaciones de transformaci√≥n\n",
    "    print(\"\\n\\nüí° RECOMENDACIONES DE TRANSFORMACI√ìN:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    transform_recommendations = []\n",
    "    \n",
    "    for idx, row in dist_df.iterrows():\n",
    "        if row['Skewness'] > 1:\n",
    "            transform_recommendations.append({\n",
    "                'Variable': row['Variable'],\n",
    "                'Problema': f\"Asimetr√≠a positiva fuerte (skew={row['Skewness']})\",\n",
    "                'Transformaci√≥n Sugerida': 'Logar√≠tmica: log(x + 1) o Box-Cox',\n",
    "                'Raz√≥n': 'Reducir asimetr√≠a y acercar a distribuci√≥n normal'\n",
    "            })\n",
    "        elif row['Skewness'] < -1:\n",
    "            transform_recommendations.append({\n",
    "                'Variable': row['Variable'],\n",
    "                'Problema': f\"Asimetr√≠a negativa fuerte (skew={row['Skewness']})\",\n",
    "                'Transformaci√≥n Sugerida': 'Cuadrado: x^2 o reflexi√≥n + log',\n",
    "                'Raz√≥n': 'Reducir asimetr√≠a negativa'\n",
    "            })\n",
    "        \n",
    "        if row['Kurtosis'] > 3:\n",
    "            transform_recommendations.append({\n",
    "                'Variable': row['Variable'],\n",
    "                'Problema': f\"Curtosis excesiva (kurt={row['Kurtosis']})\",\n",
    "                'Transformaci√≥n Sugerida': 'Recorte de outliers o Winsorizaci√≥n',\n",
    "                'Raz√≥n': 'Reducir influencia de valores extremos'\n",
    "            })\n",
    "    \n",
    "    if transform_recommendations:\n",
    "        print(f\"\\n{len(transform_recommendations)} transformaci√≥n(es) recomendada(s):\\n\")\n",
    "        \n",
    "        for idx, rec in enumerate(transform_recommendations, 1):\n",
    "            print(f\"{idx}. {rec['Variable']}\")\n",
    "            print(f\"   Problema: {rec['Problema']}\")\n",
    "            print(f\"   Transformaci√≥n: {rec['Transformaci√≥n Sugerida']}\")\n",
    "            print(f\"   Raz√≥n: {rec['Raz√≥n']}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"\\n‚úì No se requieren transformaciones mayores\")\n",
    "        print(\"  Las distribuciones son razonablemente normales o sim√©tricas\")\n",
    "    \n",
    "    # Interpretaci√≥n para modelos\n",
    "    print(\"\\nüìù IMPLICACIONES PARA MODELADO:\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\"\"\n",
    "    1. Variables con distribuci√≥n normal: Ideales para modelos lineales y regresi√≥n\n",
    "    \n",
    "    2. Variables asim√©tricas: Considerar transformaciones antes de modelos lineales\n",
    "       ‚Ä¢ Skewness > 1: Aplicar log transform\n",
    "       ‚Ä¢ Skewness < -1: Aplicar transformaci√≥n potencia\n",
    "    \n",
    "    3. Variables con alta curtosis: Pueden contener outliers influyentes\n",
    "       ‚Ä¢ Verificar outliers antes de modelar\n",
    "       ‚Ä¢ Considerar modelos robustos (e.g., Random Forest, Gradient Boosting)\n",
    "    \n",
    "    4. Modelos basados en √°rboles (Random Forest, XGBoost): No requieren\n",
    "       transformaciones, son invariantes a distribuciones\n",
    "    \"\"\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay variables num√©ricas para analizar distribuciones\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e69ae6",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Outliers\n",
    "\n",
    "Detecci√≥n de outliers usando el m√©todo del Rango Intercuart√≠lico (IQR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecci√≥n de outliers usando IQR\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS DE OUTLIERS (M√©todo IQR)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "if numeric_cols:\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        n_outliers = len(outliers)\n",
    "        \n",
    "        if n_outliers > 0:\n",
    "            pct_outliers = round((n_outliers / len(df) * 100), 2)\n",
    "            outlier_summary.append({\n",
    "                'Columna': col,\n",
    "                'Outliers': n_outliers,\n",
    "                '% Outliers': pct_outliers,\n",
    "                'Rango V√°lido': f\"[{lower_bound:.2f}, {upper_bound:.2f}]\"\n",
    "            })\n",
    "            print(f\"‚ö†Ô∏è  {col}: {n_outliers} outliers ({pct_outliers}%)\")\n",
    "    \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary)\n",
    "        print(\"\\n\" + outlier_df.to_string(index=False))\n",
    "        \n",
    "        # Boxplot de variables con outliers - Disposici√≥n vertical\n",
    "        if len(outlier_summary) > 0:\n",
    "            n_plots = len(outlier_summary)\n",
    "            n_cols = 2  # 2 columnas\n",
    "            n_rows = (n_plots + n_cols - 1) // n_cols  # Calcula filas necesarias\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 4*n_rows))\n",
    "            axes = axes.flatten()  # Convierte a array 1D para indexar f√°cilmente\n",
    "            \n",
    "            for idx, item in enumerate(outlier_summary):\n",
    "                col = item['Columna']\n",
    "                axes[idx].boxplot(df[col].dropna(), vert=True)\n",
    "                axes[idx].set_ylabel(col)\n",
    "                axes[idx].set_title(f'Boxplot de {col}')\n",
    "                axes[idx].grid(alpha=0.3, axis='y')\n",
    "            \n",
    "            # Eliminar subplots vac√≠os si los hay\n",
    "            for idx in range(len(outlier_summary), len(axes)):\n",
    "                fig.delaxes(axes[idx])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"\\n‚úì No se detectaron outliers significativos en el dataset\")\n",
    "else:\n",
    "    print(\"No hay variables num√©ricas para analizar outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619d10f",
   "metadata": {},
   "source": [
    "## 7. An√°lisis de Variables Categ√≥ricas\n",
    "\n",
    "Gr√°ficos de barras para variables categ√≥ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f38d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if categorical_cols:\n",
    "    print(f\"\\nüìä Visualizando {len(categorical_cols)} variable(s) categ√≥rica(s)...\\n\")\n",
    "    \n",
    "    # Calcular n√∫mero de filas y columnas para subplot\n",
    "    n_cols = min(3, len(categorical_cols))\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten() if len(categorical_cols) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Contar valores y crear gr√°fico\n",
    "        value_counts = df[col].value_counts()\n",
    "        \n",
    "        # Limitar a 10 categor√≠as para mejor visualizaci√≥n\n",
    "        if len(value_counts) > 10:\n",
    "            value_counts = value_counts.head(10)\n",
    "            title = f'Top 10 - {col} (hay m√°s categor√≠as)'\n",
    "        else:\n",
    "            title = f'Distribuci√≥n de {col}'\n",
    "        \n",
    "        value_counts.plot(kind='bar', ax=ax, color='teal', edgecolor='black')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Eliminar subplots vac√≠os\n",
    "    for idx in range(len(categorical_cols), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables categ√≥ricas para visualizar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b7f7b",
   "metadata": {},
   "source": [
    "## 7.5 An√°lisis Profundo de Variables Categ√≥ricas (Countplots y Tablas Pivote)\n",
    "\n",
    "Exploraci√≥n adicional con countplots de seaborn y tablas de contingencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ede51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis profundo de variables categ√≥ricas con countplot y tablas pivote\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS PROFUNDO DE VARIABLES CATEG√ìRICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usar df_clean si existe\n",
    "df_cat = df_clean if 'df_clean' in locals() else df\n",
    "categorical_cols_cat = df_cat.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Identificar target\n",
    "target_for_analysis = target_column if 'target_column' in locals() and target_column else None\n",
    "\n",
    "if categorical_cols_cat:\n",
    "    # 1. COUNTPLOTS CON SEABORN\n",
    "    print(\"1Ô∏è‚É£ COUNTPLOTS DE VARIABLES CATEG√ìRICAS:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Seleccionar primeras 6 variables para countplot\n",
    "    cat_for_plot = categorical_cols_cat[:min(6, len(categorical_cols_cat))]\n",
    "    \n",
    "    if target_for_analysis and target_for_analysis in df_cat.columns:\n",
    "        # Countplots con hue (color por target)\n",
    "        n_cols = 2\n",
    "        n_rows = (len(cat_for_plot) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))\n",
    "        axes = axes.flatten() if len(cat_for_plot) > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(cat_for_plot):\n",
    "            if col != target_for_analysis:  # No graficar target contra s√≠ mismo\n",
    "                ax = axes[idx]\n",
    "                \n",
    "                # Countplot con hue\n",
    "                sns.countplot(data=df_cat, x=col, hue=target_for_analysis, ax=ax, palette='Set2')\n",
    "                ax.set_title(f'Distribuci√≥n de {col} por {target_for_analysis}')\n",
    "                ax.set_xlabel(col)\n",
    "                ax.set_ylabel('Frecuencia')\n",
    "                ax.legend(title=target_for_analysis)\n",
    "                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Eliminar subplots vac√≠os\n",
    "        for idx in range(len(cat_for_plot), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Countplots simples sin hue\n",
    "        n_cols = 3\n",
    "        n_rows = (len(cat_for_plot) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        axes = axes.flatten() if len(cat_for_plot) > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(cat_for_plot):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            sns.countplot(data=df_cat, x=col, ax=ax, palette='viridis')\n",
    "            ax.set_title(f'Distribuci√≥n de {col}')\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel('Frecuencia')\n",
    "            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Eliminar subplots vac√≠os\n",
    "        for idx in range(len(cat_for_plot), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Countplots generados para {len(cat_for_plot)} variable(s)\\n\")\n",
    "    \n",
    "    # 2. TABLAS PIVOTE (CROSSTAB)\n",
    "    if target_for_analysis and target_for_analysis in df_cat.columns:\n",
    "        print(\"\\n2Ô∏è‚É£ TABLAS PIVOTE (CROSSTAB) - Variables vs Target:\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Seleccionar variables categ√≥ricas (excluyendo target)\n",
    "        cat_for_pivot = [col for col in categorical_cols_cat if col != target_for_analysis][:min(5, len(categorical_cols_cat)-1)]\n",
    "        \n",
    "        for col in cat_for_pivot:\n",
    "            print(f\"\\nüìä Tabla de Contingencia: {col} vs {target_for_analysis}\")\n",
    "            print(\"-\"*40)\n",
    "            \n",
    "            # Crear crosstab con totales\n",
    "            crosstab = pd.crosstab(\n",
    "                df_cat[col], \n",
    "                df_cat[target_for_analysis], \n",
    "                margins=True,\n",
    "                margins_name='Total'\n",
    "            )\n",
    "            print(crosstab)\n",
    "            \n",
    "            # Tabla de contingencia con porcentajes (por fila)\n",
    "            print(f\"\\nüìä Porcentajes por Fila ({col}):\")\n",
    "            print(\"-\"*40)\n",
    "            crosstab_pct = pd.crosstab(\n",
    "                df_cat[col], \n",
    "                df_cat[target_for_analysis], \n",
    "                normalize='index'\n",
    "            ) * 100\n",
    "            print(crosstab_pct.round(2))\n",
    "            \n",
    "            # An√°lisis de asociaci√≥n (Chi-cuadrado)\n",
    "            from scipy import stats\n",
    "            \n",
    "            contingency_table = pd.crosstab(df_cat[col], df_cat[target_for_analysis])\n",
    "            chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "            \n",
    "            print(f\"\\nüî¨ Test de Independencia (Chi-cuadrado):\")\n",
    "            print(f\"   Chi¬≤: {chi2:.4f}\")\n",
    "            print(f\"   p-value: {p_value:.4f}\")\n",
    "            print(f\"   Grados de libertad: {dof}\")\n",
    "            \n",
    "            if p_value < 0.001:\n",
    "                print(f\"   ‚úÖ Asociaci√≥n ALTAMENTE SIGNIFICATIVA (p < 0.001) ***\")\n",
    "            elif p_value < 0.01:\n",
    "                print(f\"   ‚úÖ Asociaci√≥n MUY SIGNIFICATIVA (p < 0.01) **\")\n",
    "            elif p_value < 0.05:\n",
    "                print(f\"   ‚úÖ Asociaci√≥n SIGNIFICATIVA (p < 0.05) *\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå NO hay asociaci√≥n significativa (p >= 0.05)\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # 3. VALUE_COUNTS DETALLADO\n",
    "    print(\"\\n3Ô∏è‚É£ VALUE_COUNTS DETALLADO:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for col in categorical_cols_cat[:min(5, len(categorical_cols_cat))]:\n",
    "        print(f\"\\nüìä {col}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        value_counts = df_cat[col].value_counts()\n",
    "        value_pct = df_cat[col].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Crear DataFrame combinado\n",
    "        value_summary = pd.DataFrame({\n",
    "            'Frecuencia': value_counts,\n",
    "            'Porcentaje (%)': value_pct.round(2)\n",
    "        })\n",
    "        \n",
    "        print(value_summary)\n",
    "        \n",
    "        # Estad√≠sticas adicionales\n",
    "        print(f\"\\n   Estad√≠sticas:\")\n",
    "        print(f\"   ‚Ä¢ Total categor√≠as: {df_cat[col].nunique()}\")\n",
    "        print(f\"   ‚Ä¢ Moda: {df_cat[col].mode()[0] if len(df_cat[col].mode()) > 0 else 'N/A'}\")\n",
    "        print(f\"   ‚Ä¢ Categor√≠a m√°s frecuente: {value_counts.idxmax()} ({value_counts.max()} veces, {value_pct.max():.2f}%)\")\n",
    "        print(f\"   ‚Ä¢ Categor√≠a menos frecuente: {value_counts.idxmin()} ({value_counts.min()} veces, {value_pct.min():.2f}%)\")\n",
    "        \n",
    "        # Calcular entrop√≠a (medida de diversidad)\n",
    "        entropy = -sum(value_pct/100 * np.log2(value_pct/100))\n",
    "        max_entropy = np.log2(df_cat[col].nunique())\n",
    "        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Entrop√≠a normalizada: {normalized_entropy:.3f} (0=uniforme, 1=m√°xima diversidad)\")\n",
    "        \n",
    "        if normalized_entropy < 0.3:\n",
    "            print(f\"   ‚ö†Ô∏è Variable poco diversa (dominada por pocas categor√≠as)\")\n",
    "        elif normalized_entropy > 0.8:\n",
    "            print(f\"   ‚úì Variable bien diversificada\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ An√°lisis de variables categ√≥ricas completado\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay variables categ√≥ricas para analizar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214f5aef",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Correlaci√≥n\n",
    "\n",
    "Correlaci√≥n entre variables num√©ricas y heatmap de correlaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c87aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MATRIZ DE CORRELACI√ìN\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Heatmap de correlaci√≥n - Mejorado para legibilidad\n",
    "    fig_size = max(14, len(numeric_cols) + 2)\n",
    "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                center=0, \n",
    "                square=True, \n",
    "                ax=ax, \n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                vmin=-1, \n",
    "                vmax=1,\n",
    "                annot_kws={\"size\": 8},\n",
    "                linewidths=0.5,\n",
    "                linecolor='gray')\n",
    "    \n",
    "    ax.set_title('Matriz de Correlaci√≥n de Variables Num√©ricas', fontsize=14, pad=20)\n",
    "    \n",
    "    # Rotar etiquetas para mejor legibilidad\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "    plt.yticks(rotation=0, fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Encontrar correlaciones fuertes (>0.7 o <-0.7)\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CORRELACIONES FUERTES (|r| > 0.7):\")\n",
    "    print(\"-\"*80 + \"\\n\")\n",
    "    \n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                strong_corr.append({\n",
    "                    'Variable 1': correlation_matrix.columns[i],\n",
    "                    'Variable 2': correlation_matrix.columns[j],\n",
    "                    'Correlaci√≥n': correlation_matrix.iloc[i, j].round(3)\n",
    "                })\n",
    "    \n",
    "    if strong_corr:\n",
    "        strong_corr_df = pd.DataFrame(strong_corr).sort_values('Correlaci√≥n', key=abs, ascending=False)\n",
    "        print(strong_corr_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No hay correlaciones fuertes entre variables.\")\n",
    "else:\n",
    "    print(\"\\nNo hay suficientes variables num√©ricas para calcular correlaci√≥n.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbfd58",
   "metadata": {},
   "source": [
    "## 8.5 An√°lisis Bivariado con Variable Objetivo\n",
    "\n",
    "Exploraci√≥n de la relaci√≥n entre cada feature y la variable objetivo (Diagnosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si existe variable objetivo (Diagnosis o similar)\n",
    "target_col = None\n",
    "possible_targets = ['Diagnosis', 'diagnosis', 'target', 'Target', 'label', 'Label']\n",
    "\n",
    "for col in possible_targets:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col and target_col in df.columns:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"AN√ÅLISIS BIVARIADO: FEATURES vs {target_col}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Distribuci√≥n de la variable objetivo\n",
    "    print(f\"\\nüìä Distribuci√≥n de {target_col}:\")\n",
    "    target_dist = df[target_col].value_counts()\n",
    "    print(target_dist)\n",
    "    print(f\"\\nProporci√≥n:\")\n",
    "    for label, count in target_dist.items():\n",
    "        print(f\"   Clase {label}: {count/len(df)*100:.1f}%\")\n",
    "    \n",
    "    # An√°lisis de variables num√©ricas vs target\n",
    "    if numeric_cols:\n",
    "        print(f\"\\nüìà An√°lisis de Variables Num√©ricas por {target_col}:\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Seleccionar hasta 6 variables m√°s importantes (por correlaci√≥n si existe)\n",
    "        features_to_analyze = numeric_cols[:min(6, len(numeric_cols))]\n",
    "        \n",
    "        n_cols = 3\n",
    "        n_rows = (len(features_to_analyze) + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "        axes = axes.flatten() if len(features_to_analyze) > 1 else [axes]\n",
    "        \n",
    "        for idx, col in enumerate(features_to_analyze):\n",
    "            ax = axes[idx]\n",
    "            \n",
    "            # Boxplot por clase\n",
    "            df_plot = df[[col, target_col]].dropna()\n",
    "            classes = sorted(df_plot[target_col].unique())\n",
    "            \n",
    "            data_by_class = [df_plot[df_plot[target_col] == c][col].values for c in classes]\n",
    "            \n",
    "            bp = ax.boxplot(data_by_class, labels=classes, patch_artist=True)\n",
    "            \n",
    "            # Colorear boxplots\n",
    "            colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow']\n",
    "            for patch, color in zip(bp['boxes'], colors):\n",
    "                patch.set_facecolor(color)\n",
    "            \n",
    "            ax.set_xlabel(target_col)\n",
    "            ax.set_ylabel(col)\n",
    "            ax.set_title(f'{col} por {target_col}')\n",
    "            ax.grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        # Eliminar subplots vac√≠os\n",
    "        for idx in range(len(features_to_analyze), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Tests estad√≠sticos de significancia (ejemplo con primeras 3 variables)\n",
    "        print(f\"\\nüî¨ Tests Estad√≠sticos (t-test para diferencias entre grupos):\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        from scipy import stats\n",
    "        \n",
    "        for col in features_to_analyze[:3]:\n",
    "            df_test = df[[col, target_col]].dropna()\n",
    "            classes = sorted(df_test[target_col].unique())\n",
    "            \n",
    "            if len(classes) == 2:\n",
    "                # t-test para 2 grupos\n",
    "                group1 = df_test[df_test[target_col] == classes[0]][col]\n",
    "                group2 = df_test[df_test[target_col] == classes[1]][col]\n",
    "                \n",
    "                t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                \n",
    "                print(f\"\\n{col}:\")\n",
    "                print(f\"   Media Clase {classes[0]}: {group1.mean():.3f}\")\n",
    "                print(f\"   Media Clase {classes[1]}: {group2.mean():.3f}\")\n",
    "                print(f\"   t-statistic: {t_stat:.3f}\")\n",
    "                print(f\"   p-value: {p_value:.4f} {significance}\")\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    print(f\"   ‚úì Diferencia significativa entre grupos\")\n",
    "                else:\n",
    "                    print(f\"   ‚úó No hay diferencia significativa\")\n",
    "    \n",
    "    # An√°lisis de variables categ√≥ricas vs target\n",
    "    if categorical_cols and target_col not in categorical_cols:\n",
    "        print(f\"\\n\\nüìä An√°lisis de Variables Categ√≥ricas por {target_col}:\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Seleccionar primeras 3-4 variables categ√≥ricas\n",
    "        cat_to_analyze = categorical_cols[:min(4, len(categorical_cols))]\n",
    "        \n",
    "        for col in cat_to_analyze:\n",
    "            print(f\"\\n{col}:\")\n",
    "            \n",
    "            # Tabla de contingencia\n",
    "            contingency = pd.crosstab(df[col], df[target_col], margins=True)\n",
    "            print(contingency)\n",
    "            \n",
    "            # Chi-cuadrado test\n",
    "            if len(df[col].unique()) > 1:\n",
    "                contingency_no_margins = pd.crosstab(df[col], df[target_col])\n",
    "                chi2, p_value, dof, expected = stats.chi2_contingency(contingency_no_margins)\n",
    "                \n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "                \n",
    "                print(f\"\\nChi-cuadrado: {chi2:.3f}, p-value: {p_value:.4f} {significance}\")\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    print(f\"‚úì Asociaci√≥n significativa con {target_col}\")\n",
    "                else:\n",
    "                    print(f\"‚úó No hay asociaci√≥n significativa\")\n",
    "                print(\"-\"*40)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se encontr√≥ variable objetivo (Diagnosis) en el dataset\")\n",
    "    print(\"   Si existe con otro nombre, ajustar la variable 'target_col'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416695d",
   "metadata": {},
   "source": [
    "## 8.6 An√°lisis Multivariado: Pairplot\n",
    "\n",
    "Exploraci√≥n de relaciones entre m√∫ltiples variables simult√°neamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee6bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col and len(numeric_cols) > 1:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AN√ÅLISIS MULTIVARIADO: PAIRPLOT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Seleccionar top 5-6 variables con mayor correlaci√≥n con el target (si es num√©rico)\n",
    "    # O simplemente las primeras 5-6 variables num√©ricas\n",
    "    \n",
    "    max_features = 5  # L√≠mite para que el pairplot sea legible\n",
    "    \n",
    "    if target_col in numeric_cols:\n",
    "        # Si target es num√©rico, seleccionar por correlaci√≥n\n",
    "        correlations = df[numeric_cols].corr()[target_col].abs().sort_values(ascending=False)\n",
    "        top_features = correlations[1:max_features+1].index.tolist()  # Excluir el target mismo\n",
    "        features_for_pairplot = top_features + [target_col]\n",
    "    else:\n",
    "        # Si target es categ√≥rico, seleccionar primeras N features\n",
    "        features_for_pairplot = numeric_cols[:min(max_features, len(numeric_cols))]\n",
    "    \n",
    "    print(f\"\\nüìä Creando pairplot con {len(features_for_pairplot)} variables:\")\n",
    "    for feat in features_for_pairplot:\n",
    "        print(f\"   ‚Ä¢ {feat}\")\n",
    "    \n",
    "    # Crear subset del dataframe\n",
    "    df_pairplot = df[features_for_pairplot + ([target_col] if target_col not in features_for_pairplot else [])].dropna()\n",
    "    \n",
    "    print(f\"\\n‚è≥ Generando pairplot (esto puede tomar unos segundos)...\")\n",
    "    \n",
    "    # Crear pairplot\n",
    "    if target_col in df.columns and df[target_col].dtype in ['object', 'int64'] and df[target_col].nunique() <= 5:\n",
    "        # Si target es categ√≥rico con pocas clases, usar como hue\n",
    "        pairplot = sns.pairplot(\n",
    "            df_pairplot, \n",
    "            hue=target_col,\n",
    "            diag_kind='kde',\n",
    "            plot_kws={'alpha': 0.6, 's': 30},\n",
    "            height=2.5\n",
    "        )\n",
    "        pairplot.fig.suptitle(f'Pairplot de Variables Num√©ricas por {target_col}', y=1.02, fontsize=14)\n",
    "    else:\n",
    "        # Si no, pairplot simple\n",
    "        pairplot = sns.pairplot(\n",
    "            df_pairplot,\n",
    "            diag_kind='kde',\n",
    "            plot_kws={'alpha': 0.6},\n",
    "            height=2.5\n",
    "        )\n",
    "        pairplot.fig.suptitle('Pairplot de Variables Num√©ricas Principales', y=1.02, fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Pairplot completado\")\n",
    "    print(\"\\nüí° Interpretaci√≥n:\")\n",
    "    print(\"   ‚Ä¢ Diagonal: Distribuci√≥n de cada variable\")\n",
    "    print(\"   ‚Ä¢ Fuera de diagonal: Scatter plots entre pares de variables\")\n",
    "    if target_col in df.columns and df[target_col].dtype in ['object', 'int64'] and df[target_col].nunique() <= 5:\n",
    "        print(f\"   ‚Ä¢ Colores: Representan diferentes clases de {target_col}\")\n",
    "        print(\"   ‚Ä¢ Buscar: Separaci√≥n clara entre colores indica poder predictivo\")\n",
    "    \n",
    "elif len(numeric_cols) > 1:\n",
    "    print(\"\\n‚ö†Ô∏è Variable objetivo no encontrada. Creando pairplot sin clasificaci√≥n por color.\")\n",
    "    \n",
    "    features_for_pairplot = numeric_cols[:min(5, len(numeric_cols))]\n",
    "    df_pairplot = df[features_for_pairplot].dropna()\n",
    "    \n",
    "    pairplot = sns.pairplot(df_pairplot, diag_kind='kde', height=2.5)\n",
    "    pairplot.fig.suptitle('Pairplot de Variables Num√©ricas', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay suficientes variables num√©ricas para crear un pairplot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360cf86c",
   "metadata": {},
   "source": [
    "## 8.7 Sugerencias de Features Derivados\n",
    "\n",
    "Basado en el an√°lisis exploratorio, se identifican oportunidades para crear caracter√≠sticas calculadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f61fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES DERIVADOS POTENCIALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "derived_features = []\n",
    "\n",
    "# Verificar columnas espec√≠ficas del dataset de Alzheimer\n",
    "health_indicators = []\n",
    "lifestyle_indicators = []\n",
    "cognitive_indicators = []\n",
    "cardiovascular_indicators = []\n",
    "\n",
    "# Clasificar columnas por categor√≠a\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    \n",
    "    if any(x in col_lower for x in ['cholesterol', 'bp', 'blood', 'systolic', 'diastolic']):\n",
    "        cardiovascular_indicators.append(col)\n",
    "    elif any(x in col_lower for x in ['mmse', 'memory', 'cognitive', 'functional', 'adl', 'confusion']):\n",
    "        cognitive_indicators.append(col)\n",
    "    elif any(x in col_lower for x in ['smoking', 'alcohol', 'physical', 'diet', 'sleep', 'bmi']):\n",
    "        lifestyle_indicators.append(col)\n",
    "    elif any(x in col_lower for x in ['diabetes', 'hypertension', 'cardiovascular', 'depression']):\n",
    "        health_indicators.append(col)\n",
    "\n",
    "print(\"\\nüìã CATEGOR√çAS DE VARIABLES IDENTIFICADAS:\")\n",
    "print(f\"   ‚Ä¢ Indicadores de Salud: {len(health_indicators)}\")\n",
    "print(f\"   ‚Ä¢ Indicadores de Estilo de Vida: {len(lifestyle_indicators)}\")\n",
    "print(f\"   ‚Ä¢ Indicadores Cognitivos: {len(cognitive_indicators)}\")\n",
    "print(f\"   ‚Ä¢ Indicadores Cardiovasculares: {len(cardiovascular_indicators)}\")\n",
    "\n",
    "print(\"\\n\\nüí° FEATURES DERIVADOS SUGERIDOS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Ratios de colesterol\n",
    "if 'CholesterolLDL' in df.columns and 'CholesterolHDL' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Cholesterol_Ratio_LDL_HDL',\n",
    "        'f√≥rmula': 'CholesterolLDL / CholesterolHDL',\n",
    "        'justificaci√≥n': 'Indicador de riesgo cardiovascular. Ratio alto asociado con mayor riesgo.',\n",
    "        'implementaci√≥n': 'df[\"Cholesterol_Ratio\"] = df[\"CholesterolLDL\"] / df[\"CholesterolHDL\"]'\n",
    "    })\n",
    "\n",
    "if 'CholesterolTotal' in df.columns and 'CholesterolHDL' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Cholesterol_Total_HDL_Ratio',\n",
    "        'f√≥rmula': 'CholesterolTotal / CholesterolHDL',\n",
    "        'justificaci√≥n': 'Otro indicador cardiovascular. Valores normales < 5.',\n",
    "        'implementaci√≥n': 'df[\"Total_HDL_Ratio\"] = df[\"CholesterolTotal\"] / df[\"CholesterolHDL\"]'\n",
    "    })\n",
    "\n",
    "# 2. Presi√≥n arterial media\n",
    "if 'SystolicBP' in df.columns and 'DiastolicBP' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Mean_Arterial_Pressure',\n",
    "        'f√≥rmula': 'DiastolicBP + (SystolicBP - DiastolicBP) / 3',\n",
    "        'justificaci√≥n': 'Presi√≥n arterial media, mejor indicador de perfusi√≥n cerebral.',\n",
    "        'implementaci√≥n': 'df[\"MAP\"] = df[\"DiastolicBP\"] + (df[\"SystolicBP\"] - df[\"DiastolicBP\"]) / 3'\n",
    "    })\n",
    "\n",
    "# 3. IMC categorizado\n",
    "if 'BMI' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'BMI_Category',\n",
    "        'f√≥rmula': 'Categorizaci√≥n: Bajo(<18.5), Normal(18.5-24.9), Sobrepeso(25-29.9), Obeso(>=30)',\n",
    "        'justificaci√≥n': 'Categor√≠as cl√≠nicas de IMC m√°s interpretables que valor continuo.',\n",
    "        'implementaci√≥n': 'pd.cut(df[\"BMI\"], bins=[0, 18.5, 25, 30, 100], labels=[\"Bajo\", \"Normal\", \"Sobrepeso\", \"Obeso\"])'\n",
    "    })\n",
    "\n",
    "# 4. Score de riesgo cardiovascular\n",
    "if health_indicators:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Cardiovascular_Risk_Score',\n",
    "        'f√≥rmula': f'Suma de: {\", \".join(health_indicators[:5])}',\n",
    "        'justificaci√≥n': 'Agregaci√≥n de factores de riesgo cardiovascular.',\n",
    "        'implementaci√≥n': f'df[{health_indicators[:5]}].sum(axis=1)'\n",
    "    })\n",
    "\n",
    "# 5. Score de estilo de vida saludable\n",
    "if lifestyle_indicators:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Healthy_Lifestyle_Score',\n",
    "        'f√≥rmula': 'Combinaci√≥n ponderada de h√°bitos saludables',\n",
    "        'justificaci√≥n': 'Agregaci√≥n de factores de estilo de vida que afectan salud cerebral.',\n",
    "        'implementaci√≥n': 'Normalizar y sumar: PhysicalActivity, DietQuality, SleepQuality (invertir Smoking y AlcoholConsumption)'\n",
    "    })\n",
    "\n",
    "# 6. Score cognitivo compuesto\n",
    "if cognitive_indicators:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Cognitive_Impairment_Score',\n",
    "        'f√≥rmula': f'Combinaci√≥n de: {\", \".join(cognitive_indicators[:4])}',\n",
    "        'justificaci√≥n': 'Indicador agregado de deterioro cognitivo.',\n",
    "        'implementaci√≥n': 'Suma o promedio de indicadores cognitivos'\n",
    "    })\n",
    "\n",
    "# 7. Interacciones de edad\n",
    "if 'Age' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Age_Squared',\n",
    "        'f√≥rmula': 'Age ** 2',\n",
    "        'justificaci√≥n': 'Capturar relaci√≥n no lineal entre edad y riesgo.',\n",
    "        'implementaci√≥n': 'df[\"Age_Squared\"] = df[\"Age\"] ** 2'\n",
    "    })\n",
    "    \n",
    "    if 'FamilyHistoryAlzheimers' in df.columns:\n",
    "        derived_features.append({\n",
    "            'nombre': 'Age_Family_History_Interaction',\n",
    "            'f√≥rmula': 'Age * FamilyHistoryAlzheimers',\n",
    "            'justificaci√≥n': 'Interacci√≥n entre edad y predisposici√≥n gen√©tica.',\n",
    "            'implementaci√≥n': 'df[\"Age_FH_Interaction\"] = df[\"Age\"] * df[\"FamilyHistoryAlzheimers\"]'\n",
    "        })\n",
    "\n",
    "# 8. Binning de variables continuas\n",
    "if 'Age' in df.columns:\n",
    "    derived_features.append({\n",
    "        'nombre': 'Age_Group',\n",
    "        'f√≥rmula': 'Categor√≠as: <65, 65-74, 75-84, 85+',\n",
    "        'justificaci√≥n': 'Grupos de edad cl√≠nicamente relevantes para Alzheimer.',\n",
    "        'implementaci√≥n': 'pd.cut(df[\"Age\"], bins=[0, 65, 75, 85, 120], labels=[\"<65\", \"65-74\", \"75-84\", \"85+\"])'\n",
    "    })\n",
    "\n",
    "# Mostrar tabla de features sugeridos\n",
    "if derived_features:\n",
    "    print(f\"\\nTotal de features derivados sugeridos: {len(derived_features)}\\n\")\n",
    "    \n",
    "    for idx, feature in enumerate(derived_features, 1):\n",
    "        print(f\"{idx}. {feature['nombre']}\")\n",
    "        print(f\"   F√≥rmula: {feature['f√≥rmula']}\")\n",
    "        print(f\"   Justificaci√≥n: {feature['justificaci√≥n']}\")\n",
    "        print(f\"   Implementaci√≥n: {feature['implementaci√≥n']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMENDACIONES DE IMPLEMENTACI√ìN:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\"\"\n",
    "    1. Implementar estos features en el script 'ft_engineering.py'\n",
    "    2. Crear una funci√≥n 'create_derived_features()' antes del preprocesamiento\n",
    "    3. Validar que los features derivados no tengan valores infinitos o NaN\n",
    "    4. Evaluar la importancia de estos features despu√©s del entrenamiento\n",
    "    5. Considerar regularizaci√≥n si se agregan muchos features (evitar overfitting)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Ejemplo de implementaci√≥n\n",
    "    print(\"\\nüìù EJEMPLO DE C√ìDIGO PARA IMPLEMENTACI√ìN:\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"\"\"\n",
    "def create_derived_features(df):\n",
    "    '''Crea features derivados basados en an√°lisis EDA'''\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Ratio LDL/HDL\n",
    "    if 'CholesterolLDL' in df.columns and 'CholesterolHDL' in df.columns:\n",
    "        df_new['Cholesterol_Ratio'] = df_new['CholesterolLDL'] / df_new['CholesterolHDL']\n",
    "    \n",
    "    # Presi√≥n arterial media\n",
    "    if 'SystolicBP' in df.columns and 'DiastolicBP' in df.columns:\n",
    "        df_new['MAP'] = df_new['DiastolicBP'] + (df_new['SystolicBP'] - df_new['DiastolicBP']) / 3\n",
    "    \n",
    "    # IMC categorizado\n",
    "    if 'BMI' in df.columns:\n",
    "        df_new['BMI_Category'] = pd.cut(df_new['BMI'], \n",
    "                                         bins=[0, 18.5, 25, 30, 100], \n",
    "                                         labels=['Bajo', 'Normal', 'Sobrepeso', 'Obeso'])\n",
    "    \n",
    "    # Edad al cuadrado\n",
    "    if 'Age' in df.columns:\n",
    "        df_new['Age_Squared'] = df_new['Age'] ** 2\n",
    "    \n",
    "    return df_new\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No se identificaron oportunidades claras para features derivados.\")\n",
    "    print(\"   Esto puede deberse a que el dataset no contiene las columnas esperadas.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b770712",
   "metadata": {},
   "source": [
    "## 8.8 Reglas de Validaci√≥n de Datos\n",
    "\n",
    "Identificaci√≥n y documentaci√≥n de reglas de negocio y restricciones de integridad para el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar y documentar reglas de validaci√≥n de datos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGLAS DE VALIDACI√ìN DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Usar df_clean si existe\n",
    "df_validate = df_clean if 'df_clean' in locals() else df\n",
    "\n",
    "# Estructura para almacenar reglas\n",
    "validation_rules_dict = {}\n",
    "\n",
    "print(\"üìã REGLAS DE VALIDACI√ìN IDENTIFICADAS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# REGLA 1: RANGOS V√ÅLIDOS PARA VARIABLES NUM√âRICAS\n",
    "print(\"\\n1Ô∏è‚É£ RANGOS V√ÅLIDOS PARA VARIABLES NUM√âRICAS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "numeric_ranges = {\n",
    "    'Age': {\n",
    "        'min': 0,\n",
    "        'max': 120,\n",
    "        'tipo': 'Edad en a√±os',\n",
    "        'justificaci√≥n': 'Rango biol√≥gico humano'\n",
    "    },\n",
    "    'BMI': {\n",
    "        'min': 10,\n",
    "        'max': 60,\n",
    "        'tipo': '√çndice de Masa Corporal',\n",
    "        'justificaci√≥n': 'Rango cl√≠nico v√°lido'\n",
    "    },\n",
    "    'SystolicBP': {\n",
    "        'min': 70,\n",
    "        'max': 250,\n",
    "        'tipo': 'Presi√≥n arterial sist√≥lica (mmHg)',\n",
    "        'justificaci√≥n': 'Rango fisiol√≥gico compatible con vida'\n",
    "    },\n",
    "    'DiastolicBP': {\n",
    "        'min': 40,\n",
    "        'max': 150,\n",
    "        'tipo': 'Presi√≥n arterial diast√≥lica (mmHg)',\n",
    "        'justificaci√≥n': 'Rango fisiol√≥gico compatible con vida'\n",
    "    },\n",
    "    'CholesterolTotal': {\n",
    "        'min': 100,\n",
    "        'max': 400,\n",
    "        'tipo': 'Colesterol total (mg/dL)',\n",
    "        'justificaci√≥n': 'Rango cl√≠nico observado'\n",
    "    },\n",
    "    'CholesterolLDL': {\n",
    "        'min': 50,\n",
    "        'max': 300,\n",
    "        'tipo': 'Colesterol LDL (mg/dL)',\n",
    "        'justificaci√≥n': 'Rango cl√≠nico observado'\n",
    "    },\n",
    "    'CholesterolHDL': {\n",
    "        'min': 20,\n",
    "        'max': 100,\n",
    "        'tipo': 'Colesterol HDL (mg/dL)',\n",
    "        'justificaci√≥n': 'Rango cl√≠nico observado'\n",
    "    },\n",
    "    'CholesterolTriglycerides': {\n",
    "        'min': 50,\n",
    "        'max': 500,\n",
    "        'tipo': 'Triglic√©ridos (mg/dL)',\n",
    "        'justificaci√≥n': 'Rango cl√≠nico observado'\n",
    "    },\n",
    "    'MMSE': {\n",
    "        'min': 0,\n",
    "        'max': 30,\n",
    "        'tipo': 'Mini-Mental State Examination',\n",
    "        'justificaci√≥n': 'Escala de 0-30 puntos'\n",
    "    },\n",
    "    'FunctionalAssessment': {\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'tipo': 'Evaluaci√≥n funcional',\n",
    "        'justificaci√≥n': 'Escala de 0-10 puntos'\n",
    "    },\n",
    "    'ADL': {\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'tipo': 'Activities of Daily Living',\n",
    "        'justificaci√≥n': 'Escala de 0-10 puntos'\n",
    "    }\n",
    "}\n",
    "\n",
    "for col, rules in numeric_ranges.items():\n",
    "    if col in df_validate.columns:\n",
    "        print(f\"\\n   ‚Ä¢ {col}: [{rules['min']}, {rules['max']}]\")\n",
    "        print(f\"     Tipo: {rules['tipo']}\")\n",
    "        print(f\"     Justificaci√≥n: {rules['justificaci√≥n']}\")\n",
    "        \n",
    "        # Validar\n",
    "        out_of_range = df_validate[(df_validate[col] < rules['min']) | (df_validate[col] > rules['max'])][col].count()\n",
    "        \n",
    "        if out_of_range > 0:\n",
    "            print(f\"     ‚ö†Ô∏è {out_of_range} valor(es) fuera de rango\")\n",
    "        else:\n",
    "            print(f\"     ‚úì Todos los valores en rango v√°lido\")\n",
    "        \n",
    "        validation_rules_dict[col] = rules\n",
    "\n",
    "# REGLA 2: RELACIONES L√ìGICAS ENTRE VARIABLES\n",
    "print(\"\\n\\n2Ô∏è‚É£ RELACIONES L√ìGICAS ENTRE VARIABLES:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "logical_rules = []\n",
    "\n",
    "# Regla: Systolic BP > Diastolic BP\n",
    "if 'SystolicBP' in df_validate.columns and 'DiastolicBP' in df_validate.columns:\n",
    "    rule = {\n",
    "        'nombre': 'Presi√≥n Sist√≥lica > Diast√≥lica',\n",
    "        'condici√≥n': 'SystolicBP > DiastolicBP',\n",
    "        'justificaci√≥n': 'Principio fisiol√≥gico b√°sico'\n",
    "    }\n",
    "    logical_rules.append(rule)\n",
    "    \n",
    "    violations = (df_validate['SystolicBP'] <= df_validate['DiastolicBP']).sum()\n",
    "    print(f\"\\n   ‚Ä¢ {rule['nombre']}\")\n",
    "    print(f\"     Condici√≥n: {rule['condici√≥n']}\")\n",
    "    print(f\"     Justificaci√≥n: {rule['justificaci√≥n']}\")\n",
    "    print(f\"     Violaciones: {violations} registro(s)\")\n",
    "\n",
    "# Regla: LDL + HDL ‚â§ Colesterol Total\n",
    "if all(col in df_validate.columns for col in ['CholesterolTotal', 'CholesterolLDL', 'CholesterolHDL']):\n",
    "    rule = {\n",
    "        'nombre': 'LDL + HDL ‚â§ Colesterol Total',\n",
    "        'condici√≥n': 'CholesterolLDL + CholesterolHDL ‚â§ CholesterolTotal * 1.1',\n",
    "        'justificaci√≥n': 'El total debe ser suma de componentes (con margen 10%)'\n",
    "    }\n",
    "    logical_rules.append(rule)\n",
    "    \n",
    "    violations = ((df_validate['CholesterolLDL'] + df_validate['CholesterolHDL']) > \n",
    "                  df_validate['CholesterolTotal'] * 1.1).sum()\n",
    "    print(f\"\\n   ‚Ä¢ {rule['nombre']}\")\n",
    "    print(f\"     Condici√≥n: {rule['condici√≥n']}\")\n",
    "    print(f\"     Justificaci√≥n: {rule['justificaci√≥n']}\")\n",
    "    print(f\"     Violaciones: {violations} registro(s)\")\n",
    "\n",
    "# Regla: BMI coherente con altura/peso (si existen)\n",
    "if all(col in df_validate.columns for col in ['Height', 'Weight', 'BMI']):\n",
    "    rule = {\n",
    "        'nombre': 'BMI = Weight / (Height¬≤)',\n",
    "        'condici√≥n': 'BMI ‚âà Weight / ((Height/100)¬≤)',\n",
    "        'justificaci√≥n': 'F√≥rmula matem√°tica del IMC'\n",
    "    }\n",
    "    logical_rules.append(rule)\n",
    "    \n",
    "    calculated_bmi = df_validate['Weight'] / ((df_validate['Height']/100) ** 2)\n",
    "    violations = (abs(df_validate['BMI'] - calculated_bmi) > 2).sum()  # Margen de 2 unidades\n",
    "    print(f\"\\n   ‚Ä¢ {rule['nombre']}\")\n",
    "    print(f\"     Condici√≥n: {rule['condici√≥n']}\")\n",
    "    print(f\"     Justificaci√≥n: {rule['justificaci√≥n']}\")\n",
    "    print(f\"     Violaciones: {violations} registro(s) (diferencia > 2)\")\n",
    "\n",
    "# REGLA 3: VALORES OBLIGATORIOS (NO NULOS)\n",
    "print(\"\\n\\n3Ô∏è‚É£ CAMPOS OBLIGATORIOS (NO NULOS):\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "mandatory_fields = ['Age', 'Gender', 'Diagnosis']  # Campos que SIEMPRE deben tener valor\n",
    "\n",
    "for field in mandatory_fields:\n",
    "    if field in df_validate.columns:\n",
    "        null_count = df_validate[field].isnull().sum()\n",
    "        print(f\"\\n   ‚Ä¢ {field}\")\n",
    "        print(f\"     Valores nulos: {null_count}\")\n",
    "        \n",
    "        if null_count > 0:\n",
    "            print(f\"     ‚ö†Ô∏è VIOLACI√ìN: Campo obligatorio con valores nulos\")\n",
    "        else:\n",
    "            print(f\"     ‚úì Cumple: Sin valores nulos\")\n",
    "\n",
    "# REGLA 4: VALORES CATEG√ìRICOS V√ÅLIDOS\n",
    "print(\"\\n\\n4Ô∏è‚É£ VALORES CATEG√ìRICOS V√ÅLIDOS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "categorical_constraints = {\n",
    "    'Gender': ['Male', 'Female', 'M', 'F', 0, 1],\n",
    "    'Smoking': [0, 1, 'Yes', 'No'],\n",
    "    'FamilyHistoryAlzheimers': [0, 1, 'Yes', 'No'],\n",
    "    'CardiovascularDisease': [0, 1, 'Yes', 'No'],\n",
    "    'Diabetes': [0, 1, 'Yes', 'No'],\n",
    "    'Depression': [0, 1, 'Yes', 'No'],\n",
    "    'HeadInjury': [0, 1, 'Yes', 'No'],\n",
    "    'Hypertension': [0, 1, 'Yes', 'No']\n",
    "}\n",
    "\n",
    "for col, valid_values in categorical_constraints.items():\n",
    "    if col in df_validate.columns:\n",
    "        invalid_count = (~df_validate[col].isin(valid_values)).sum()\n",
    "        unique_invalid = df_validate[~df_validate[col].isin(valid_values)][col].unique()\n",
    "        \n",
    "        print(f\"\\n   ‚Ä¢ {col}\")\n",
    "        print(f\"     Valores v√°lidos: {valid_values}\")\n",
    "        print(f\"     Valores inv√°lidos: {invalid_count}\")\n",
    "        \n",
    "        if invalid_count > 0:\n",
    "            print(f\"     ‚ö†Ô∏è Valores no reconocidos: {unique_invalid}\")\n",
    "        else:\n",
    "            print(f\"     ‚úì Todos los valores son v√°lidos\")\n",
    "\n",
    "# REGLA 5: CONSISTENCIA TEMPORAL/L√ìGICA\n",
    "print(\"\\n\\n5Ô∏è‚É£ CONSISTENCIA L√ìGICA ADICIONAL:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "consistency_rules = []\n",
    "\n",
    "# Edad y diagn√≥stico de Alzheimer\n",
    "if 'Age' in df_validate.columns and 'Diagnosis' in df_validate.columns:\n",
    "    # Alzheimer es m√°s com√∫n en mayores de 65\n",
    "    young_alzheimers = df_validate[(df_validate['Age'] < 50) & (df_validate['Diagnosis'] == 1)]\n",
    "    \n",
    "    print(f\"\\n   ‚Ä¢ Diagn√≥stico de Alzheimer en menores de 50 a√±os:\")\n",
    "    print(f\"     Casos detectados: {len(young_alzheimers)}\")\n",
    "    \n",
    "    if len(young_alzheimers) > 0:\n",
    "        print(f\"     ‚ö†Ô∏è Alzheimer de inicio temprano (early-onset) - revisar si es v√°lido\")\n",
    "    else:\n",
    "        print(f\"     ‚úì No hay casos de Alzheimer en menores de 50 a√±os\")\n",
    "\n",
    "# MMSE bajo con diagn√≥stico negativo (inconsistencia)\n",
    "if 'MMSE' in df_validate.columns and 'Diagnosis' in df_validate.columns:\n",
    "    low_mmse_no_diagnosis = df_validate[(df_validate['MMSE'] < 20) & (df_validate['Diagnosis'] == 0)]\n",
    "    \n",
    "    print(f\"\\n   ‚Ä¢ MMSE bajo (<20) sin diagn√≥stico de Alzheimer:\")\n",
    "    print(f\"     Casos detectados: {len(low_mmse_no_diagnosis)}\")\n",
    "    \n",
    "    if len(low_mmse_no_diagnosis) > 0:\n",
    "        print(f\"     ‚ö†Ô∏è Posible inconsistencia: MMSE bajo sugiere deterioro cognitivo\")\n",
    "    else:\n",
    "        print(f\"     ‚úì Consistente: MMSE bajo asociado con diagn√≥stico positivo\")\n",
    "\n",
    "# RESUMEN EJECUTIVO\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE VALIDACI√ìN:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_rules = len(numeric_ranges) + len(logical_rules) + len(mandatory_fields) + len(categorical_constraints)\n",
    "\n",
    "print(f\"\\nTotal de reglas de validaci√≥n definidas: {total_rules}\")\n",
    "print(f\"   ‚Ä¢ Rangos num√©ricos: {len(numeric_ranges)}\")\n",
    "print(f\"   ‚Ä¢ Relaciones l√≥gicas: {len(logical_rules)}\")\n",
    "print(f\"   ‚Ä¢ Campos obligatorios: {len(mandatory_fields)}\")\n",
    "print(f\"   ‚Ä¢ Restricciones categ√≥ricas: {len(categorical_constraints)}\")\n",
    "\n",
    "print(\"\\nüí° RECOMENDACIONES:\")\n",
    "print(\"-\"*80)\n",
    "print(\"\"\"\n",
    "1. Implementar estas reglas en el pipeline de ingesta de datos\n",
    "2. Crear tests autom√°ticos que validen cada regla\n",
    "3. Documentar excepciones v√°lidas (e.g., Alzheimer de inicio temprano)\n",
    "4. Establecer alertas para violaciones cr√≠ticas\n",
    "5. Revisar peri√≥dicamente rangos basados en nuevos datos\n",
    "6. Crear funciones de validaci√≥n reutilizables\n",
    "\"\"\")\n",
    "\n",
    "# Guardar reglas para uso posterior\n",
    "data_validation_rules = {\n",
    "    'numeric_ranges': numeric_ranges,\n",
    "    'logical_rules': logical_rules,\n",
    "    'mandatory_fields': mandatory_fields,\n",
    "    'categorical_constraints': categorical_constraints\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Reglas de validaci√≥n documentadas y guardadas en 'data_validation_rules'\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f9e1b",
   "metadata": {},
   "source": [
    "## 9. Resumen Ejecutivo\n",
    "\n",
    "Conclusiones y recomendaciones generales sobre el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO DEL EDA\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"üìä Dataset: {data_path}\")\n",
    "print(f\"üìà Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"\\nüîç Composici√≥n del Dataset:\")\n",
    "print(f\"   ‚Ä¢ Variables num√©ricas: {len(numeric_cols)}\")\n",
    "print(f\"   ‚Ä¢ Variables categ√≥ricas: {len(categorical_cols)}\")\n",
    "\n",
    "# Calidad de datos\n",
    "pct_missing = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100)\n",
    "print(f\"\\n‚öôÔ∏è  Calidad de Datos:\")\n",
    "print(f\"   ‚Ä¢ % Valores faltantes totales: {pct_missing:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Rows completas: {len(df.dropna()):,} ({(len(df.dropna())/len(df)*100):.1f}%)\")\n",
    "\n",
    "# Recomendaciones\n",
    "print(f\"\\nüí° Recomendaciones:\")\n",
    "\n",
    "if pct_missing > 5:\n",
    "    print(f\"   ‚ö†Ô∏è  Alta proporci√≥n de valores faltantes ({pct_missing:.2f}%). Considerar:\")\n",
    "    print(f\"       - Imputaci√≥n de valores\")\n",
    "    print(f\"       - Eliminaci√≥n de columnas/filas con muchos faltantes\")\n",
    "else:\n",
    "    print(f\"   ‚úì Proporci√≥n aceptable de valores faltantes ({pct_missing:.2f}%)\")\n",
    "\n",
    "if len(outlier_summary) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  Se detectaron outliers. Considerar:\")\n",
    "    print(f\"       - An√°lisis causa-efecto de los outliers\")\n",
    "    print(f\"       - Posible transformaci√≥n o remoci√≥n\")\n",
    "else:\n",
    "    print(f\"   ‚úì No hay outliers significativos detectados\")\n",
    "\n",
    "if len(numeric_cols) > 1 and strong_corr:\n",
    "    print(f\"   üí¨ Se encontraron {len(strong_corr)} correlaci√≥n(es) fuerte(s)\")\n",
    "    print(f\"       - Revisar multicolinealidad para modelos predictivos\")\n",
    "\n",
    "print(f\"\\n‚úÖ EDA completado exitosamente\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
