{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4887eb4",
   "metadata": {},
   "source": [
    "# Feature Engineering - Pipeline MLOps\n",
    "\n",
    "Este notebook implementa la **Ingenier√≠a de Caracter√≠sticas** para el proyecto de predicci√≥n de Alzheimer.\n",
    "\n",
    "**üîç Prop√≥sito de este Notebook:**\n",
    "- Este notebook es **AUTOCONTENIDO** y puede ejecutarse de forma independiente\n",
    "- Muestra de forma manual y gr√°fica el proceso de Feature Engineering\n",
    "- No depende de scripts externos ni de pasos anteriores\n",
    "- Ideal para revisi√≥n, comprensi√≥n y experimentaci√≥n\n",
    "\n",
    "**Funcionalidades:**\n",
    "- Carga de datos directamente desde el CSV original\n",
    "- Limpieza b√°sica de datos (eliminar IDs, duplicados)\n",
    "- Creaci√≥n de features derivados basados en el an√°lisis EDA\n",
    "- Clasificaci√≥n autom√°tica de tipos de variables\n",
    "- Construcci√≥n de pipelines de preprocesamiento (sklearn)\n",
    "- Transformaciones: imputaci√≥n, escalado, codificaci√≥n\n",
    "- Separaci√≥n train-test estratificada\n",
    "- Visualizaciones del proceso de transformaci√≥n\n",
    "- Guardado de artefactos (preprocessor, datasets transformados)\n",
    "\n",
    "**üìã Diferencia con los Scripts:**\n",
    "- **Notebooks (`/notebooks/`)**: Revisi√≥n manual paso a paso, con visualizaciones y explicaciones\n",
    "- **Scripts (`/scripts/`)**: Automatizaci√≥n para ejecuci√≥n completa del pipeline y Docker\n",
    "\n",
    "**Basado en hallazgos del EDA:** `comprension_eda.ipynb` (Secci√≥n 8.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c10d0412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Librer√≠as importadas correctamente\n",
      "‚úì Pandas versi√≥n: 1.5.3\n",
      "‚úì Numpy versi√≥n: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn para feature engineering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Librer√≠as importadas correctamente\")\n",
    "print(f\"‚úì Pandas versi√≥n: {pd.__version__}\")\n",
    "print(f\"‚úì Numpy versi√≥n: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862c29f",
   "metadata": {},
   "source": [
    "## 1. Cargar Datos Originales y Configuraci√≥n\n",
    "\n",
    "Cargamos el dataset original directamente desde el CSV y la configuraci√≥n del proyecto.\n",
    "\n",
    "**Nota:** Este notebook NO depende de pasos anteriores. Carga los datos originales y realiza toda la limpieza necesaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68dec5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Configuraci√≥n cargada desde config.json\n",
      "\n",
      "üìä Par√°metros de configuraci√≥n:\n",
      "   ‚Ä¢ Test size: 0.2\n",
      "   ‚Ä¢ Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# Cargar configuraci√≥n\n",
    "config_path = \"../../../config.json\"\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"‚úì Configuraci√≥n cargada desde config.json\")\n",
    "else:\n",
    "    config = {\"training\": {\"test_size\": 0.2, \"random_state\": 42}}\n",
    "    print(\"‚ö†Ô∏è config.json no encontrado. Usando configuraci√≥n por defecto\")\n",
    "\n",
    "# Extraer par√°metros de entrenamiento\n",
    "test_size = config.get('training', {}).get('test_size', 0.2)\n",
    "random_state = config.get('training', {}).get('random_state', 42)\n",
    "\n",
    "print(f\"\\nüìä Par√°metros de configuraci√≥n:\")\n",
    "print(f\"   ‚Ä¢ Test size: {test_size}\")\n",
    "print(f\"   ‚Ä¢ Random state: {random_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18280f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå ERROR: No se encontr√≥ el archivo de datos\n",
      "   Buscado en: alzheimers_disease_data.csv\n",
      "   Por favor, verifica que el archivo CSV existe en la ruta correcta\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset ORIGINAL (no procesado)\n",
    "# Buscar primero desde config.json, sino usar el path por defecto\n",
    "\n",
    "data_path = None\n",
    "\n",
    "# Intentar cargar desde config\n",
    "if os.path.exists(config_path):\n",
    "    data_path = config.get('data_path_notebooks', \"../../../alzheimers_disease_data.csv\")\n",
    "else:\n",
    "    data_path = \"../../../alzheimers_disease_data.csv\"\n",
    "\n",
    "# Cargar dataset original\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"‚ùå ERROR: No se encontr√≥ el archivo de datos\")\n",
    "    print(f\"   Buscado en: {data_path}\")\n",
    "    print(\"   Por favor, verifica que el archivo CSV existe en la ruta correcta\")\n",
    "else:\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "    print(f\"‚úì Dataset ORIGINAL cargado desde: {data_path}\")\n",
    "    print(f\"  Dimensiones: {df_raw.shape[0]} filas √ó {df_raw.shape[1]} columnas\")\n",
    "    print(f\"\\nüìã Primeras filas del dataset ORIGINAL:\")\n",
    "    display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a647b",
   "metadata": {},
   "source": [
    "## 1.5 Limpieza B√°sica de Datos\n",
    "\n",
    "Realizamos limpieza b√°sica antes del feature engineering:\n",
    "- Eliminar columnas de identificaci√≥n (PatientID, DoctorInCharge)\n",
    "- Eliminar duplicados\n",
    "- Verificar tipos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c9dd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LIMPIEZA B√ÅSICA DE DATOS\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Crear copia para no modificar el original\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mdf_raw\u001b[49m.copy()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Dataset original: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filas √ó \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columnas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 1. Eliminar duplicados\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LIMPIEZA B√ÅSICA DE DATOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Crear copia para no modificar el original\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(f\"üìä Dataset original: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "\n",
    "# 1. Eliminar duplicados\n",
    "n_duplicates = df.duplicated().sum()\n",
    "if n_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úì Eliminados {n_duplicates} registros duplicados\")\n",
    "else:\n",
    "    print(\"‚úì No se encontraron duplicados\")\n",
    "\n",
    "# 2. Eliminar columnas de identificaci√≥n (no son features predictivos)\n",
    "id_columns = ['PatientID', 'DoctorInCharge']\n",
    "existing_id_cols = [col for col in id_columns if col in df.columns]\n",
    "\n",
    "if existing_id_cols:\n",
    "    df = df.drop(columns=existing_id_cols)\n",
    "    print(f\"‚úì Eliminadas columnas de identificaci√≥n: {existing_id_cols}\")\n",
    "\n",
    "# 3. Informaci√≥n sobre valores faltantes (se manejar√°n en el pipeline)\n",
    "missing_info = df.isnull().sum()\n",
    "if missing_info.sum() > 0:\n",
    "    print(f\"\\nüìä Valores faltantes detectados (se imputar√°n en el pipeline):\")\n",
    "    for col, count in missing_info[missing_info > 0].items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"   {col}: {count} ({pct:.2f}%)\")\n",
    "else:\n",
    "    print(\"\\n‚úì No hay valores faltantes\")\n",
    "\n",
    "# 4. Verificar tipos de datos\n",
    "print(f\"\\nüìã Tipos de datos:\")\n",
    "numeric_cols_check = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols_check = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"   Num√©ricas: {len(numeric_cols_check)} columnas\")\n",
    "print(f\"   Categ√≥ricas: {len(categorical_cols_check)} columnas\")\n",
    "\n",
    "print(f\"\\n‚úÖ Limpieza completada\")\n",
    "print(f\"   Dimensiones finales: {df.shape[0]} filas √ó {df.shape[1]} columnas\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33578d35",
   "metadata": {},
   "source": [
    "## 2. Creaci√≥n de Features Derivados\n",
    "\n",
    "Basado en el an√°lisis EDA (`comprension_eda.ipynb` Secci√≥n 8.7), implementamos los features derivados m√°s relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bce785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_derived_features(df):\n",
    "    \"\"\"\n",
    "    Crea features derivados basados en el an√°lisis EDA.\n",
    "    \n",
    "    Features implementados:\n",
    "    1. Cholesterol_Ratio (LDL/HDL) - Indicador de riesgo cardiovascular\n",
    "    2. Cholesterol_Total_HDL_Ratio - Otro indicador cardiovascular\n",
    "    3. Mean_Arterial_Pressure (MAP) - Perfusi√≥n cerebral\n",
    "    4. Age_Squared - Capturar relaci√≥n no lineal con edad\n",
    "    5. Age_FH_Interaction - Interacci√≥n edad x historia familiar\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con features derivados agregados\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "    features_created = []\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CREANDO FEATURES DERIVADOS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 1. Ratio de Colesterol LDL/HDL\n",
    "    if 'CholesterolLDL' in df.columns and 'CholesterolHDL' in df.columns:\n",
    "        df_new['Cholesterol_Ratio_LDL_HDL'] = df_new['CholesterolLDL'] / df_new['CholesterolHDL']\n",
    "        features_created.append('Cholesterol_Ratio_LDL_HDL')\n",
    "        print(\"‚úì Creado: Cholesterol_Ratio_LDL_HDL (LDL/HDL)\")\n",
    "    \n",
    "    # 2. Ratio de Colesterol Total/HDL\n",
    "    if 'CholesterolTotal' in df.columns and 'CholesterolHDL' in df.columns:\n",
    "        df_new['Cholesterol_Total_HDL_Ratio'] = df_new['CholesterolTotal'] / df_new['CholesterolHDL']\n",
    "        features_created.append('Cholesterol_Total_HDL_Ratio')\n",
    "        print(\"‚úì Creado: Cholesterol_Total_HDL_Ratio (Total/HDL)\")\n",
    "    \n",
    "    # 3. Presi√≥n Arterial Media (MAP)\n",
    "    if 'SystolicBP' in df.columns and 'DiastolicBP' in df.columns:\n",
    "        df_new['Mean_Arterial_Pressure'] = (\n",
    "            df_new['DiastolicBP'] + (df_new['SystolicBP'] - df_new['DiastolicBP']) / 3\n",
    "        )\n",
    "        features_created.append('Mean_Arterial_Pressure')\n",
    "        print(\"‚úì Creado: Mean_Arterial_Pressure (MAP)\")\n",
    "    \n",
    "    # 4. Edad al cuadrado\n",
    "    if 'Age' in df.columns:\n",
    "        df_new['Age_Squared'] = df_new['Age'] ** 2\n",
    "        features_created.append('Age_Squared')\n",
    "        print(\"‚úì Creado: Age_Squared (Age¬≤)\")\n",
    "    \n",
    "    # 5. Interacci√≥n Edad x Historia Familiar\n",
    "    if 'Age' in df.columns and 'FamilyHistoryAlzheimers' in df.columns:\n",
    "        df_new['Age_FH_Interaction'] = df_new['Age'] * df_new['FamilyHistoryAlzheimers']\n",
    "        features_created.append('Age_FH_Interaction')\n",
    "        print(\"‚úì Creado: Age_FH_Interaction (Age √ó FamilyHistory)\")\n",
    "    \n",
    "    # 6. Score de riesgo cardiovascular (suma de condiciones)\n",
    "    cv_conditions = ['CardiovascularDisease', 'Diabetes', 'Hypertension']\n",
    "    if all(col in df.columns for col in cv_conditions):\n",
    "        df_new['CV_Risk_Score'] = df_new[cv_conditions].sum(axis=1)\n",
    "        features_created.append('CV_Risk_Score')\n",
    "        print(\"‚úì Creado: CV_Risk_Score (suma de condiciones cardiovasculares)\")\n",
    "    \n",
    "    # Manejar valores infinitos o NaN resultantes\n",
    "    for col in features_created:\n",
    "        # Reemplazar infinitos con NaN\n",
    "        df_new[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        # Contar NaNs\n",
    "        n_nan = df_new[col].isna().sum()\n",
    "        if n_nan > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {col}: {n_nan} valores NaN (ser√°n imputados en el pipeline)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total de features derivados creados: {len(features_created)}\")\n",
    "    print(f\"   Dimensiones nuevas: {df_new.shape[0]} filas √ó {df_new.shape[1]} columnas\")\n",
    "    print(f\"   Features agregados: {len(features_created)}\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Aplicar transformaci√≥n\n",
    "df_with_features = create_derived_features(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15321cbb",
   "metadata": {},
   "source": [
    "## 3. Clasificaci√≥n de Tipos de Variables\n",
    "\n",
    "Identificamos y clasificamos las variables en num√©ricas, categ√≥ricas nominales y categ√≥ricas ordinales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variable objetivo\n",
    "target_col = 'Diagnosis'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLASIFICACI√ìN DE CARACTER√çSTICAS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Separar features del target\n",
    "if target_col in df_with_features.columns:\n",
    "    feature_cols = [col for col in df_with_features.columns if col != target_col]\n",
    "    df_features = df_with_features[feature_cols]\n",
    "    print(f\"‚úì Variable objetivo identificada: {target_col}\")\n",
    "    print(f\"‚úì Total de features: {len(feature_cols)}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Variable objetivo '{target_col}' no encontrada\")\n",
    "    df_features = df_with_features\n",
    "    feature_cols = df_features.columns.tolist()\n",
    "\n",
    "# Detectar tipos autom√°ticamente\n",
    "numeric_features = df_features.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df_features.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Para este dataset, todas las categ√≥ricas son nominales\n",
    "# Si hubiera ordinales espec√≠ficas (ej: 'EducationLevel'), se definir√≠an aqu√≠\n",
    "ordinal_features = []\n",
    "nominal_features = categorical_features\n",
    "\n",
    "print(f\"\\nüìä Variables Num√©ricas ({len(numeric_features)}):\")\n",
    "if numeric_features:\n",
    "    for i, col in enumerate(numeric_features[:10], 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    if len(numeric_features) > 10:\n",
    "        print(f\"   ... y {len(numeric_features) - 10} m√°s\")\n",
    "\n",
    "print(f\"\\nüìù Variables Categ√≥ricas Nominales ({len(nominal_features)}):\")\n",
    "if nominal_features:\n",
    "    for i, col in enumerate(nominal_features, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "else:\n",
    "    print(\"   Ninguna\")\n",
    "\n",
    "print(f\"\\nüìà Variables Categ√≥ricas Ordinales ({len(ordinal_features)}):\")\n",
    "if ordinal_features:\n",
    "    for i, col in enumerate(ordinal_features, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "else:\n",
    "    print(\"   Ninguna\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f847a",
   "metadata": {},
   "source": [
    "## 4. Construcci√≥n de Pipelines de Preprocesamiento\n",
    "\n",
    "Creamos pipelines espec√≠ficos para cada tipo de variable usando sklearn `Pipeline` y `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CONSTRUCCI√ìN DE PIPELINES DE PREPROCESAMIENTO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "transformers_list = []\n",
    "\n",
    "# Pipeline para variables num√©ricas\n",
    "if numeric_features:\n",
    "    numeric_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    transformers_list.append(('numeric', numeric_pipeline, numeric_features))\n",
    "    \n",
    "    print(f\"‚úì Pipeline Num√©rico ({len(numeric_features)} features):\")\n",
    "    print(f\"    1. SimpleImputer(strategy='median') - Imputa valores faltantes con la mediana\")\n",
    "    print(f\"    2. StandardScaler() - Normaliza con media=0 y std=1\")\n",
    "    print()\n",
    "\n",
    "# Pipeline para variables categ√≥ricas nominales\n",
    "if nominal_features:\n",
    "    nominal_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    transformers_list.append(('nominal', nominal_pipeline, nominal_features))\n",
    "    \n",
    "    print(f\"‚úì Pipeline Categ√≥rico Nominal ({len(nominal_features)} features):\")\n",
    "    print(f\"    1. SimpleImputer(strategy='most_frequent') - Imputa con el valor m√°s frecuente\")\n",
    "    print(f\"    2. OneHotEncoder(handle_unknown='ignore') - Codificaci√≥n one-hot\")\n",
    "    print()\n",
    "\n",
    "# Pipeline para variables categ√≥ricas ordinales\n",
    "if ordinal_features:\n",
    "    ordinal_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "    ])\n",
    "    transformers_list.append(('ordinal', ordinal_pipeline, ordinal_features))\n",
    "    \n",
    "    print(f\"‚úì Pipeline Categ√≥rico Ordinal ({len(ordinal_features)} features):\")\n",
    "    print(f\"    1. SimpleImputer(strategy='most_frequent')\")\n",
    "    print(f\"    2. OrdinalEncoder(handle_unknown='use_encoded_value')\")\n",
    "    print()\n",
    "\n",
    "# Crear ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=transformers_list,\n",
    "    remainder='drop'  # Eliminar columnas no especificadas\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ColumnTransformer creado con {len(transformers_list)} transformadores\")\n",
    "print(f\"   ‚Ä¢ Transformadores: {[t[0] for t in transformers_list]}\")\n",
    "print(f\"   ‚Ä¢ Remainder: 'drop' (columnas no especificadas ser√°n eliminadas)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40bb11",
   "metadata": {},
   "source": [
    "## 5. Separaci√≥n Train-Test\n",
    "\n",
    "Separamos los datos en conjuntos de entrenamiento y evaluaci√≥n con estratificaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d73b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SEPARACI√ìN DE DATOS TRAIN-TEST\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Separar features (X) y target (y)\n",
    "X = df_with_features.drop(columns=[target_col])\n",
    "y = df_with_features[target_col]\n",
    "\n",
    "print(f\"‚úì Features (X): {X.shape}\")\n",
    "print(f\"‚úì Target (y): {y.shape}\")\n",
    "\n",
    "# Train-test split con estratificaci√≥n\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=y  # Mantener proporci√≥n de clases\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Divisi√≥n train-test ({int((1-test_size)*100)}-{int(test_size*100)}):\")\n",
    "print(f\"   Entrenamiento: {X_train.shape[0]:,} muestras ({(X_train.shape[0]/len(y)*100):.1f}%)\")\n",
    "print(f\"   Evaluaci√≥n:    {X_test.shape[0]:,} muestras ({(X_test.shape[0]/len(y)*100):.1f}%)\")\n",
    "\n",
    "# Verificar distribuci√≥n de clases\n",
    "print(f\"\\nüìà Distribuci√≥n del target en ENTRENAMIENTO:\")\n",
    "train_dist = y_train.value_counts().sort_index()\n",
    "for label, count in train_dist.items():\n",
    "    print(f\"   Clase {label}: {count:,} ({count/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Distribuci√≥n del target en EVALUACI√ìN:\")\n",
    "test_dist = y_test.value_counts().sort_index()\n",
    "for label, count in test_dist.items():\n",
    "    print(f\"   Clase {label}: {count:,} ({count/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Verificar que la estratificaci√≥n funcion√≥ correctamente\n",
    "print(\"\\n‚úì Estratificaci√≥n exitosa: proporciones similares entre train y test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420031b3",
   "metadata": {},
   "source": [
    "## 6. Ajuste y Transformaci√≥n de Datos\n",
    "\n",
    "Ajustamos el preprocessor SOLO con datos de entrenamiento (evitar data leakage) y transformamos ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63918f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"AJUSTE Y TRANSFORMACI√ìN\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìä Ajustando preprocessor con datos de entrenamiento...\")\n",
    "print(\"   (IMPORTANTE: Solo usar train para evitar data leakage)\")\n",
    "preprocessor.fit(X_train)\n",
    "print(\"‚úì Preprocessor ajustado exitosamente\")\n",
    "\n",
    "print(\"\\nüîÑ Transformando datos de entrenamiento...\")\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "print(f\"‚úì X_train transformado: {X_train_transformed.shape}\")\n",
    "\n",
    "print(\"\\nüîÑ Transformando datos de evaluaci√≥n...\")\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "print(f\"‚úì X_test transformado: {X_test_transformed.shape}\")\n",
    "\n",
    "print(f\"\\nüìà Resumen de transformaci√≥n:\")\n",
    "print(f\"   Features originales:    {X_train.shape[1]}\")\n",
    "print(f\"   Features transformados: {X_train_transformed.shape[1]}\")\n",
    "print(f\"   Diferencia: {X_train_transformed.shape[1] - X_train.shape[1]:+d}\")\n",
    "\n",
    "# Explicar el aumento en features (si aplica)\n",
    "if X_train_transformed.shape[1] > X_train.shape[1]:\n",
    "    print(f\"\\nüí° Nota: El aumento de features se debe a:\")\n",
    "    print(f\"   ‚Ä¢ OneHotEncoder crea una columna por cada categor√≠a\")\n",
    "    print(f\"   ‚Ä¢ Variables categ√≥ricas: {len(nominal_features)}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb8005",
   "metadata": {},
   "source": [
    "## 7. Visualizaci√≥n Post-Transformaci√≥n\n",
    "\n",
    "Verifiquemos visualmente c√≥mo se ven los datos transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc169a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar primeras 6 features transformadas\n",
    "print(\"üìä Primeras 6 features transformadas (X_train):\\n\")\n",
    "\n",
    "# Convertir a DataFrame para visualizaci√≥n\n",
    "X_train_df = pd.DataFrame(\n",
    "    X_train_transformed[:5], \n",
    "    columns=[f'Feature_{i}' for i in range(X_train_transformed.shape[1])]\n",
    ")\n",
    "display(X_train_df.iloc[:, :6])  # Mostrar primeras 6 columnas\n",
    "\n",
    "# Estad√≠sticas de las features transformadas\n",
    "print(\"\\nüìà Estad√≠sticas de features transformadas:\")\n",
    "print(f\"   Min:  {X_train_transformed.min():.4f}\")\n",
    "print(f\"   Max:  {X_train_transformed.max():.4f}\")\n",
    "print(f\"   Mean: {X_train_transformed.mean():.4f}\")\n",
    "print(f\"   Std:  {X_train_transformed.std():.4f}\")\n",
    "\n",
    "# Verificar valores NaN o infinitos\n",
    "n_nan = np.isnan(X_train_transformed).sum()\n",
    "n_inf = np.isinf(X_train_transformed).sum()\n",
    "\n",
    "if n_nan > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Advertencia: {n_nan} valores NaN encontrados\")\n",
    "else:\n",
    "    print(f\"\\n‚úì No hay valores NaN\")\n",
    "\n",
    "if n_inf > 0:\n",
    "    print(f\"‚ö†Ô∏è Advertencia: {n_inf} valores infinitos encontrados\")\n",
    "else:\n",
    "    print(f\"‚úì No hay valores infinitos\")\n",
    "\n",
    "# Visualizar distribuci√≥n de algunas features\n",
    "print(\"\\nüìä Distribuci√≥n de primeras 4 features num√©ricas escaladas:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(4, X_train_transformed.shape[1])):\n",
    "    ax = axes[i]\n",
    "    ax.hist(X_train_transformed[:, i], bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "    ax.set_title(f'Feature {i}')\n",
    "    ax.set_xlabel('Valor escalado')\n",
    "    ax.set_ylabel('Frecuencia')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretaci√≥n:\")\n",
    "print(\"   ‚Ä¢ Features num√©ricas deber√≠an estar centradas en 0 con std ‚âà 1 (StandardScaler)\")\n",
    "print(\"   ‚Ä¢ Features one-hot deber√≠an tener valores 0 o 1\")\n",
    "print(\"   ‚Ä¢ Distribuciones pueden ser no-normales, pero sin valores extremos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0598ef",
   "metadata": {},
   "source": [
    "## 8. Guardado de Artefactos\n",
    "\n",
    "Guardamos el preprocessor ajustado y los datasets transformados para uso en el siguiente paso del pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffe873",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GUARDANDO ARTEFACTOS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Crear directorios si no existen\n",
    "artifacts_dir = Path(\"../../artifacts\")\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_dir = Path(\"../../data/processed\")\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Guardar preprocessor ajustado\n",
    "preprocessor_path = artifacts_dir / \"preprocessor.joblib\"\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"üíæ Preprocessor guardado en: {preprocessor_path}\")\n",
    "print(f\"   Tama√±o: {preprocessor_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# 2. Guardar datasets transformados como CSV\n",
    "X_train_df = pd.DataFrame(X_train_transformed)\n",
    "X_test_df = pd.DataFrame(X_test_transformed)\n",
    "y_train_df = pd.DataFrame(y_train).reset_index(drop=True)\n",
    "y_test_df = pd.DataFrame(y_test).reset_index(drop=True)\n",
    "\n",
    "X_train_path = data_dir / \"X_train.csv\"\n",
    "X_test_path = data_dir / \"X_test.csv\"\n",
    "y_train_path = data_dir / \"y_train.csv\"\n",
    "y_test_path = data_dir / \"y_test.csv\"\n",
    "\n",
    "X_train_df.to_csv(X_train_path, index=False)\n",
    "X_test_df.to_csv(X_test_path, index=False)\n",
    "y_train_df.to_csv(y_train_path, index=False)\n",
    "y_test_df.to_csv(y_test_path, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Datasets guardados en: {data_dir}\")\n",
    "print(f\"   ‚Ä¢ X_train.csv: {X_train_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   ‚Ä¢ X_test.csv:  {X_test_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   ‚Ä¢ y_train.csv: {y_train_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   ‚Ä¢ y_test.csv:  {y_test_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# 3. Guardar metadatos\n",
    "metadata = {\n",
    "    \"n_features_original\": X_train.shape[1],\n",
    "    \"n_features_transformed\": X_train_transformed.shape[1],\n",
    "    \"n_numeric_features\": len(numeric_features),\n",
    "    \"n_categorical_features\": len(nominal_features),\n",
    "    \"n_samples_train\": X_train.shape[0],\n",
    "    \"n_samples_test\": X_test.shape[0],\n",
    "    \"test_size\": test_size,\n",
    "    \"random_state\": random_state,\n",
    "    \"target_column\": target_col,\n",
    "    \"features_created\": [\n",
    "        'Cholesterol_Ratio_LDL_HDL',\n",
    "        'Cholesterol_Total_HDL_Ratio',\n",
    "        'Mean_Arterial_Pressure',\n",
    "        'Age_Squared',\n",
    "        'Age_FH_Interaction',\n",
    "        'CV_Risk_Score'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = artifacts_dir / \"feature_engineering_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Metadata guardado en: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ARTEFACTOS GUARDADOS EXITOSAMENTE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5cad8",
   "metadata": {},
   "source": [
    "## 9. Resumen y Pr√≥ximos Pasos\n",
    "\n",
    "Resumen de lo realizado y recomendaciones para continuar con el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed09f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE FEATURE ENGINEERING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìä DATOS PROCESADOS:\")\n",
    "print(f\"   ‚Ä¢ Dataset original: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"   ‚Ä¢ Features derivados creados: 6\")\n",
    "print(f\"   ‚Ä¢ Features finales (antes de transformaci√≥n): {X_train.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Features despu√©s de transformaci√≥n: {X_train_transformed.shape[1]}\")\n",
    "\n",
    "print(f\"\\nüîÑ TRANSFORMACIONES APLICADAS:\")\n",
    "print(f\"   ‚Ä¢ Variables num√©ricas ({len(numeric_features)}): Imputaci√≥n (median) + StandardScaler\")\n",
    "print(f\"   ‚Ä¢ Variables categ√≥ricas ({len(nominal_features)}): Imputaci√≥n (most_frequent) + OneHotEncoder\")\n",
    "if ordinal_features:\n",
    "    print(f\"   ‚Ä¢ Variables ordinales ({len(ordinal_features)}): Imputaci√≥n + OrdinalEncoder\")\n",
    "\n",
    "print(f\"\\nüìà DIVISI√ìN DE DATOS:\")\n",
    "print(f\"   ‚Ä¢ Entrenamiento: {X_train.shape[0]:,} muestras ({(X_train.shape[0]/(X_train.shape[0]+X_test.shape[0]))*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Evaluaci√≥n: {X_test.shape[0]:,} muestras ({(X_test.shape[0]/(X_train.shape[0]+X_test.shape[0]))*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Estratificaci√≥n: ‚úì Activada (mantiene proporci√≥n de clases)\")\n",
    "\n",
    "print(f\"\\nüíæ ARTEFACTOS GENERADOS:\")\n",
    "print(f\"   1. preprocessor.joblib - Pipeline de transformaci√≥n ajustado\")\n",
    "print(f\"   2. X_train.csv - Features de entrenamiento transformados\")\n",
    "print(f\"   3. X_test.csv - Features de evaluaci√≥n transformados\")\n",
    "print(f\"   4. y_train.csv - Labels de entrenamiento\")\n",
    "print(f\"   5. y_test.csv - Labels de evaluaci√≥n\")\n",
    "print(f\"   6. feature_engineering_metadata.json - Metadatos del proceso\")\n",
    "\n",
    "print(f\"\\n‚úÖ CHECKLIST DE CALIDAD:\")\n",
    "print(f\"   ‚úì Features derivados basados en EDA\")\n",
    "print(f\"   ‚úì Pipelines sklearn implementados\")\n",
    "print(f\"   ‚úì Imputaci√≥n de valores faltantes\")\n",
    "print(f\"   ‚úì Escalado de variables num√©ricas\")\n",
    "print(f\"   ‚úì Codificaci√≥n de variables categ√≥ricas\")\n",
    "print(f\"   ‚úì Separaci√≥n train-test estratificada\")\n",
    "print(f\"   ‚úì Sin data leakage (fit solo en train)\")\n",
    "print(f\"   ‚úì Artefactos guardados para reproducibilidad\")\n",
    "\n",
    "print(f\"\\nüöÄ PR√ìXIMOS PASOS:\")\n",
    "print(f\"   1. Ejecutar notebook/script de model_training_evaluation.ipynb\")\n",
    "print(f\"   2. Entrenar modelos de clasificaci√≥n\")\n",
    "print(f\"   3. Evaluar performance con m√©tricas\")\n",
    "print(f\"   4. Analizar feature importance\")\n",
    "print(f\"   5. Ajustar hiperpar√°metros\")\n",
    "\n",
    "print(f\"\\nüí° RECOMENDACIONES:\")\n",
    "print(f\"   ‚Ä¢ Revisar feature importance despu√©s del entrenamiento\")\n",
    "print(f\"   ‚Ä¢ Considerar eliminar features poco importantes\")\n",
    "print(f\"   ‚Ä¢ Evaluar si agregar m√°s features derivados mejora el modelo\")\n",
    "print(f\"   ‚Ä¢ Monitorear data drift en producci√≥n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ FEATURE ENGINEERING COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232516d",
   "metadata": {},
   "source": [
    "## 10. Verificaci√≥n Final (Opcional)\n",
    "\n",
    "Celdas opcionales para verificar que todo se guard√≥ correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4cec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que los archivos existen y pueden ser cargados\n",
    "print(\"üîç Verificando artefactos guardados...\\n\")\n",
    "\n",
    "# 1. Verificar preprocessor\n",
    "try:\n",
    "    loaded_preprocessor = joblib.load(preprocessor_path)\n",
    "    print(\"‚úì preprocessor.joblib se puede cargar correctamente\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar preprocessor: {e}\")\n",
    "\n",
    "# 2. Verificar datasets\n",
    "try:\n",
    "    X_train_loaded = pd.read_csv(X_train_path)\n",
    "    X_test_loaded = pd.read_csv(X_test_path)\n",
    "    y_train_loaded = pd.read_csv(y_train_path)\n",
    "    y_test_loaded = pd.read_csv(y_test_path)\n",
    "    \n",
    "    print(\"‚úì Todos los CSV se pueden cargar correctamente\")\n",
    "    print(f\"  X_train: {X_train_loaded.shape}\")\n",
    "    print(f\"  X_test: {X_test_loaded.shape}\")\n",
    "    print(f\"  y_train: {y_train_loaded.shape}\")\n",
    "    print(f\"  y_test: {y_test_loaded.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar datasets: {e}\")\n",
    "\n",
    "# 3. Verificar metadata\n",
    "try:\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        loaded_metadata = json.load(f)\n",
    "    print(\"\\n‚úì Metadata cargado correctamente:\")\n",
    "    for key, value in loaded_metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar metadata: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Verificaci√≥n completada - Todos los artefactos est√°n listos para el siguiente paso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
