{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9de4ae",
   "metadata": {},
   "source": [
    "# Paso 3: Entrenamiento y Evaluaci√≥n de Modelos\n",
    "\n",
    "Este notebook implementa el **Paso 3 del Pipeline MLOps**, donde se:\n",
    "1. Cargan los datos transformados del paso anterior (Feature Engineering)\n",
    "2. Entrenan m√∫ltiples modelos supervisados\n",
    "3. Eval√∫an los modelos con m√©tricas de clasificaci√≥n\n",
    "4. Seleccionan el mejor modelo basado en performance\n",
    "5. Guardan el modelo y resultados\n",
    "\n",
    "## Checklist de Requisitos\n",
    "\n",
    "- [x] Se entrenan m√∫ltiples modelos supervisados (RandomForest, XGBoost, LogisticRegression, SVM, KNN, DecisionTree)\n",
    "- [x] Se utiliza una funci√≥n `build_model()` para estructurar el entrenamiento repetible\n",
    "- [x] Se aplican t√©cnicas de validaci√≥n (train/test split)\n",
    "- [x] Se guarda el objeto del modelo seleccionado\n",
    "- [x] Se utiliza la funci√≥n `summarize_classification()` para resumir m√©tricas\n",
    "- [x] Se comparan modelos con m√©tricas: accuracy, precision, recall, F1-score, ROC-AUC\n",
    "- [x] Se presentan gr√°ficos comparativos (matriz de confusi√≥n, gr√°ficos de barras)\n",
    "- [x] Se justifica la selecci√≥n del modelo final\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852dd63",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos\n",
    "Notebook para entrenar y comparar modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f29f7",
   "metadata": {},
   "source": [
    "## 1. Importar Librer√≠as Necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f376abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones de an√°lisis y visualizaci√≥n\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Importaciones de sklearn - Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Importaciones de sklearn - M√©tricas\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    roc_auc_score, \n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "project_root = Path.cwd().parent.parent\n",
    "data_dir = project_root / \"data\" / \"processed\"\n",
    "artifacts_dir = project_root / \"artifacts\"\n",
    "\n",
    "print(\"‚úì Todas las librer√≠as importadas exitosamente\")\n",
    "print(f\"‚úì Ruta del proyecto: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fe5f7",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos Procesados del Paso Anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafd344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data():\n",
    "    \"\"\"\n",
    "    Carga los datasets procesados del paso anterior (Feature Engineering).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: Si faltan archivos de datos procesados\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"CARGANDO DATOS PROCESADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Definir archivos requeridos\n",
    "    required_files = {\n",
    "        'X_train': data_dir / \"X_train.csv\",\n",
    "        'X_test': data_dir / \"X_test.csv\",\n",
    "        'y_train': data_dir / \"y_train.csv\",\n",
    "        'y_test': data_dir / \"y_test.csv\"\n",
    "    }\n",
    "    \n",
    "    # Verificar que existan todos los archivos\n",
    "    missing_files = [name for name, path in required_files.items() if not path.exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Faltan archivos de datos procesados: {missing_files}\\n\"\n",
    "            \"Por favor, ejecuta primero ft_engineering.py o el notebook correspondiente\"\n",
    "        )\n",
    "    \n",
    "    # Cargar datos\n",
    "    print(\"\\nüìÇ Cargando archivos CSV...\")\n",
    "    X_train = pd.read_csv(required_files['X_train'])\n",
    "    X_test = pd.read_csv(required_files['X_test'])\n",
    "    y_train = pd.read_csv(required_files['y_train'])['Diagnosis']\n",
    "    y_test = pd.read_csv(required_files['y_test'])['Diagnosis']\n",
    "    \n",
    "    print(f\"\\n‚úì Datos cargados exitosamente:\")\n",
    "    print(f\"   X_train: {X_train.shape} - Caracter√≠sticas de entrenamiento\")\n",
    "    print(f\"   X_test: {X_test.shape}  - Caracter√≠sticas de evaluaci√≥n\")\n",
    "    print(f\"   y_train: {y_train.shape} - Etiquetas de entrenamiento\")\n",
    "    print(f\"   y_test: {y_test.shape}  - Etiquetas de evaluaci√≥n\")\n",
    "    \n",
    "    # Mostrar distribuci√≥n de clases\n",
    "    print(f\"\\nüìä Distribuci√≥n de clases en y_train:\")\n",
    "    print(y_train.value_counts().sort_index())\n",
    "    print(f\"\\nüìä Distribuci√≥n de clases en y_test:\")\n",
    "    print(y_test.value_counts().sort_index())\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Ejecutar la carga de datos\n",
    "X_train, X_test, y_train, y_test = load_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5fd2",
   "metadata": {},
   "source": [
    "## 3. Definir M√∫ltiples Modelos Supervisados\n",
    "\n",
    "Se entrenar√°n 6 algoritmos diferentes para comparar performance:\n",
    "\n",
    "| Modelo | Tipo | Ventajas |\n",
    "|--------|------|----------|\n",
    "| Logistic Regression | Lineal | R√°pido, interpretable |\n",
    "| Random Forest | Ensemble | Robusto, maneja no-linealidades |\n",
    "| Gradient Boosting | Ensemble | Alta performance, previene overfitting |\n",
    "| Decision Tree | √Årbol | Interpretable, r√°pido |\n",
    "| KNN | Instancia | Flexible, sin suposiciones |\n",
    "| SVM | Kernel | Bueno en altas dimensiones |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6f72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name):\n",
    "    \"\"\"\n",
    "    Construye y retorna un modelo supervisado espec√≠fico.\n",
    "    Esta funci√≥n permite reutilizar el entrenamiento de forma estructurada.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Nombre del modelo a construir\n",
    "    \n",
    "    Returns:\n",
    "        Modelo de sklearn configurado\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Si el nombre del modelo no es v√°lido\n",
    "    \"\"\"\n",
    "    models_config = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'Decision Tree': DecisionTreeClassifier(\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(\n",
    "            n_neighbors=5,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'SVM': SVC(\n",
    "            kernel='rbf',\n",
    "            probability=True,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    if model_name not in models_config:\n",
    "        raise ValueError(f\"Modelo {model_name} no disponible. Opciones: {list(models_config.keys())}\")\n",
    "    \n",
    "    return models_config[model_name]\n",
    "\n",
    "# Obtener lista de modelos a entrenar\n",
    "def get_models_to_train():\n",
    "    \"\"\"\n",
    "    Define los modelos a entrenar y evaluar.\n",
    "    Retorna un diccionario con instancias de cada modelo.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {nombre_modelo: instancia_modelo}\n",
    "    \"\"\"\n",
    "    model_names = [\n",
    "        'Logistic Regression',\n",
    "        'Random Forest',\n",
    "        'Gradient Boosting',\n",
    "        'Decision Tree',\n",
    "        'KNN',\n",
    "        'SVM'\n",
    "    ]\n",
    "    \n",
    "    models = {}\n",
    "    for model_name in model_names:\n",
    "        models[model_name] = build_model(model_name)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Inicializar modelos\n",
    "models = get_models_to_train()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODELOS CONFIGURADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìö Total de modelos a entrenar: {len(models)}\")\n",
    "for i, model_name in enumerate(models.keys(), 1):\n",
    "    print(f\"   {i}. {model_name}: {type(models[model_name]).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257811a2",
   "metadata": {},
   "source": [
    "## 4. Funci√≥n para Resumir M√©tricas de Clasificaci√≥n\n",
    "\n",
    "Se implementa la funci√≥n `summarize_classification()` para obtener un resumen completo de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3836511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(model, X_train, X_test, y_train, y_test, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Funci√≥n para resumir y retornar todas las m√©tricas de un modelo de clasificaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        X_train: Features de entrenamiento\n",
    "        X_test: Features de prueba\n",
    "        y_train: Etiquetas de entrenamiento\n",
    "        y_test: Etiquetas de prueba\n",
    "        model_name: Nombre del modelo (para reportes)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con todas las m√©tricas y predicciones\n",
    "    \"\"\"\n",
    "    # Predicciones en ambos conjuntos\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Obtener probabilidades si es posible\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test)\n",
    "        if y_test_proba.shape[1] == 2:  # Clasificaci√≥n binaria\n",
    "            y_test_proba = y_test_proba[:, 1]\n",
    "        else:\n",
    "            y_test_proba = None\n",
    "    except AttributeError:\n",
    "        y_test_proba = None\n",
    "    \n",
    "    # M√©tricas de entrenamiento\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # M√©tricas de evaluaci√≥n/prueba\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC-AUC (para clasificaci√≥n binaria)\n",
    "    try:\n",
    "        if y_test_proba is not None and len(np.unique(y_test)) == 2:\n",
    "            roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "        else:\n",
    "            roc_auc = None\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Calcular overfitting (diferencia entre train y test accuracy)\n",
    "    overfitting = train_accuracy - test_accuracy\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    # Reporte de clasificaci√≥n\n",
    "    clf_report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    # Resumen completo\n",
    "    summary = {\n",
    "        'model_name': model_name,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'overfitting': overfitting,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': clf_report,\n",
    "        'y_pred': y_test_pred,\n",
    "        'y_proba': y_test_proba\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"‚úì Funci√≥n summarize_classification() definida correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d1487",
   "metadata": {},
   "source": [
    "## 5. Entrenar y Evaluar Todos los Modelos\n",
    "\n",
    "Se entrena cada modelo con los datos de entrenamiento y se eval√∫a con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006717fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(models, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Entrena y eval√∫a todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        models: Diccionario de modelos a entrenar\n",
    "        X_train, X_test: Features de entrenamiento y prueba\n",
    "        y_train, y_test: Etiquetas de entrenamiento y prueba\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_df, trained_models, all_summaries)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = []\n",
    "    trained_models = {}\n",
    "    all_summaries = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nüìö Entrenando: {model_name}...\")\n",
    "        \n",
    "        # Medir tiempo de entrenamiento\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   ‚úì Entrenamiento completado en {training_time:.2f} segundos\")\n",
    "        \n",
    "        # Evaluar modelo\n",
    "        print(f\"   üìä Evaluando modelo...\")\n",
    "        summary = summarize_classification(\n",
    "            model, X_train, X_test, y_train, y_test, model_name\n",
    "        )\n",
    "        \n",
    "        # Guardar resumen\n",
    "        all_summaries[model_name] = summary\n",
    "        trained_models[model_name] = model\n",
    "        \n",
    "        # Crear fila de resultados\n",
    "        result = {\n",
    "            'Modelo': model_name,\n",
    "            'Train Accuracy': round(summary['train_accuracy'], 4),\n",
    "            'Test Accuracy': round(summary['test_accuracy'], 4),\n",
    "            'Precision': round(summary['precision'], 4),\n",
    "            'Recall': round(summary['recall'], 4),\n",
    "            'F1-Score': round(summary['f1_score'], 4),\n",
    "            'ROC-AUC': round(summary['roc_auc'], 4) if summary['roc_auc'] else None,\n",
    "            'Overfitting': round(summary['overfitting'], 4),\n",
    "            'Training Time (s)': round(training_time, 2)\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"   ‚úì Test Accuracy: {result['Test Accuracy']:.4f}\")\n",
    "        print(f\"   ‚úì F1-Score: {result['F1-Score']:.4f}\")\n",
    "        if result['ROC-AUC']:\n",
    "            print(f\"   ‚úì ROC-AUC: {result['ROC-AUC']:.4f}\")\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df, trained_models, all_summaries\n",
    "\n",
    "# Ejecutar entrenamiento y evaluaci√≥n\n",
    "results_df, trained_models, all_summaries = train_and_evaluate_models(\n",
    "    models, X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb2324",
   "metadata": {},
   "source": [
    "## 6. Comparaci√≥n de Modelos - Tabla de Resultados\n",
    "\n",
    "Resumen de todas las m√©tricas para comparar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar tabla de resultados con formato\n",
    "print(\"üìä RESULTADOS DE TODOS LOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# An√°lisis estad√≠stico\n",
    "print(\"\\nüìà ESTAD√çSTICAS POR M√âTRICA:\")\n",
    "print(\"\\nAccuracy en Test:\")\n",
    "print(f\"   Media: {results_df['Test Accuracy'].mean():.4f}\")\n",
    "print(f\"   Desv. Est.: {results_df['Test Accuracy'].std():.4f}\")\n",
    "print(f\"   Mejor: {results_df['Test Accuracy'].max():.4f}\")\n",
    "print(f\"   Peor: {results_df['Test Accuracy'].min():.4f}\")\n",
    "\n",
    "print(\"\\nF1-Score:\")\n",
    "print(f\"   Media: {results_df['F1-Score'].mean():.4f}\")\n",
    "print(f\"   Desv. Est.: {results_df['F1-Score'].std():.4f}\")\n",
    "print(f\"   Mejor: {results_df['F1-Score'].max():.4f}\")\n",
    "print(f\"   Peor: {results_df['F1-Score'].min():.4f}\")\n",
    "\n",
    "print(\"\\nOverfitting (Train-Test Accuracy):\")\n",
    "print(f\"   Media: {results_df['Overfitting'].mean():.4f}\")\n",
    "print(f\"   M√°ximo: {results_df['Overfitting'].max():.4f}\")\n",
    "print(f\"   M√≠nimo: {results_df['Overfitting'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b15b2a",
   "metadata": {},
   "source": [
    "## 7. Gr√°ficos Comparativos de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.1 Gr√°fico de Barras - Comparaci√≥n de M√©tricas\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Preparar datos\n",
    "metrics_to_plot = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.2\n",
    "\n",
    "# Crear barras para cada m√©trica\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    offset = width * (i - len(metrics_to_plot)/2 + 0.5)\n",
    "    ax.bar(x + offset, results_df[metric], width, label=metric, color=colors[i], alpha=0.8)\n",
    "\n",
    "# Personalizaci√≥n\n",
    "ax.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaci√≥n de Modelos - M√©tricas de Evaluaci√≥n', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results_df['Modelo'], rotation=45, ha='right')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(artifacts_dir / \"model_comparison.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Gr√°fico comparativo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.2 Gr√°fico de Matriz de Correlaci√≥n entre M√©tricas\n",
    "\n",
    "# Seleccionar columnas num√©ricas relevantes\n",
    "metrics_cols = ['Train Accuracy', 'Test Accuracy', 'Precision', 'Recall', 'F1-Score', 'Overfitting']\n",
    "correlation_matrix = results_df[metrics_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            cbar_kws={'label': 'Correlaci√≥n'}, ax=ax, vmin=-1, vmax=1)\n",
    "ax.set_title('Correlaci√≥n entre M√©tricas de Rendimiento', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(artifacts_dir / \"metrics_correlation.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Matriz de correlaci√≥n guardada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.3 An√°lisis de Overfitting y Tiempo de Entrenamiento\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico 1: Overfitting\n",
    "ax1 = axes[0]\n",
    "colors_overfit = ['#27AE60' if x < 0.05 else '#E67E22' if x < 0.1 else '#E74C3C' \n",
    "                  for x in results_df['Overfitting']]\n",
    "bars1 = ax1.bar(results_df['Modelo'], results_df['Overfitting'], color=colors_overfit, alpha=0.8)\n",
    "ax1.axhline(y=0.05, color='green', linestyle='--', linewidth=2, label='Bueno (<5%)', alpha=0.7)\n",
    "ax1.axhline(y=0.1, color='orange', linestyle='--', linewidth=2, label='Aceptable (5-10%)', alpha=0.7)\n",
    "ax1.set_ylabel('Diferencia (Train - Test Accuracy)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('An√°lisis de Overfitting por Modelo', fontsize=12, fontweight='bold')\n",
    "ax1.set_xticklabels(results_df['Modelo'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Gr√°fico 2: Tiempo de entrenamiento\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(results_df['Modelo'], results_df['Training Time (s)'], color='#3498DB', alpha=0.8)\n",
    "ax2.set_ylabel('Tiempo (segundos)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Tiempo de Entrenamiento por Modelo', fontsize=12, fontweight='bold')\n",
    "ax2.set_xticklabels(results_df['Modelo'], rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(artifacts_dir / \"overfitting_time_analysis.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì An√°lisis de overfitting y tiempo guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d5f60",
   "metadata": {},
   "source": [
    "## 8. Selecci√≥n del Mejor Modelo\n",
    "\n",
    "Se selecciona el modelo √≥ptimo basado en un sistema de criterios jer√°rquicos:\n",
    "1. **F1-Score**: M√©trica principal (balance precision-recall)\n",
    "2. **Test Accuracy**: M√©trica secundaria\n",
    "3. **Overfitting**: Preferir modelos con menor brecha train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model(results_df, trained_models):\n",
    "    \"\"\"\n",
    "    Selecciona el mejor modelo basado en criterios jer√°rquicos:\n",
    "    1. F1-Score (principal)\n",
    "    2. Test Accuracy (secundario)\n",
    "    3. Overfitting (preferir bajo)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (best_model_name, best_model, results_sorted)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SELECCI√ìN DEL MEJOR MODELO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ordenar por criterios\n",
    "    results_sorted = results_df.sort_values(\n",
    "        by=['F1-Score', 'Test Accuracy', 'Overfitting'],\n",
    "        ascending=[False, False, True]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nüìä Ranking de modelos (ordenados por rendimiento):\")\n",
    "    print(results_sorted[['Modelo', 'F1-Score', 'Test Accuracy', 'Precision', 'Recall', 'Overfitting']].to_string(index=False))\n",
    "    \n",
    "    # Seleccionar el mejor\n",
    "    best_row = results_sorted.iloc[0]\n",
    "    best_model_name = best_row['Modelo']\n",
    "    best_model = trained_models[best_model_name]\n",
    "    \n",
    "    return best_model_name, best_model, results_sorted\n",
    "\n",
    "# Ejecutar selecci√≥n\n",
    "best_model_name, best_model, results_sorted = select_best_model(results_df, trained_models)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"üèÜ MEJOR MODELO SELECCIONADO: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Mostrar detalles del mejor modelo\n",
    "best_row = results_sorted.iloc[0]\n",
    "print(f\"\\nüìä M√âTRICAS DEL MEJOR MODELO:\")\n",
    "print(f\"   ‚îú‚îÄ Test Accuracy:     {best_row['Test Accuracy']:.4f}\")\n",
    "print(f\"   ‚îú‚îÄ F1-Score:          {best_row['F1-Score']:.4f}\")\n",
    "print(f\"   ‚îú‚îÄ Precision:         {best_row['Precision']:.4f}\")\n",
    "print(f\"   ‚îú‚îÄ Recall:            {best_row['Recall']:.4f}\")\n",
    "if best_row['ROC-AUC'] is not None:\n",
    "    print(f\"   ‚îú‚îÄ ROC-AUC:           {best_row['ROC-AUC']:.4f}\")\n",
    "print(f\"   ‚îú‚îÄ Train Accuracy:    {best_row['Train Accuracy']:.4f}\")\n",
    "print(f\"   ‚îú‚îÄ Overfitting:       {best_row['Overfitting']:.4f}\")\n",
    "print(f\"   ‚îî‚îÄ Training Time:     {best_row['Training Time (s)']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b488e",
   "metadata": {},
   "source": [
    "## 9. Justificaci√≥n de la Selecci√≥n del Modelo\n",
    "\n",
    "Se eval√∫an varios aspectos clave para justificar la selecci√≥n del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d78b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.1 Justificaci√≥n de Performance\n",
    "\n",
    "# An√°lisis de rendimiento\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JUSTIFICACI√ìN DE LA SELECCI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_f1 = results_sorted.iloc[0]['F1-Score']\n",
    "mean_f1 = results_df['F1-Score'].mean()\n",
    "best_accuracy = results_sorted.iloc[0]['Test Accuracy']\n",
    "mean_accuracy = results_df['Test Accuracy'].mean()\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ F1-Score del mejor modelo: {best_f1:.4f}\")\n",
    "print(f\"   ‚Ä¢ F1-Score promedio: {mean_f1:.4f}\")\n",
    "print(f\"   ‚Ä¢ Mejora respecto a promedio: {(best_f1 - mean_f1):.4f} ({((best_f1 - mean_f1)/mean_f1 * 100):.2f}%)\")\n",
    "print(f\"\\n   ‚Ä¢ Test Accuracy del mejor: {best_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Accuracy promedio: {mean_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Mejora respecto a promedio: {(best_accuracy - mean_accuracy):.4f} ({((best_accuracy - mean_accuracy)/mean_accuracy * 100):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70597ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.2 Justificaci√≥n de Consistencia\n",
    "\n",
    "best_overfit = results_sorted.iloc[0]['Overfitting']\n",
    "mean_overfit = results_df['Overfitting'].mean()\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ CONSISTENCIA (Control de Overfitting):\")\n",
    "print(f\"   ‚Ä¢ Overfitting del mejor modelo: {best_overfit:.4f}\")\n",
    "print(f\"   ‚Ä¢ Overfitting promedio: {mean_overfit:.4f}\")\n",
    "\n",
    "if best_overfit < 0.05:\n",
    "    print(f\"   ‚úÖ Excelente: Overfitting < 5% (muy buena generalizaci√≥n)\")\n",
    "elif best_overfit < 0.1:\n",
    "    print(f\"   ‚úÖ Bueno: Overfitting < 10% (buena generalizaci√≥n)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Aceptable: Overfitting entre 10-15%\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Train-Test Gap: {best_overfit:.4f}\")\n",
    "print(f\"   ‚Ä¢ Diferencia respecto a promedio: {(best_overfit - mean_overfit):.4f}\")\n",
    "\n",
    "# Comparaci√≥n con otros modelos\n",
    "better_than_count = (results_df['Overfitting'] > best_overfit).sum()\n",
    "print(f\"   ‚Ä¢ Mejor overfitting que {better_than_count}/{len(results_df)} modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.3 Justificaci√≥n de Escalabilidad\n",
    "\n",
    "best_time = results_sorted.iloc[0]['Training Time (s)']\n",
    "mean_time = results_df['Training Time (s)'].mean()\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ ESCALABILIDAD (Eficiencia Computacional):\")\n",
    "print(f\"   ‚Ä¢ Tiempo de entrenamiento: {best_time:.2f} segundos\")\n",
    "print(f\"   ‚Ä¢ Tiempo promedio: {mean_time:.2f} segundos\")\n",
    "print(f\"   ‚Ä¢ Factor de eficiencia: {(mean_time / best_time):.2f}x\")\n",
    "\n",
    "if best_time < mean_time:\n",
    "    print(f\"   ‚úÖ M√°s r√°pido que el promedio (+{((mean_time - best_time) / mean_time * 100):.1f}% m√°s eficiente)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è M√°s lento que el promedio ({((best_time - mean_time) / mean_time * 100):.1f}% menos eficiente)\")\n",
    "\n",
    "print(f\"\\n   ‚Ä¢ Ranking de velocidad: {(results_df['Training Time (s)'] > best_time).sum() + 1}/{len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3547b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9.4 Visualizaci√≥n - Comparaci√≥n del Mejor Modelo\n",
    "\n",
    "# Crear visualizaci√≥n de posici√≥n del mejor modelo\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Posici√≥n en ranking\n",
    "models_list = results_sorted['Modelo'].tolist()\n",
    "positions = np.arange(len(models_list))\n",
    "colors_rank = ['#27AE60' if i == 0 else '#3498DB' for i in range(len(models_list))]\n",
    "\n",
    "bars = ax.barh(positions, results_sorted['F1-Score'].values, color=colors_rank, alpha=0.8)\n",
    "ax.set_yticks(positions)\n",
    "ax.set_yticklabels(models_list)\n",
    "ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Ranking de Modelos por F1-Score\\nüèÜ Ganador: {best_model_name}', \n",
    "             fontsize=13, fontweight='bold', color='#27AE60')\n",
    "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for i, (bar, value) in enumerate(zip(bars, results_sorted['F1-Score'].values)):\n",
    "    ax.text(value - 0.02, bar.get_y() + bar.get_height()/2, \n",
    "            f'{value:.4f}', ha='right', va='center', fontweight='bold', color='white', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(artifacts_dir / \"model_ranking.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Ranking de modelos guardado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904ec5f",
   "metadata": {},
   "source": [
    "## 10. An√°lisis Detallado del Mejor Modelo\n",
    "\n",
    "Se presentan visualizaciones adicionales para el modelo seleccionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.1 Matriz de Confusi√≥n del Mejor Modelo\n",
    "\n",
    "# Obtener datos del mejor modelo\n",
    "best_summary = all_summaries[best_model_name]\n",
    "cm_best = best_summary['confusion_matrix']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,\n",
    "            xticklabels=['Negativo', 'Positivo'], yticklabels=['Negativo', 'Positivo'])\n",
    "ax.set_ylabel('Verdadero', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Predicho', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Matriz de Confusi√≥n - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(artifacts_dir / \"confusion_matrix_best_model.png\"), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Matriz de confusi√≥n guardada\")\n",
    "\n",
    "# Calcular m√©tricas adicionales desde la matriz\n",
    "tn, fp, fn, tp = cm_best.ravel()\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä An√°lisis de la Matriz de Confusi√≥n:\")\n",
    "print(f\"   ‚Ä¢ Verdaderos Positivos (TP): {tp}\")\n",
    "print(f\"   ‚Ä¢ Falsos Positivos (FP): {fp}\")\n",
    "print(f\"   ‚Ä¢ Verdaderos Negativos (TN): {tn}\")\n",
    "print(f\"   ‚Ä¢ Falsos Negativos (FN): {fn}\")\n",
    "print(f\"   ‚Ä¢ Sensibilidad (TPR): {sensitivity:.4f}\")\n",
    "print(f\"   ‚Ä¢ Especificidad (TNR): {specificity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dd26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.2 Reporte de Clasificaci√≥n Detallado\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REPORTE DE CLASIFICACI√ìN DETALLADO - MEJOR MODELO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mostrar reporte\n",
    "report_dict = best_summary['classification_report']\n",
    "print(f\"\\n{classification_report(y_test, best_summary['y_pred'], target_names=['Negativo', 'Positivo'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.3 Curva ROC del Mejor Modelo (si es aplicable)\n",
    "\n",
    "if best_summary['y_proba'] is not None and len(np.unique(y_test)) == 2:\n",
    "    # Calcular curva ROC\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, best_summary['y_proba'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(9, 8))\n",
    "    \n",
    "    # Curva ROC\n",
    "    ax.plot(fpr, tpr, color='#2E86C1', linewidth=2.5, \n",
    "            label=f'Curva ROC (AUC = {roc_auc:.4f})')\n",
    "    \n",
    "    # Diagonal de referencia (clasificador aleatorio)\n",
    "    ax.plot([0, 1], [0, 1], color='#E74C3C', linewidth=2, linestyle='--', \n",
    "            label='Clasificador Aleatorio (AUC = 0.5000)')\n",
    "    \n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    ax.set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'Curva ROC - {best_model_name}', fontsize=13, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.grid(alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(artifacts_dir / \"roc_curve_best_model.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Curva ROC guardada\")\n",
    "    print(f\"\\nüìä ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    print(f\"   ‚Ä¢ AUC > 0.9: Excelente\")\n",
    "    print(f\"   ‚Ä¢ AUC > 0.8: Bueno\")\n",
    "    print(f\"   ‚Ä¢ AUC > 0.7: Aceptable\")\n",
    "    print(f\"   ‚Ä¢ AUC < 0.7: Pobre\")\n",
    "    \n",
    "    if roc_auc > 0.9:\n",
    "        print(f\"   ‚úÖ Clasificador excelente\")\n",
    "    elif roc_auc > 0.8:\n",
    "        print(f\"   ‚úÖ Clasificador bueno\")\n",
    "    elif roc_auc > 0.7:\n",
    "        print(f\"   ‚úÖ Clasificador aceptable\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Clasificador pobre\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se puede generar curva ROC (modelos no probabil√≠sticos o multiclase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab617a2",
   "metadata": {},
   "source": [
    "## 11. Guardar el Modelo Entrenado y Artefactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb431eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_and_model(best_model_name, best_model, results_df):\n",
    "    \"\"\"\n",
    "    Guarda el modelo entrenado y los resultados de evaluaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        best_model_name: Nombre del mejor modelo\n",
    "        best_model: Objeto del mejor modelo entrenado\n",
    "        results_df: DataFrame con resultados de evaluaci√≥n\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GUARDANDO ARTEFACTOS DEL MODELO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Crear directorio de artefactos si no existe\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Guardar el mejor modelo\n",
    "    model_path = artifacts_dir / \"best_model.joblib\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\n‚úÖ Modelo guardado exitosamente\")\n",
    "    print(f\"   üìÅ Ubicaci√≥n: {model_path}\")\n",
    "    print(f\"   üìù Modelo: {best_model_name}\")\n",
    "    print(f\"   üîß Tipo: {type(best_model).__name__}\")\n",
    "    \n",
    "    # 2. Guardar metadata del modelo\n",
    "    metadata = {\n",
    "        'model_name': best_model_name,\n",
    "        'model_type': type(best_model).__name__,\n",
    "        'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'features_count': X_train.shape[1],\n",
    "        'training_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0],\n",
    "        'metrics': {\n",
    "            'test_accuracy': float(all_summaries[best_model_name]['test_accuracy']),\n",
    "            'f1_score': float(all_summaries[best_model_name]['f1_score']),\n",
    "            'precision': float(all_summaries[best_model_name]['precision']),\n",
    "            'recall': float(all_summaries[best_model_name]['recall']),\n",
    "            'roc_auc': float(all_summaries[best_model_name]['roc_auc']) if all_summaries[best_model_name]['roc_auc'] else None,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = artifacts_dir / \"model_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "    print(f\"\\n‚úÖ Metadata del modelo guardada\")\n",
    "    print(f\"   üìÅ Ubicaci√≥n: {metadata_path}\")\n",
    "    \n",
    "    # 3. Guardar resultados de evaluaci√≥n de todos los modelos\n",
    "    results_path = artifacts_dir / \"model_evaluation_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    print(f\"\\n‚úÖ Resultados de evaluaci√≥n guardados\")\n",
    "    print(f\"   üìÅ Ubicaci√≥n: {results_path}\")\n",
    "    print(f\"   üìä Modelos evaluados: {len(results_df)}\")\n",
    "    \n",
    "    # 4. Guardar resumen en formato JSON\n",
    "    summary_path = artifacts_dir / \"training_summary.json\"\n",
    "    training_summary = {\n",
    "        'best_model': best_model_name,\n",
    "        'total_models_trained': len(results_df),\n",
    "        'evaluation_metrics': results_df.to_dict('records'),\n",
    "        'timestamp': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(training_summary, f, indent=4)\n",
    "    print(f\"\\n‚úÖ Resumen de entrenamiento guardado\")\n",
    "    print(f\"   üìÅ Ubicaci√≥n: {summary_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ú® TODOS LOS ARTEFACTOS GUARDADOS EN: {artifacts_dir}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': str(model_path),\n",
    "        'metadata_path': str(metadata_path),\n",
    "        'results_path': str(results_path),\n",
    "        'summary_path': str(summary_path)\n",
    "    }\n",
    "\n",
    "# Ejecutar guardado de artefactos\n",
    "saved_paths = save_results_and_model(best_model_name, best_model, results_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a61a00",
   "metadata": {},
   "source": [
    "## 12. Resumen Ejecutivo y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d6374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO - PASO 3: ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "üìä RESULTADOS GLOBALES:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  ‚úì Modelos entrenados: {len(models)}\n",
    "  ‚úì Modelos evaluados: {len(results_df)}\n",
    "  ‚úì Mejor modelo seleccionado: {best_model_name}\n",
    "\n",
    "üèÜ MEJOR MODELO: {best_model_name}\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  Rendimiento:\n",
    "    ‚Ä¢ Test Accuracy:  {results_sorted.iloc[0]['Test Accuracy']:.4f} ({results_sorted.iloc[0]['Test Accuracy']*100:.2f}%)\n",
    "    ‚Ä¢ F1-Score:       {results_sorted.iloc[0]['F1-Score']:.4f}\n",
    "    ‚Ä¢ Precision:      {results_sorted.iloc[0]['Precision']:.4f}\n",
    "    ‚Ä¢ Recall:         {results_sorted.iloc[0]['Recall']:.4f}\n",
    "    \n",
    "  Consistencia:\n",
    "    ‚Ä¢ Train Accuracy: {results_sorted.iloc[0]['Train Accuracy']:.4f}\n",
    "    ‚Ä¢ Overfitting:    {results_sorted.iloc[0]['Overfitting']:.4f}\n",
    "    \n",
    "  Eficiencia:\n",
    "    ‚Ä¢ Tiempo de entrenamiento: {results_sorted.iloc[0]['Training Time (s)']:.2f} segundos\n",
    "\n",
    "üìà COMPARACI√ìN CON OTROS MODELOS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  Mejor F1-Score:     {results_sorted['F1-Score'].max():.4f} (Ganador: {results_sorted.iloc[0]['Modelo']})\n",
    "  Peor F1-Score:      {results_sorted['F1-Score'].min():.4f}\n",
    "  Promedio F1-Score:  {results_sorted['F1-Score'].mean():.4f}\n",
    "  \n",
    "  Rango de Accuracy:  {results_sorted['Test Accuracy'].min():.4f} - {results_sorted['Test Accuracy'].max():.4f}\n",
    "\n",
    "üíæ ARTEFACTOS GUARDADOS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  ‚úì Modelo: {saved_paths['model_path']}\n",
    "  ‚úì Metadata: {saved_paths['metadata_path']}\n",
    "  ‚úì Resultados: {saved_paths['results_path']}\n",
    "  ‚úì Resumen: {saved_paths['summary_path']}\n",
    "  ‚úì Gr√°ficos:\n",
    "    - model_comparison.png\n",
    "    - metrics_correlation.png\n",
    "    - overfitting_time_analysis.png\n",
    "    - model_ranking.png\n",
    "    - confusion_matrix_best_model.png\n",
    "    - roc_curve_best_model.png (si aplica)\n",
    "\n",
    "‚úÖ PR√ìXIMOS PASOS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "  1. Ejecutar model_deploy.py para desplegar el modelo en producci√≥n\n",
    "  2. Utilizar el modelo para hacer predicciones\n",
    "  3. Monitorear el rendimiento en datos nuevos\n",
    "  4. Reentrenar peri√≥dicamente si se detecta drift\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ú® PASO 3 COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f12c05",
   "metadata": {},
   "source": [
    "## 13. Notas y Observaciones\n",
    "\n",
    "### Checklist de Requisitos - Estado Final\n",
    "\n",
    "‚úÖ **Se entrenan m√∫ltiples modelos supervisados**\n",
    "- Logistic Regression, Random Forest, Gradient Boosting, Decision Tree, KNN, SVM\n",
    "\n",
    "‚úÖ **Se utiliza funci√≥n `build_model()` para estructura repetible**\n",
    "- Funci√≥n `build_model(model_name)` permite construir y configurar cada modelo de forma modular\n",
    "\n",
    "‚úÖ **Se aplican t√©cnicas de validaci√≥n**\n",
    "- Train/Test split aplicado (70/30)\n",
    "- Datos procesados cargados del paso anterior\n",
    "\n",
    "‚úÖ **Se guarda el objeto del modelo seleccionado**\n",
    "- Modelo guardado como `best_model.joblib` con joblib\n",
    "- Metadata guardada en JSON con detalles del entrenamiento\n",
    "\n",
    "‚úÖ **Se utiliza funci√≥n `summarize_classification()`**\n",
    "- Funci√≥n implementada para calcular todas las m√©tricas de clasificaci√≥n\n",
    "- Retorna un resumen completo con predicciones y probabilidades\n",
    "\n",
    "‚úÖ **Se comparan modelos con m√∫ltiples m√©tricas**\n",
    "- Accuracy, Precision, Recall, F1-Score, ROC-AUC\n",
    "- An√°lisis de overfitting (Train-Test Accuracy gap)\n",
    "- Tiempo de entrenamiento\n",
    "\n",
    "‚úÖ **Se presentan gr√°ficos comparativos**\n",
    "- Gr√°fico de barras comparativo de m√©tricas\n",
    "- Matriz de confusi√≥n del mejor modelo\n",
    "- Curva ROC (si aplica)\n",
    "- Ranking de modelos\n",
    "- An√°lisis de overfitting\n",
    "\n",
    "‚úÖ **Se justifica la selecci√≥n del modelo final**\n",
    "- Performance: Comparaci√≥n con promedio\n",
    "- Consistencia: An√°lisis de overfitting\n",
    "- Escalabilidad: Eficiencia computacional\n",
    "- Ranking y posici√≥n en el conjunto\n",
    "\n",
    "### Consideraciones Importantes\n",
    "\n",
    "1. **Validaci√≥n**: Se utiliza el esquema de train/test split del paso anterior. Para datasets peque√±os, considerar k-fold cross-validation.\n",
    "\n",
    "2. **Clase Desbalanceada**: Si hay desbalance de clases, ajustar `class_weight='balanced'` en modelos que lo soporten.\n",
    "\n",
    "3. **Hyperpar√°metros**: Los par√°metros mostrados son valores por defecto. Para producci√≥n, realizar GridSearchCV.\n",
    "\n",
    "4. **Reproducibilidad**: Se utilizan `random_state=42` en todos los modelos para garantizar resultados reproducibles.\n",
    "\n",
    "5. **Probabilidades**: Se calculan curvas ROC solo para problemas de clasificaci√≥n binaria donde el modelo soporta `predict_proba()`.\n",
    "\n",
    "### Archivos Generados\n",
    "\n",
    "- `best_model.joblib`: Modelo entrenado listo para predicciones\n",
    "- `model_metadata.json`: Informaci√≥n sobre el modelo y su fecha de entrenamiento\n",
    "- `model_evaluation_results.csv`: Tabla con resultados de todos los modelos\n",
    "- `training_summary.json`: Resumen completo del entrenamiento\n",
    "- Gr√°ficos PNG: Visualizaciones para an√°lisis y reporte"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
