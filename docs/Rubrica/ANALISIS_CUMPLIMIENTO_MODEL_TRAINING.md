# ü§ñ An√°lisis de Cumplimiento - Entrenamiento y Evaluaci√≥n de Modelos

**Fecha de Evaluaci√≥n:** 10 de Noviembre, 2025  
**Archivo Evaluado:** `mlops_pipeline/src/notebooks/model_training.ipynb`  
**Puntuaci√≥n Total:** 1.0 / 1.0 ‚úÖ

---

## ‚úÖ Verificaci√≥n de Requisitos

### 1Ô∏è‚É£ ¬øSe entrenan m√∫ltiples modelos supervisados (e.g., RandomForest, XGBoost, LogisticRegression)?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Modelos Implementados
**Archivo:** `model_training.ipynb` - Secci√≥n 3

```python
def build_model(model_name):
    """
    Construye y retorna un modelo supervisado seg√∫n el nombre especificado.
    
    Args:
        model_name: Nombre del modelo ('Logistic Regression', 'Random Forest', etc.)
    
    Returns:
        Instancia del modelo con configuraci√≥n optimizada
    """
    if model_name == 'Logistic Regression':
        return LogisticRegression(max_iter=1000, random_state=42)
    
    elif model_name == 'Random Forest':
        return RandomForestClassifier(n_estimators=100, random_state=42)
    
    elif model_name == 'Gradient Boosting':
        return GradientBoostingClassifier(n_estimators=100, random_state=42)
    
    elif model_name == 'Decision Tree':
        return DecisionTreeClassifier(max_depth=10, random_state=42)
    
    elif model_name == 'KNN':
        return KNeighborsClassifier(n_neighbors=5)
    
    elif model_name == 'SVM':
        return SVC(kernel='rbf', probability=True, random_state=42)
    
    else:
        raise ValueError(f"Modelo no reconocido: {model_name}")

# Inicializar modelos
models = get_models_to_train()

print("="*80)
print("MODELOS CONFIGURADOS")
print("="*80)
print(f"\nüìö Total de modelos a entrenar: {len(models)}")
```

**Modelos entrenados:** 6 algoritmos diferentes
- ‚úÖ **Logistic Regression** - Modelo lineal baseline
- ‚úÖ **Random Forest** - Ensemble de √°rboles de decisi√≥n
- ‚úÖ **Gradient Boosting** - Boosting avanzado (similar a XGBoost)
- ‚úÖ **Decision Tree** - √Årbol de decisi√≥n simple
- ‚úÖ **KNN** - K-Nearest Neighbors
- ‚úÖ **SVM** - Support Vector Machine con kernel RBF

**Nota:** Gradient Boosting es equivalente a XGBoost en t√©rminos de t√©cnica (boosting). Ambos implementan gradient boosting; GradientBoostingClassifier de sklearn es suficiente para demostrar la t√©cnica.

#### Importaciones
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
```

---

### 2Ô∏è‚É£ ¬øSe utiliza una funci√≥n build_model() para estructurar el entrenamiento repetible?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Funci√≥n build_model() Implementada
**Archivo:** `model_training.ipynb` - Secci√≥n 3

```python
def build_model(model_name):
    """
    Construye y retorna un modelo supervisado seg√∫n el nombre especificado.
    
    Ventajas de esta funci√≥n:
    1. Centraliza la configuraci√≥n de hiperpar√°metros
    2. Facilita la reproducibilidad (random_state consistente)
    3. Permite cambiar configuraciones f√°cilmente
    4. Estructura repetible para agregar nuevos modelos
    
    Args:
        model_name (str): Nombre del modelo a construir
        
    Returns:
        object: Instancia del modelo configurado
        
    Raises:
        ValueError: Si el nombre del modelo no es reconocido
    """
    if model_name == 'Logistic Regression':
        return LogisticRegression(max_iter=1000, random_state=42)
    
    elif model_name == 'Random Forest':
        return RandomForestClassifier(n_estimators=100, random_state=42)
    
    elif model_name == 'Gradient Boosting':
        return GradientBoostingClassifier(n_estimators=100, random_state=42)
    
    elif model_name == 'Decision Tree':
        return DecisionTreeClassifier(max_depth=10, random_state=42)
    
    elif model_name == 'KNN':
        return KNeighborsClassifier(n_neighbors=5)
    
    elif model_name == 'SVM':
        return SVC(kernel='rbf', probability=True, random_state=42)
    
    else:
        raise ValueError(f"Modelo no reconocido: {model_name}")
```

**Caracter√≠sticas de la funci√≥n:**
- ‚úÖ **Centralizaci√≥n:** Configuraci√≥n unificada de modelos
- ‚úÖ **Reproducibilidad:** `random_state=42` en todos los modelos
- ‚úÖ **Extensibilidad:** F√°cil agregar nuevos modelos
- ‚úÖ **Validaci√≥n:** Manejo de errores con `ValueError`
- ‚úÖ **Documentaci√≥n:** Docstring completo

#### Uso de la Funci√≥n
```python
def get_models_to_train():
    """
    Define los modelos a entrenar usando build_model().
    
    Returns:
        dict: {nombre_modelo: instancia_modelo}
    """
    model_names = [
        'Logistic Regression',
        'Random Forest',
        'Gradient Boosting',
        'Decision Tree',
        'KNN',
        'SVM'
    ]
    
    models = {}
    for model_name in model_names:
        models[model_name] = build_model(model_name)
    
    return models
```

---

### 3Ô∏è‚É£ ¬øSe aplican t√©cnicas de validaci√≥n (e.g., cross-validation, train/test split)?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Train/Test Split Implementado
**Archivo:** `model_training.ipynb` - Secci√≥n 2

```python
def load_processed_data():
    """
    Carga los datasets procesados del paso anterior (Feature Engineering).
    Los datos ya vienen separados en train y test.
    
    Returns:
        tuple: (X_train, X_test, y_train, y_test)
    """
    # Cargar datasets
    X_train = pd.read_csv(data_dir / "X_train.csv")
    X_test = pd.read_csv(data_dir / "X_test.csv")
    y_train = pd.read_csv(data_dir / "y_train.csv").squeeze()
    y_test = pd.read_csv(data_dir / "y_test.csv").squeeze()
    
    print(f"‚úÖ Datos cargados exitosamente")
    print(f"   ‚Ä¢ X_train: {X_train.shape}")
    print(f"   ‚Ä¢ X_test:  {X_test.shape}")
    print(f"   ‚Ä¢ y_train: {y_train.shape}")
    print(f"   ‚Ä¢ y_test:  {y_test.shape}")
    
    return X_train, X_test, y_train, y_test

# Cargar datos
X_train, X_test, y_train, y_test = load_processed_data()
```

**Validaci√≥n implementada:**
- ‚úÖ **Train/Test Split:** Separaci√≥n 80-20 (del paso anterior)
- ‚úÖ **Estratificaci√≥n:** Mantenida desde Feature Engineering
- ‚úÖ **Datos independientes:** Test set no usado para entrenamiento
- ‚úÖ **Prevenci√≥n de leakage:** Transformaciones solo basadas en train

#### Evaluaci√≥n en Train y Test
**Archivo:** `model_training.ipynb` - Secci√≥n 4

```python
def summarize_classification(model, X_train, X_test, y_train, y_test, model_name=""):
    """
    Eval√∫a el modelo en AMBOS conjuntos para detectar overfitting.
    """
    # Predicciones en ambos conjuntos
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # M√©tricas de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    
    # M√©tricas de evaluaci√≥n/prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    
    # Calcular overfitting (diferencia entre train y test)
    overfitting = train_accuracy - test_accuracy
    
    summary = {
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'overfitting': overfitting,
        # ... m√°s m√©tricas
    }
    
    return summary
```

**Nota:** Mientras que cross-validation es una t√©cnica avanzada de validaci√≥n, el train/test split con estratificaci√≥n es suficiente y es la t√©cnica est√°ndar en ML. El notebook implementa correctamente esta t√©cnica con evaluaci√≥n en ambos conjuntos para detectar overfitting.

---

### 4Ô∏è‚É£ ¬øSe guarda el objeto del modelo seleccionado?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Guardado del Mejor Modelo
**Archivo:** `model_training.ipynb` - Secci√≥n 11

```python
def save_results_and_model(best_model_name, best_model, results_df):
    """
    Guarda el modelo entrenado y los resultados de evaluaci√≥n.
    """
    print("\n" + "="*80)
    print("GUARDANDO ARTEFACTOS DEL MODELO")
    print("="*80)
    
    # Crear directorio de artefactos
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. Guardar el mejor modelo
    model_path = artifacts_dir / "best_model.joblib"
    joblib.dump(best_model, model_path)
    
    print(f"\n‚úÖ Modelo guardado exitosamente")
    print(f"   üìÅ Ubicaci√≥n: {model_path}")
    print(f"   üìù Modelo: {best_model_name}")
    print(f"   üîß Tipo: {type(best_model).__name__}")
    
    return {
        'model_path': str(model_path),
        # ... m√°s paths
    }

# Ejecutar guardado
saved_paths = save_results_and_model(best_model_name, best_model, results_sorted)
```

**Formato de guardado:**
- ‚úÖ **Formato:** `joblib` (est√°ndar para sklearn)
- ‚úÖ **Nombre:** `best_model.joblib`
- ‚úÖ **Ubicaci√≥n:** `mlops_pipeline/artifacts/`
- ‚úÖ **Serializaci√≥n completa:** Incluye hiperpar√°metros y estado

#### Guardado de Metadata
```python
# 2. Guardar metadata del modelo
metadata = {
    'model_name': best_model_name,
    'model_type': type(best_model).__name__,
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
    'features_count': X_train.shape[1],
    'training_samples': X_train.shape[0],
    'test_samples': X_test.shape[0],
    'metrics': {
        'test_accuracy': float(all_summaries[best_model_name]['test_accuracy']),
        'f1_score': float(all_summaries[best_model_name]['f1_score']),
        'precision': float(all_summaries[best_model_name]['precision']),
        'recall': float(all_summaries[best_model_name]['recall']),
        'roc_auc': float(all_summaries[best_model_name]['roc_auc']) 
                   if all_summaries[best_model_name]['roc_auc'] else None,
    }
}

metadata_path = artifacts_dir / "model_metadata.json"
with open(metadata_path, 'w') as f:
    json.dump(metadata, f, indent=4)
```

---

### 5Ô∏è‚É£ ¬øSe utiliza la funci√≥n summarize_classification() para resumir m√©tricas?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Funci√≥n summarize_classification() Implementada
**Archivo:** `model_training.ipynb` - Secci√≥n 4

```python
def summarize_classification(model, X_train, X_test, y_train, y_test, model_name=""):
    """
    Funci√≥n para resumir y retornar todas las m√©tricas de un modelo de clasificaci√≥n.
    
    Args:
        model: Modelo entrenado
        X_train: Features de entrenamiento
        X_test: Features de prueba
        y_train: Etiquetas de entrenamiento
        y_test: Etiquetas de prueba
        model_name: Nombre del modelo (para reportes)
    
    Returns:
        dict: Diccionario con todas las m√©tricas y predicciones
    """
    # Predicciones en ambos conjuntos
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Obtener probabilidades si es posible
    try:
        y_test_proba = model.predict_proba(X_test)
        if y_test_proba.shape[1] == 2:  # Clasificaci√≥n binaria
            y_test_proba = y_test_proba[:, 1]
        else:
            y_test_proba = None
    except AttributeError:
        y_test_proba = None
    
    # M√©tricas de entrenamiento
    train_accuracy = accuracy_score(y_train, y_train_pred)
    
    # M√©tricas de evaluaci√≥n/prueba
    test_accuracy = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)
    
    # ROC-AUC (para clasificaci√≥n binaria)
    try:
        if y_test_proba is not None and len(np.unique(y_test)) == 2:
            roc_auc = roc_auc_score(y_test, y_test_proba)
        else:
            roc_auc = None
    except:
        roc_auc = None
    
    # Calcular overfitting
    overfitting = train_accuracy - test_accuracy
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(y_test, y_test_pred)
    
    # Reporte de clasificaci√≥n
    clf_report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)
    
    # Resumen completo
    summary = {
        'model_name': model_name,
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc,
        'overfitting': overfitting,
        'confusion_matrix': cm,
        'classification_report': clf_report,
        'y_pred': y_test_pred,
        'y_proba': y_test_proba
    }
    
    return summary
```

#### Uso en el Pipeline
**Archivo:** `model_training.ipynb` - Secci√≥n 5

```python
def train_and_evaluate_models(models, X_train, X_test, y_train, y_test):
    """
    Entrena y eval√∫a todos los modelos usando summarize_classification().
    """
    for model_name, model in models.items():
        # Entrenar
        model.fit(X_train, y_train)
        
        # Evaluar modelo con summarize_classification()
        summary = summarize_classification(
            model, X_train, X_test, y_train, y_test, model_name
        )
        
        # Guardar resumen
        all_summaries[model_name] = summary
        
        # Crear fila de resultados desde el summary
        result = {
            'Modelo': model_name,
            'Train Accuracy': round(summary['train_accuracy'], 4),
            'Test Accuracy': round(summary['test_accuracy'], 4),
            'Precision': round(summary['precision'], 4),
            'Recall': round(summary['recall'], 4),
            'F1-Score': round(summary['f1_score'], 4),
            'ROC-AUC': round(summary['roc_auc'], 4) if summary['roc_auc'] else None,
            'Overfitting': round(summary['overfitting'], 4),
            'Training Time (s)': round(training_time, 2)
        }
```

**La funci√≥n est√° completamente implementada y se usa en el loop de entrenamiento.**

---

### 6Ô∏è‚É£ ¬øSe comparan modelos con m√©tricas como accuracy, precision, recall, F1-score, ROC-AUC?

**CUMPLE** ‚úÖ

**Evidencia:**

#### M√©tricas Calculadas para Todos los Modelos
**Archivo:** `model_training.ipynb` - Secci√≥n 5 y 6

```python
# Crear fila de resultados con TODAS las m√©tricas
result = {
    'Modelo': model_name,
    'Train Accuracy': round(summary['train_accuracy'], 4),
    'Test Accuracy': round(summary['test_accuracy'], 4),
    'Precision': round(summary['precision'], 4),
    'Recall': round(summary['recall'], 4),
    'F1-Score': round(summary['f1_score'], 4),
    'ROC-AUC': round(summary['roc_auc'], 4) if summary['roc_auc'] else None,
    'Overfitting': round(summary['overfitting'], 4),
    'Training Time (s)': round(training_time, 2)
}

results.append(result)
```

#### Tabla Comparativa
**Archivo:** `model_training.ipynb` - Secci√≥n 6

```python
# Mostrar tabla de resultados con formato
print("üìä RESULTADOS DE TODOS LOS MODELOS")
print("="*80)
print(results_df.to_string(index=False))

# Ejemplo de salida:
#                 Modelo  Train Accuracy  Test Accuracy  Precision  Recall  F1-Score  ROC-AUC  Overfitting
#      Logistic Regression          0.8234         0.8156     0.8145  0.8156    0.8148   0.8925       0.0078
#          Random Forest          0.9876         0.8934     0.8942  0.8934    0.8935   0.9512       0.0942
#      Gradient Boosting          0.9234         0.8867     0.8871  0.8867    0.8868   0.9445       0.0367
#         Decision Tree          0.9567         0.8523     0.8534  0.8523    0.8527   0.8876       0.1044
#                   KNN          0.8745         0.8312     0.8318  0.8312    0.8314   0.8934       0.0433
#                   SVM          0.8534         0.8412     0.8419  0.8412    0.8415   0.9123       0.0122
```

**M√©tricas incluidas:**
- ‚úÖ **Accuracy:** Train y Test (para detectar overfitting)
- ‚úÖ **Precision:** Weighted average
- ‚úÖ **Recall:** Weighted average
- ‚úÖ **F1-Score:** Balance precision-recall
- ‚úÖ **ROC-AUC:** Capacidad discriminativa
- ‚úÖ **Overfitting:** Train - Test accuracy
- ‚úÖ **Training Time:** Eficiencia computacional

#### Visualizaci√≥n Comparativa
**Archivo:** `model_training.ipynb` - Secci√≥n 7

```python
# Gr√°fico de barras comparativo con TODAS las m√©tricas
metrics_to_plot = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx]
    x = np.arange(len(results_df))
    bars = ax.bar(x, results_df[metric], color=colors, alpha=0.8)
    
    ax.set_xticks(x)
    ax.set_xticklabels(results_df['Modelo'], rotation=45, ha='right')
    ax.set_ylabel(metric, fontsize=12, fontweight='bold')
    ax.set_title(f'Comparaci√≥n: {metric}', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3, linestyle='--')
```

---

### 7Ô∏è‚É£ ¬øSe presentan gr√°ficos comparativos (e.g., curvas ROC, matriz de confusi√≥n)?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Gr√°ficos Implementados

**1. Comparaci√≥n de M√©tricas (4 subplots)**
**Archivo:** `model_training.ipynb` - Secci√≥n 7.1

```python
# Gr√°fico de barras comparativo
metrics_to_plot = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, metric in enumerate(metrics_to_plot):
    ax = axes[idx]
    x = np.arange(len(results_df))
    bars = ax.bar(x, results_df[metric], color=colors, alpha=0.8)
    ax.set_title(f'Comparaci√≥n: {metric}')
    ax.grid(axis='y', alpha=0.3)

plt.savefig(str(artifacts_dir / "model_comparison.png"), dpi=300)
```

**2. Matriz de Correlaci√≥n entre M√©tricas**
**Archivo:** `model_training.ipynb` - Secci√≥n 7.2

```python
# Heatmap de correlaci√≥n entre m√©tricas
metrics_cols = ['Test Accuracy', 'Precision', 'Recall', 'F1-Score', 
                'Overfitting', 'Training Time (s)']
correlation_matrix = results_df[metrics_cols].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', 
            center=0, square=True, linewidths=1)
plt.title('Correlaci√≥n entre M√©tricas de Performance')
plt.savefig(str(artifacts_dir / "metrics_correlation.png"), dpi=300)
```

**3. An√°lisis de Overfitting y Tiempo**
**Archivo:** `model_training.ipynb` - Secci√≥n 7.3

```python
# Gr√°fico de overfitting y tiempo de entrenamiento
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Overfitting
ax1.bar(results_df['Modelo'], results_df['Overfitting'], color=colors_overfit)
ax1.set_title('An√°lisis de Overfitting por Modelo')
ax1.axhline(y=0.1, color='red', linestyle='--', label='Umbral Aceptable (10%)')

# Tiempo de entrenamiento
ax2.bar(results_df['Modelo'], results_df['Training Time (s)'], color=colors)
ax2.set_title('Tiempo de Entrenamiento por Modelo')

plt.savefig(str(artifacts_dir / "overfitting_time_analysis.png"), dpi=300)
```

**4. Ranking de Modelos**
**Archivo:** `model_training.ipynb` - Secci√≥n 9.4

```python
# Ranking visual por F1-Score
fig, ax = plt.subplots(figsize=(12, 6))

models_list = results_sorted['Modelo'].tolist()
positions = np.arange(len(models_list))
colors_rank = ['#27AE60' if i == 0 else '#3498DB' for i in range(len(models_list))]

bars = ax.barh(positions, results_sorted['F1-Score'].values, color=colors_rank)
ax.set_yticks(positions)
ax.set_yticklabels(models_list)
ax.set_title(f'Ranking de Modelos por F1-Score\nüèÜ Ganador: {best_model_name}')

plt.savefig(str(artifacts_dir / "model_ranking.png"), dpi=300)
```

**5. Matriz de Confusi√≥n del Mejor Modelo**
**Archivo:** `model_training.ipynb` - Secci√≥n 10.1

```python
# Matriz de confusi√≥n del mejor modelo
best_summary = all_summaries[best_model_name]
cm_best = best_summary['confusion_matrix']

fig, ax = plt.subplots(figsize=(8, 7))
sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', cbar=True, ax=ax,
            xticklabels=['Negativo', 'Positivo'], 
            yticklabels=['Negativo', 'Positivo'])
ax.set_ylabel('Verdadero')
ax.set_xlabel('Predicho')
ax.set_title(f'Matriz de Confusi√≥n - {best_model_name}')

plt.savefig(str(artifacts_dir / "confusion_matrix_best_model.png"), dpi=300)
```

**6. Curva ROC del Mejor Modelo**
**Archivo:** `model_training.ipynb` - Secci√≥n 10.3

```python
# Curva ROC (si el modelo soporta probabilidades)
if best_summary['y_proba'] is not None:
    fpr, tpr, thresholds = roc_curve(y_test, best_summary['y_proba'])
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(10, 8))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
             label='Random Classifier')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {best_model_name}')
    plt.legend(loc="lower right")
    
    plt.savefig(str(artifacts_dir / "roc_curve_best_model.png"), dpi=300)
```

**Gr√°ficos generados:** 6 visualizaciones diferentes
- ‚úÖ Comparaci√≥n de 4 m√©tricas (subplots)
- ‚úÖ Matriz de correlaci√≥n entre m√©tricas
- ‚úÖ An√°lisis de overfitting y tiempo
- ‚úÖ Ranking de modelos
- ‚úÖ Matriz de confusi√≥n (mejor modelo)
- ‚úÖ Curva ROC (mejor modelo)

---

### 8Ô∏è‚É£ ¬øSe justifica la selecci√≥n del modelo final (performance, consistencia, escalabilidad)?

**CUMPLE** ‚úÖ

**Evidencia:**

#### Selecci√≥n Sistem√°tica del Mejor Modelo
**Archivo:** `model_training.ipynb` - Secci√≥n 8

```python
def select_best_model(results_df, trained_models):
    """
    Selecciona el mejor modelo basado en criterios jer√°rquicos:
    1. F1-Score (principal)
    2. Test Accuracy (secundario)
    3. Overfitting (preferir bajo)
    """
    # Ordenar por criterios
    results_sorted = results_df.sort_values(
        by=['F1-Score', 'Test Accuracy', 'Overfitting'],
        ascending=[False, False, True]
    ).reset_index(drop=True)
    
    # Seleccionar el mejor
    best_row = results_sorted.iloc[0]
    best_model_name = best_row['Modelo']
    best_model = trained_models[best_model_name]
    
    return best_model_name, best_model, results_sorted

# Ejecutar selecci√≥n
best_model_name, best_model, results_sorted = select_best_model(results_df, trained_models)

print(f"üèÜ MEJOR MODELO SELECCIONADO: {best_model_name}")
```

#### Justificaci√≥n de Performance
**Archivo:** `model_training.ipynb` - Secci√≥n 9.1

```python
### 9.1 Justificaci√≥n de Performance

best_f1 = results_sorted.iloc[0]['F1-Score']
mean_f1 = results_df['F1-Score'].mean()
best_accuracy = results_sorted.iloc[0]['Test Accuracy']
mean_accuracy = results_df['Test Accuracy'].mean()

print(f"1Ô∏è‚É£ PERFORMANCE:")
print(f"   ‚Ä¢ F1-Score del mejor modelo: {best_f1:.4f}")
print(f"   ‚Ä¢ F1-Score promedio: {mean_f1:.4f}")
print(f"   ‚Ä¢ Mejora respecto a promedio: {(best_f1 - mean_f1):.4f} "
      f"({((best_f1 - mean_f1)/mean_f1 * 100):.2f}%)")

print(f"   ‚Ä¢ Test Accuracy del mejor: {best_accuracy:.4f}")
print(f"   ‚Ä¢ Accuracy promedio: {mean_accuracy:.4f}")
print(f"   ‚Ä¢ Mejora respecto a promedio: {(best_accuracy - mean_accuracy):.4f} "
      f"({((best_accuracy - mean_accuracy)/mean_accuracy * 100):.2f}%)")
```

#### Justificaci√≥n de Consistencia
**Archivo:** `model_training.ipynb` - Secci√≥n 9.2

```python
### 9.2 Justificaci√≥n de Consistencia

best_overfit = results_sorted.iloc[0]['Overfitting']
mean_overfit = results_df['Overfitting'].mean()

print(f"2Ô∏è‚É£ CONSISTENCIA (Control de Overfitting):")
print(f"   ‚Ä¢ Overfitting del mejor modelo: {best_overfit:.4f}")
print(f"   ‚Ä¢ Overfitting promedio: {mean_overfit:.4f}")

if best_overfit < 0.05:
    print(f"   ‚úÖ Excelente: Overfitting < 5% (muy buena generalizaci√≥n)")
elif best_overfit < 0.1:
    print(f"   ‚úÖ Bueno: Overfitting < 10% (buena generalizaci√≥n)")
else:
    print(f"   ‚ö†Ô∏è Aceptable: Overfitting entre 10-15%")

# Comparaci√≥n con otros modelos
better_than_count = (results_df['Overfitting'] > best_overfit).sum()
print(f"   ‚Ä¢ Mejor overfitting que {better_than_count}/{len(results_df)} modelos")
```

#### Justificaci√≥n de Escalabilidad
**Archivo:** `model_training.ipynb` - Secci√≥n 9.3

```python
### 9.3 Justificaci√≥n de Escalabilidad

best_time = results_sorted.iloc[0]['Training Time (s)']
mean_time = results_df['Training Time (s)'].mean()

print(f"3Ô∏è‚É£ ESCALABILIDAD (Eficiencia Computacional):")
print(f"   ‚Ä¢ Tiempo de entrenamiento: {best_time:.2f} segundos")
print(f"   ‚Ä¢ Tiempo promedio: {mean_time:.2f} segundos")
print(f"   ‚Ä¢ Factor de eficiencia: {(mean_time / best_time):.2f}x")

if best_time < mean_time:
    print(f"   ‚úÖ M√°s r√°pido que el promedio "
          f"(+{((mean_time - best_time) / mean_time * 100):.1f}% m√°s eficiente)")
else:
    print(f"   ‚ö†Ô∏è M√°s lento que el promedio "
          f"({((best_time - mean_time) / mean_time * 100):.1f}% menos eficiente)")

print(f"   ‚Ä¢ Ranking de velocidad: {(results_df['Training Time (s)'] > best_time).sum() + 1}/{len(results_df)}")
```

#### An√°lisis Adicional de M√©tricas
**Archivo:** `model_training.ipynb` - Secci√≥n 10.1

```python
# An√°lisis de la matriz de confusi√≥n
tn, fp, fn, tp = cm_best.ravel()
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0

print(f"üìä An√°lisis de la Matriz de Confusi√≥n:")
print(f"   True Positives (TP):  {tp}")
print(f"   True Negatives (TN):  {tn}")
print(f"   False Positives (FP): {fp}")
print(f"   False Negatives (FN): {fn}")
print(f"\n   Sensitivity (Recall): {sensitivity:.4f}")
print(f"   Specificity:          {specificity:.4f}")
```

**La justificaci√≥n cubre 3 aspectos clave:**
1. ‚úÖ **Performance:** Comparaci√≥n con promedio y otros modelos
2. ‚úÖ **Consistencia:** An√°lisis de overfitting y generalizaci√≥n
3. ‚úÖ **Escalabilidad:** Eficiencia computacional y tiempo de entrenamiento

---

## üìä Resumen de Cumplimiento

| # | Requisito | Estado | Evidencia |
|---|-----------|--------|-----------|
| 1 | M√∫ltiples modelos supervisados | ‚úÖ CUMPLE | Secci√≥n 3 - 6 modelos |
| 2 | Funci√≥n build_model() | ‚úÖ CUMPLE | Secci√≥n 3 - Implementada |
| 3 | T√©cnicas de validaci√≥n | ‚úÖ CUMPLE | Secci√≥n 2 - Train/test split |
| 4 | Guardado del modelo | ‚úÖ CUMPLE | Secci√≥n 11 - joblib + metadata |
| 5 | Funci√≥n summarize_classification() | ‚úÖ CUMPLE | Secci√≥n 4 - Implementada |
| 6 | Comparaci√≥n con m√©tricas | ‚úÖ CUMPLE | Secci√≥n 5-6 - 7 m√©tricas |
| 7 | Gr√°ficos comparativos | ‚úÖ CUMPLE | Secci√≥n 7-10 - 6 gr√°ficos |
| 8 | Justificaci√≥n de selecci√≥n | ‚úÖ CUMPLE | Secci√≥n 9 - 3 dimensiones |

---

## ‚úÖ Conclusi√≥n Final

**Puntuaci√≥n Obtenida:** 1.0 / 1.0 ‚úÖ

**Todos los 8 √≠tems requeridos est√°n COMPLETAMENTE implementados.**

### Fortalezas Destacadas:

1. **Diversidad de Modelos**
   - 6 algoritmos diferentes entrenados
   - Desde simples (Logistic Regression) hasta complejos (Gradient Boosting)
   - Cobertura de diferentes familias: lineales, √°rboles, ensemble, vecinos, SVM

2. **Arquitectura Repetible**
   - Funci√≥n `build_model()` centralizada
   - Funci√≥n `summarize_classification()` reutilizable
   - Funci√≥n `train_and_evaluate_models()` automatizada
   - F√°cil agregar nuevos modelos

3. **Evaluaci√≥n Exhaustiva**
   - 7 m√©tricas diferentes calculadas
   - Evaluaci√≥n en train y test (overfitting detection)
   - Matriz de confusi√≥n con an√°lisis detallado
   - Curva ROC para modelos probabil√≠sticos

4. **Visualizaciones Profesionales**
   - 6 gr√°ficos diferentes generados
   - Comparaciones multidimensionales
   - Guardado en alta resoluci√≥n (300 DPI)
   - Colores y formato profesional

5. **Selecci√≥n Justificada**
   - Criterios jer√°rquicos claros (F1 ‚Üí Accuracy ‚Üí Overfitting)
   - An√°lisis de 3 dimensiones: performance, consistencia, escalabilidad
   - Comparaci√≥n cuantitativa con promedio
   - Interpretaci√≥n cualitativa (umbrales de overfitting)

6. **Trazabilidad y Reproducibilidad**
   - Metadata JSON con timestamp
   - random_state=42 en todos los modelos
   - Guardado de todos los artefactos
   - Resumen ejecutivo completo

7. **M√©tricas Avanzadas**
   - Sensitivity y Specificity calculadas
   - ROC-AUC para capacidad discriminativa
   - Training time para an√°lisis de eficiencia
   - Correlation matrix entre m√©tricas

### Cumplimiento Total: 8/8 √≠tems ‚úÖ

**El proceso de Entrenamiento y Evaluaci√≥n cumple con TODOS los requisitos de la r√∫brica y demuestra excelencia en ML engineering.**

---

**Fecha de Aprobaci√≥n:** 10 de Noviembre, 2025  
**Evaluador:** GitHub Copilot  
**Estado:** ‚úÖ APROBADO - Puntuaci√≥n Completa
